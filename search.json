[{"title":"休息一下","url":"https://hahally.github.io//articles/休息一下/","content":"<blockquote>\n<p>那个暑假我们说过最多的话就是“休息一下”</p>\n</blockquote>\n<p>夏天来的很急促，一晃六月就到了。那几天疯狂找暑期实习，银行基本投了个遍，但是简历都被筛了。后面boss直聘上也开始投，最后进了一家小公司。生活的轨迹开始发生了变化。</p>\n<p>带着证件资料办理入职，合同看了好几遍，生怕被骗。最后还是签下了。那天上午简单了解后，知道他们正在打一个文本生成的比赛。在一个靠里面的工位坐下，打开电脑，开始干活。我对面是一个留学生，大四刚毕业。右边也是大四即将毕业，学校在附近，我们喊他后端大佬。他对面是个大三的学生，和我一个学校。后端大佬的右边是研三的，和我一个学校，算是我的学长。</p>\n<p>第一天我把自己之前比赛的代码拿过来跑了一下，提交了两次冲到了前十。大家很震惊，问我怎么做的。简单说了一下自己的发现，比赛的评分对标点符号比较敏感，训练数据集中的标点符号都是半角的，而测试集里的是全角，模型的输出也是带半角的，于是我把标点符号转成了全角，涨了好几个百分点。中午吃饭的时候，大家都在说公司和老板不靠谱，说完，k问我不会下午就跑了吧。我犹豫了一下，先干着吧。</p>\n<p>此时的我，因为看不惯导师，选择逃避，跑出来实习了，但是论文还没有写完。有一天她喊我出去讨论，结果一顿内涵我情商低，不喜欢交流之类的。十多分钟下来，内心已经气炸了，但是还得表现没事一样。内涵我的那件事情不止一次和我说了。她以前某个学生和我很像，也是能力很强，很傲慢巴拉巴拉的，不喜欢听取别人的意见……(字打到这里，我掐了一下人中才缓过来)，所有不打算说下去了。</p>\n<p>说实话，这群人还挺有趣的，在这上上班，也不累。大家吃饭的时候就一起吐槽老板。g说前几天他进去把老板骂了一顿，k后面进去又说了一下老板。后来端午节放假后，g被老板辞退了，以一个莫名其妙的理由。估计是老板觉得那次丢面子了。</p>\n<p>在我上班第二天，来了一个女生，做产品实习。至此，这个公司达到人数最多的时候。6个实习生加一个人事和一个会计。人事我们叫王哥，会计我们喊廖姐。两个人坐在最里面靠窗那一排，平常也不说话。不过在g被辞后，就只有5个实习生了。</p>\n<p>日子依旧正常过着，上班的时候时不时嘻嘻哈哈几下。小蜜蜂上也经常发些表情包。k总在小蜜蜂上喊我们休息一下。后端大佬不是在debug就是在与环境作斗争。产品被喊去打电话。留学生负责查找资料，我训练模型，k精致的后处理，我们的核心科技。六月下旬，我们就这样还算愉快的过去了。我们也以第三名的成绩进入复赛。</p>\n<p>30号那天，刚好周五。后端大佬已经和老板说了离职了。中午一起吃了顿散伙饭。后端大佬说想去外面的地方看看。那天下午快下班的时候，被老板喊进办公室开会了。拖延了十多分钟。出来时，后端大佬已经先走了。那天下班回学校路上还挺感伤的，平常都是一起去地铁站。今天突然少了一个人，而且明天也是，心里空落落的。后来，后端大佬去了南宁旅游，再后来去深圳上班了，那里是他说的外面的地方。</p>\n<blockquote>\n<p>一双无形的手轻轻地点了一下平静的湖面，泛起涟漪。于是，时间有了形状，生活有了色彩，人才是人…..</p>\n</blockquote>\n<p>周末结束，七月第一天上班，那天天气格外的热。我的右边工位已经没人了。k一大早就来了，在办公室和老板谈话。印象里谈了很久，我以为他也要走了，突然一下子觉得上班没有一点意思了。最后他还是留下来，他说老板卖惨挽留他，磨了他很久，最后他就转正了，继续在这待着。七月大部分只有我们四个人在这。后面又招了一个产品和一个后端接替之前的工作。新来的后端不怎么说话，平常一个人摸鱼就在那玩纸牌游戏或者打麻将。</p>\n<p>那段时间开始变得无趣起来，每天七点多起床洗漱，出门。这个点学校里已经很热闹了，有人在打篮球，有人在打太极，有人在跳舞…在校门口买个早餐，边走边吃，到地铁口差不多八点左右。早高峰确实有点吓人，一号线经常是满的，偶尔有个空位，我站进去刚好塞满。六号线换乘，人来人往，匆匆忙忙的脚步声回荡在站内，都是打工人在赶地铁。记得来面试那天第一次觉得这个换乘站好大，换线都要五到十分钟。上班后，突然感觉这个站还是小了。有天换乘的时候坐反了，那天迟到了半个小时。出站后，还要走个十分钟左右才到公司。路边很多早餐店，有一个路口围坐一群大爷在那打牌。有家药店门口经常放一把椅子，椅子上栓了一只小泰迪。再往前有所小学，六月份的时候经常有摆摊卖小吃，之后转弯，过个路口，就差不多到公司所在的那栋楼了。到公司坐下，缓一缓，然后开始干活。中午吃完午饭，就趴桌子上睡午觉。下午差不多快两点继续干活。六点下班，k住附近，在路口与我们分开。去地铁路上，只有我，留学生和产品三个人。我们总结了规律，如果六点下班，正常的话会赶上六点二十二或者二十四的地铁，而且在车头位置大概率能够抢到位置坐。第一个下车的是产品，然后是我，最后是留学生。转一号线，人就比较多了，出站后大概七点多了。然后在路口等红绿灯，我会找辆共享电动车坐一会，时不时看看天，看看来往的车，看看聚集的人，看看对面的倒计时。晚饭选择并不多，大概率杀猪粉，其次黄焖鸡，偶尔炒饭，记得有段时间吃了一周的杀猪粉。吃完回到自习室休息一下大概就八点了。这段时间还参加了高校大数据挑战赛。所以回学校后，基本在做这个比赛。竞争很激烈，拿奖也比去年难了，但是还是想试一下。</p>\n<blockquote>\n<p>像是早已约定好的计划，冥冥之中，我们被推着前进，赴一场约，演一场确定好结局的演出。</p>\n</blockquote>\n<p>日子不紧不慢的过着，直到复赛的日子到了。那天收拾东西准备去南京，一大早老板告诉我们票没有抢到，让我们自己购票。拖着行李出门到高铁站那一段时间有点煎熬。甚至都不想去了。后面疯狂抢票，买到了九点之前的，不过要从武汉换乘。一段神奇的旅途就这样开始了……</p>\n<p>到武汉出站后，坐地铁赶汉口站转乘。开始跑图，一个多小时后，到达，本来想在那附近找个饭店吃饭的，但是时间来不及了。打算去肯德基看看，结果人太多了，时间也来不及。最后在高铁上吃了一顿。</p>\n","categories":[],"tags":[]},{"title":"基于弱监督的深度语义文本哈希","url":"https://hahally.github.io//articles/基于弱监督的深度语义文本哈希/","content":"<blockquote>\n<p>论文：Deep Semantic Text Hashing with Weak Supervision，SIGIR，2018</p>\n</blockquote>\n<p>论文提出一种弱监督学习方法。采用bm25对相似文档进行排序，提取数据中的弱监督信号。先训练一个可以得到整个文档的语义向量表示的模型，然后根据语义向量，运用一些规则（设置阈值）将对应维度变成0或1。</p>\n<ul>\n<li>通过使用无监督排序来逼近真实的文档空间，从而弥补了标记数据的不足。</li>\n<li>设计了两个深度生成模型来利用文档的内容和估计的邻域来学习语义哈希函数。（NbrReg和NbrReg+doc）</li>\n</ul>\n<p>两个语义向量表示模型（NbrReg和NbrReg+doc）区别在于是否利用了近邻文档信息。每个模型包含两个部分：encoder、decoder。</p>\n<p>该方法步骤包括三个部分：Document Space Estimation —&gt; NbrReg（NbrReg+doc） —&gt; Binarization</p>\n<ul>\n<li>Document Space Estimation：得到整个文档数据的空间分布情况</li>\n</ul>\n<p>在有标签信息的情况下，可以得到真实文档空间分布。没有标签信息的时候，利用bm25为每个文档 d 检索出一组与之最相似的近邻文档NN(d)。论文假设：近邻文档中大多数与文档 d 具有相同标签，因此任何文档的二进制哈希值在相近的向量空间模型中应该更加近似。</p>\n<ul>\n<li>NbrReg：语义向量模型</li>\n</ul>\n<p>文档语义向量 s ，满足标准正态分布 N(0,1)</p>\n<p> $w_i \\in d$ ，概率 $P_A(w_i|s)$  ； $\\hat w_j \\in NN(d)$ ,概率$P_B(\\hat w_j|s)$</p>\n<p>定义联合概率： $P(d) = \\prod_{i}P_A(w_i|s)$ ，$P(NN(d))=\\prod_{j}P_B(\\hat w_j|s)$</p>\n<p>目标函数：最大化$P(d,NN(d)) = P(d)P(NN(d))$  </p>\n<script type=\"math/tex; mode=display\">\nlogP(d,NN(d)) = log\\int_{s}P(d|s)P(NN(d)|s)P(s)ds\\\\\\geq E_{Q(s|·)}[logP(dd|s)] + E_{Q(s|·)}[logP(NN(d)|s)]-D_{KL}(Q(s|·)||P(s))</script><p>其中 $Q(d|·)$ 表示从数据中学到的近似后验概率分布；<strong>·</strong> 符号表示输入随机变量的占位符；$D_{KL}$ 表示KL散度；</p>\n<p><strong>Decoder Function</strong></p>\n<script type=\"math/tex; mode=display\">\nP(d) = \\prod_{i}P_A(w_i|s)=\\prod_{i}\\frac{exp(s^TAe_i)}{exp(\\sum_{j}s^TAe_j)}</script><p>$e_j$ 表示一个词袋向量，矩阵A将语义向量s映射到词编码空间。$P(NN(d))$ 与上面类似，只是映射矩阵用B表示。</p>\n<p><strong>Encoder Function</strong></p>\n<p>定义 $Q(s|·)$ 为文档d参数化的正态分布：$Q(s|·) = N(s,f(d))$ 。<em>f(·)</em> 函数将d表示为均值为$\\mu$ 标准差为$\\sigma$ 正态分布的向量。 为了表征两个参数，定义$f = <f_{\\mu},f_{\\sigma}>$  ，相当于定义了两个前馈神经网络：</f_{\\mu},f_{\\sigma}></p>\n<script type=\"math/tex; mode=display\">\nf_{\\mu}(d) = W_{\\mu}·h(d)+b_{\\mu} \\\\f_{\\sigma}(d) = W_{\\sigma}·h(d)+b_{\\sigma}\\\\h(d) = relu(W_2·relu(W_1·d+b_1)+b_2)</script><p>语义向量s从Q中采样：</p>\n<script type=\"math/tex; mode=display\">\ns \\sim Q(s|d)=N(s;\\mu=f_{\\mu}(d),\\sigma = f_{\\sigma}(d))</script><ul>\n<li>Utilize Neighbor Documents：(NbrReg+Doc）</li>\n</ul>\n<p>论文中提到相邻文档使用的一组单词可以表示该区域所有文档的主题，但是来自相邻文档的额外的词可能会引入噪声，混淆模型。为了削减噪声带来的影响，引入了一层隐藏层，用该层向量来表示近邻文档，使用一个平均池化层得到 近邻文档的中心表示。只有编码器部分有所不同，其他与NbrReg一致。</p>\n<script type=\"math/tex; mode=display\">\nZ^{NN} = relu(W_2^{NN}·relu(W_1^{NN}·NN(d)+b_1^{NN})+b_2)\\\\h_{NN}(NN(d)) = mean(Z^{NN})\\\\f_{\\mu}(d,NN(d)) = W_{\\mu}·(h(d)+h_{NN}(NN(d)))+b_{\\mu}</script><ul>\n<li>Binarization</li>\n</ul>\n<p>根据编码器 $Q(s|·)$ 为文档d生成一个连续的语义向量。论文中使用编码器输出的正态分布的均值来表示语义向量 $\\overline s = E[Q(s|·)]$，然后使用中值法生成二进制编码。若大于该阈值就令该位为1，否者为0.</p>\n<blockquote>\n<p>思考</p>\n</blockquote>\n<p>论文并没有显示道德直接学习二进制表示，而是通过训练一个语义模型，假设语义相近文档对应二进制表示应该相近，然后通过语义向量进一步转化为二进制哈希值。值得一提的是语义向量是服从正太分布的，一方面便于训练，另一方面也可以给模型提供很好的可解释性，所有文档可以映射到正态分布的语义空间，语义相近的向量具有相近的分布值（论文假设语义向量服从正太分布，并用其均值表示），这也确保了二值化的时候语义相近的文档在映射为二进制哈希值后也保持距离相近。</p>\n<blockquote>\n<p>开源代码</p>\n</blockquote>\n<p>github上找到两处开源代码，一个是作者的低调开源，一个是路人甲的好心复现。</p>\n<ul>\n<li><p>作者开源：<a href=\"https://github.com/unsuthee/SemanticHashingWeakSupervision\" target=\"_blank\" rel=\"noopener\">https://github.com/unsuthee/SemanticHashingWeakSupervision</a></p>\n</li>\n<li><p>复现代码：<a href=\"https://github.com/yfy-/nbrreg\" target=\"_blank\" rel=\"noopener\">https://github.com/yfy-/nbrreg</a></p>\n</li>\n</ul>\n<p>作者开源的代码，一言难尽，虽然很贴心的把对比模型也复现了出来，但是数据没给，如何用bm25算法处理的过程都给省去了。于是找到了一个好心人提供了nbrreg模型的复现，而且给了一份数据，以及对数据进行处理的代码。但是模型训练没有考虑到用gpu的情况。所以下面主要对复现代码进行分析。</p>\n<p><strong>数据处理</strong></p>\n<p>提供的数据是20newsgroups数据集，20ng-all-stemmed.txt：18820行，20个类别</p>\n<figure class=\"highlight applescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alt.atheism\talt atheism faq atheist resourc archiv <span class=\"built_in\">name</span> atheism resourc alt atheism...</span><br><span class=\"line\">···</span><br></pre></td></tr></table></figure>\n<p>格式为：label    w1 w2 w3…，一行为一条数据，由标签和对应文档组成，文档由一个空格分开的词组成。</p>\n<p>数据处理代码为：prepare_data.py</p>\n<ul>\n<li>输入：20ng-all-stemmed.txt中的文本</li>\n<li>输出：train_docs、cv_docs、test_docs、train_cats、cv_cats、test_cats、train_knn<ul>\n<li>train_docs、cv_docs、test_docs：分别为训练集、验证集、测试集，维度为vocab_size。</li>\n<li>train_cats、cv_cats、test_cats：对应标签，one-hot向量，维度为20。</li>\n<li>train_knn：train_docs中每条数据的近邻文档的索引。</li>\n</ul>\n</li>\n</ul>\n<p>这部分代码主要是得到用于模型输入的数据，即将文本数据用数值表示。这里将每个文档用bm25权重值表示。BM25是信息索引领域用来计算query与文档相似度得分的经典算法。论文中使用bm25检索近邻文档，作为训练的弱监督信号。</p>\n<p>BM25的一般公式：</p>\n<script type=\"math/tex; mode=display\">\nScore(Q,d) = \\sum_{i=1}^{n}W_i*R(q_i,d)</script><p>$Q$表示一个query，$q_i$  表示$Q$中的单词，$d$表示某个搜索文档。$W_i$ 表示单词权重，用$idf$ 表示：</p>\n<script type=\"math/tex; mode=display\">\nidf(q_i) = log\\frac{N-df_i+0.5}{df_i+0.5}</script><p>$df_i$ 为包含了$q_i$ 的文档个数。依据IDF的作用，对于某个 $q_i$，包含 $q_i$的文档数越多，说明$q_i$重要性越小，或者区分度越低，IDF越小，因此IDF可以用来刻画$q_i$与文档的相似性。</p>\n<p>$R(q_i,d)$ 表示为：</p>\n<script type=\"math/tex; mode=display\">\nR(q_i,d) = \\frac{(k_1+1)·f(q_i,d)}{f(q_i,d)+k_1·(1-b+b·\\frac{|d|}{avgdL})}</script><p>$f(q_i,d)$ 表示$q_i$在文档 d 中的词频，$|d|$ 表示文档 d的长度，avgdL是语料库全部文档的平均长度。$k_1$ 和 $b$ 为经验参数，一般的$k_1\\in [1.2,2.0],b=0.75$</p>\n<p>假设一共有 n 个文档，按照该公式计算最终一个文档 d 会得到 n 个得分。但是代码中计算的是$Score(d,d)$ ，而且没有求和操作。所以一个文档 d 会由一个vocab_size维度大小的向量表示。按照论文要求，会根据 n 个得分进行降序排列，选 k 个作为文档 d 的近邻文档$NN(d)$ 。复现的代码中则是根据上述向量计算余弦相似度然后选取近邻文档的。</p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%96%87%E6%9C%AC%E5%93%88%E5%B8%8C/image-20220126205246354.png\" alt=\"image-20220126205246354\"></p>\n<p>其中$term_freq$ 对应词频$f(q_i,d)$ 的$n\\times vocab_size$ 大小的矩阵，$cosin_similarity(train_docs)$ 计算文档与文档之间的余弦相似度得分。<strong>代码中近邻文档选取了100个</strong> 。</p>\n<p>计算idf值代码：</p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%96%87%E6%9C%AC%E5%93%88%E5%B8%8C/image-20220126210125077.png\" alt=\"image-20220126210125077\"></p>\n<p>这一处分母应该是$(df+0.5)$ 。少了一个括号！！！</p>\n<p>模型训练测试代码都在一个文件里：nbrreg.py</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NbrReg</span><span class=\"params\">(torch.nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, lex_size, bit_size=<span class=\"number\">32</span>, h_size=<span class=\"number\">1000</span>)</span>:</span></span><br><span class=\"line\">        super(NbrReg, self).__init__()</span><br><span class=\"line\">        self.lnr_h1 = torch.nn.Linear(lex_size, h_size)</span><br><span class=\"line\">        self.lnr_h2 = torch.nn.Linear(h_size, h_size)</span><br><span class=\"line\">        self.lnr_mu = torch.nn.Linear(h_size, bit_size)</span><br><span class=\"line\">        self.lnr_sigma = torch.nn.Linear(h_size, bit_size)</span><br><span class=\"line\">        self.lnr_rec_doc = torch.nn.Linear(bit_size, lex_size)</span><br><span class=\"line\">        self.lnr_nn_rec_doc = torch.nn.Linear(bit_size, lex_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, docs)</span>:</span></span><br><span class=\"line\">        mu, sigma = self.encode(docs)</span><br><span class=\"line\">        <span class=\"comment\"># qdist表示语义向量s，服从正态分布 N~(mu,sigma^2)</span></span><br><span class=\"line\">        qdist = tdist.Normal(mu, sigma)</span><br><span class=\"line\">        log_prob_words, log_nn_prob_words = self.decode(qdist.rsample())</span><br><span class=\"line\">        <span class=\"keyword\">return</span> qdist, log_prob_words, log_nn_prob_words</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"comment\"># 对应论文中的编码函数</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encode</span><span class=\"params\">(self, docs)</span>:</span></span><br><span class=\"line\">        relu = torch.nn.ReLU()</span><br><span class=\"line\">        sigmoid = torch.nn.Sigmoid()</span><br><span class=\"line\">        hidden = relu(self.lnr_h2(relu(self.lnr_h1(docs))))</span><br><span class=\"line\">        mu = self.lnr_mu(hidden)</span><br><span class=\"line\">        <span class=\"comment\"># Use sigmoid for positive standard deviation</span></span><br><span class=\"line\">        sigma = sigmoid(self.lnr_sigma(hidden))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mu, sigma</span><br><span class=\"line\">\t<span class=\"comment\"># 对应论文中解码函数</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decode</span><span class=\"params\">(self, latent)</span>:</span></span><br><span class=\"line\">        log_softmax = torch.nn.LogSoftmax(dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        log_prob_words = log_softmax(self.lnr_rec_doc(latent))</span><br><span class=\"line\">        log_nn_prob_words = log_softmax(self.lnr_nn_rec_doc(latent))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> log_prob_words, log_nn_prob_words</span><br></pre></td></tr></table></figure>\n<p>模型部分按照论文中的描述，使前馈神经网络就可以实现。值得一提的是 $qdist$ 应该才是文中对应的服从正态分布的语义向量 s。但在生成二进制哈希值时，取的是编码器输出的均值。</p>\n<p>训练代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(train_docs, train_cats, train_knn, cv_docs, cv_cats, bitsize=<span class=\"number\">32</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">          epoch=<span class=\"number\">30</span>, bsize=<span class=\"number\">100</span>, lr=<span class=\"number\">1e-3</span>, latent_size=<span class=\"number\">1000</span>, resume=None,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">          imp_trial=<span class=\"number\">0</span>)</span>:</span></span><br><span class=\"line\">    nsize, lexsize = train_docs.shape</span><br><span class=\"line\">    num_iter = int(np.ceil(nsize / bsize))</span><br><span class=\"line\">    model = resume <span class=\"keyword\">if</span> resume <span class=\"keyword\">else</span> NbrReg(lexsize, bitsize, h_size=latent_size)</span><br><span class=\"line\">    model.double()</span><br><span class=\"line\">    optim = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class=\"line\">    norm = tdist.Normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">    best_prec = <span class=\"number\">0.0</span></span><br><span class=\"line\">    trial = <span class=\"number\">0</span></span><br><span class=\"line\">    epoch_range = itertools.count() <span class=\"keyword\">if</span> imp_trial <span class=\"keyword\">else</span> epoch</span><br><span class=\"line\">    epoch = <span class=\"string\">\"INF\"</span> <span class=\"keyword\">if</span> imp_trial <span class=\"keyword\">else</span> epoch</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> epoch_range:</span><br><span class=\"line\">        model.train()</span><br><span class=\"line\">        losses = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_iter):</span><br><span class=\"line\">            print(<span class=\"string\">f\"Epoch: <span class=\"subst\">&#123;e + <span class=\"number\">1</span>&#125;</span>/<span class=\"subst\">&#123;epoch&#125;</span>, Iteration: <span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>/<span class=\"subst\">&#123;num_iter&#125;</span>\"</span>,</span><br><span class=\"line\">                  end=<span class=\"string\">\"\\r\"</span>)</span><br><span class=\"line\">            batch_i = np.random.choice(nsize, bsize)</span><br><span class=\"line\">            np_batch = train_docs[batch_i].todense()</span><br><span class=\"line\">            doc_batch = torch.from_numpy(np_batch).double()</span><br><span class=\"line\">            knn_batch = train_knn[batch_i]</span><br><span class=\"line\">            optim.zero_grad()</span><br><span class=\"line\">            qdist, log_prob_words, log_nn_prob_words = model(doc_batch)</span><br><span class=\"line\">            doc_rl = doc_rec_loss(log_prob_words, doc_batch)</span><br><span class=\"line\">            doc_nn_rl = doc_nn_rec_loss(log_nn_prob_words, knn_batch,train_docs)</span><br><span class=\"line\">            kl_loss = tdist.kl_divergence(qdist, norm)</span><br><span class=\"line\">            kl_loss = torch.mean(torch.sum(kl_loss, dim=<span class=\"number\">1</span>))</span><br><span class=\"line\">            loss = doc_rl + doc_nn_rl + kl_loss</span><br><span class=\"line\">            losses.append(loss.item())</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            optim.step()</span><br><span class=\"line\">        avg_loss = np.mean(losses)</span><br><span class=\"line\">        avg_prec = test(train_docs, train_cats, cv_docs, cv_cats, model)</span><br><span class=\"line\">        best_prec = max(avg_prec, best_prec)</span><br><span class=\"line\">        print(<span class=\"string\">f\"Epoch <span class=\"subst\">&#123;e + <span class=\"number\">1</span>&#125;</span>: Avg Loss: <span class=\"subst\">&#123;avg_loss&#125;</span>, Avg Prec: <span class=\"subst\">&#123;avg_prec&#125;</span>\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> best_prec == avg_prec:</span><br><span class=\"line\">            trial = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            trial += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> trial == imp_trial:</span><br><span class=\"line\">                print(<span class=\"string\">f\"Avg Prec could not be improved for <span class=\"subst\">&#123;imp_trial&#125;</span> times, \"</span></span><br><span class=\"line\">                      <span class=\"string\">\"giving up training\"</span>)</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model, best_prec</span><br></pre></td></tr></table></figure>\n<p>没有使用GPU！！！<code>kl_loss = tdist.kl_divergence(qdist, norm)​</code> 计算KL散度。norm 为标准正态分布。</p>\n<p>测试代码：</p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%96%87%E6%9C%AC%E5%93%88%E5%B8%8C/image-20220126214013570.png\" alt=\"image-20220126214013570\"></p>\n<p>这里 k=100，表示近邻文档取100，这里为test进行二进制哈希映射后，根据汉明距离选取距离最近的k个，然后统计这k个中与test标签相同的数目，相同数目越大表示即准确率越大，模型效果越好。</p>\n<blockquote>\n<p>注意事项</p>\n</blockquote>\n<p>在使用该代码时，需要对数据处理成 20ng-all-stemmed.txt文件里的格式。然后用<code>prepare_data.py</code> 处理生成对应的<code>.mat</code> 文件。将源句子与其复述句标记为相同标签。</p>\n<ul>\n<li>固定种子，保证结果可复现。（基本操作）</li>\n<li>计算 idf 时，把代码里的小错误纠正了。（分母加了括号）</li>\n<li>去掉余弦相似度计算，在已知标签的情况下，近邻文档直接从标签相同的文档中取k个。（bm25已经名存实亡，文档向量用TF-IDF值效果差不多）</li>\n<li>k值调整，代码中默认100，论文中说为50的时候准确率不在提升，真的是谜之操作。要根据实际情况而定，看每个源句子对应的复述句子的数量，如果k设置过大，则会引入大量噪声。<code>test</code> 函数中的k要与数据处理中的k保持一致，或者小于。（至关重要，不然准确率上不去，而且低到百分之零点几，k=2时，平均准确率有0.43+）</li>\n<li>改成了可以使用gpu训练的代码。（至少可以快七倍）</li>\n<li>解耦，把训练、测试、模型、数据处理分开。</li>\n</ul>\n<p>开始小数据训练，准确率很低。后面就增加数据，准确率依旧那样。开始以为bm25权重计算错误，然后发现代码中 idf 的计算与公式有出入。然后改正了，接着训练，效果还是不好。然后将两份代码对比，发现作者开源的代码里对KL散度值给了一个权重。然后又加权重值，效果还是那样。训练时开始调整knn-size的值，效果好了一点点，但还是很低很低。然后尝试解耦代码，把各个模块代码重新整理，然后发现<code>test</code> 函数里有个参数 k，默认值100，训练一轮后测试模型时，并没有设置该参数，还是默认100。<code>train_knn</code> 的 k 值过大，则会引入噪声，<code>test</code> 中 k 值过大，造成分母过大，准确率很难上去。</p>\n","categories":[],"tags":["paper reading"]},{"title":"knn-lm","url":"https://hahally.github.io//articles/knn-lm/","content":"<p>论文：<a href=\"https://arxiv.org/abs/1911.00172\" target=\"_blank\" rel=\"noopener\">GENERALIZATION THROUGH MEMORIZATION: NEAREST NEIGHBOR LANGUAGE MODELS</a></p>\n<p>code：<a href=\"https://github.com/urvashik/knnlm\" target=\"_blank\" rel=\"noopener\">knn-lm</a></p>\n<p>参考链接：<a href=\"https://zhuanlan.zhihu.com/p/90890672?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1046686491727724544&amp;utm_campaign=shareopn\" target=\"_blank\" rel=\"noopener\">香侬读 | 用上文K最近邻特征表示增强语言模型 </a></p>\n<p>论文的主要思想是使用传统 <em>knn</em> 算法对预训练神经语言模型进行线性插值扩展。</p>\n<p>ps：传统算法在这个深度学习领域的一次融合……印象中，都是使用预训练模型在小数据集上进行微调，这篇论文似乎有点东西。</p>\n<p>对于语言模型<strong>LM</strong>，给定一个上下文序列tokens：</p>\n<script type=\"math/tex; mode=display\">\nc_t = (w_1,...,w_{t-1})</script><p>自回归语言模型通过建模$p(w_t|c_t)$ 来预测目标词 $w_t$  的概率分布。</p>\n<p>kNN-LM可以在没有任何额外的训练情况下，用最邻近检索机制增强预训练语言模型。在模型预训练后，会对训练集的文本集合进行一次前向传播，任何得到 context-target pairs，并将其以键值对形式存储起来（a key-value datastore），以便在推理过程中查找。</p>\n<p><img src=\"/articles/knn-lm/image-20211109212234007.png\" alt=\"image-20211109212234007\"></p>\n<p>具体的：设语言模型为 f(·)，可以将一个上文 c 映射为固定长度的向量表示。对于给定的第 i 个训练样本$(c_i, w_i)\\in D$ ，定义一个键值对$(k_i, v_i)$ ，$k_i$ 表示上文$c_i$ 的向量表示，$v_i$ 表示目标词$w_i$ ，<strong>datastore </strong>(K, V)表示这样一个集合：</p>\n<script type=\"math/tex; mode=display\">\n(K, V) = \\{(f(c_i), w_i)|(c_i,w_i)\\in D\\}</script><p>在推理阶段，对于给定上文信息 x ，预测 y 概率分布。使用knn算法进行插值，有：</p>\n<script type=\"math/tex; mode=display\">\np(y|x) = \\lambda p_{knn}(y|x) + (1-\\lambda)p_{LM}(y|x)\\\\\np_{knn}(y|x) \\propto \\sum_{(k_i,v_i)\\in N} 1_{y=v_i}exp(-d(k_i,f(x)))\\\\</script><p>$\\lambda$ 表示调谐参数，N表示更具距离得到的k邻近集合。距离计算公式采用欧氏距离（L2范数）。在这里knn只是为了得到集合N。</p>\n<p>当然这种使用knn算法的方法不免存在一些算法本身的缺点。一是距离计算公式的选择，二是查询速度，三是k的选择。对于一个预训练语言模型，需要的语料是巨大的，该方法需要将训练集语料的所有键值对保存下来，便于查询。可想而知，从如此巨大的键值对中获取 k 近邻集合N，其查询代价是相当巨大的！！！</p>\n<p>正因如此，为了knn-lm更好的work，在实现时，使用了FAISS库来加速查询过程。</p>\n<p><strong>一点补充</strong></p>\n<p>原本看完论文后，我就知道这个保存的datastore是很大的，但是我没想到这大的如此离谱！！！</p>\n<p>readme中提到模型训练使用了8块GPU，而且基于是Fairseq的。脑阔疼，对Fairseq本来就没什么好印象。索性他提供了一个checkpoint，可以跳过模型训练部分了。但看到后面生成datastore时，我。。。</p>\n<blockquote>\n<p><strong>Caution</strong>: Running this step requires a large amount of disk space (400GB!). Please read the note about hardware above, before running this!</p>\n</blockquote>\n<p><strong>400GB</strong>的磁盘大小！！！！！真的是离了一个大谱！！！！！！</p>\n<p>现在想想论文摘要里的那句：</p>\n<blockquote>\n<p>our kNN-LM achieves a new state-of-the-art perplexity of 15.79 – a 2.9 point improvement with no additional training.</p>\n</blockquote>\n<p>这让我不得不怀疑，这sota拼的是磁盘大小啊。真的是有点东西，我一个小作坊，GPU都就是白嫖的，现在整个400G磁盘，我也是活久见。</p>\n<p><strong>一个小故事</strong></p>\n<p>我本一介凡人，但是一心向往修仙炼丹之术。早闻各路大神每年都会在修仙圣地 <strong>ICLR</strong> 交流切磋修仙炼丹心得。 一次偶然机会，受高人指点，得到一本秘籍。看完秘籍，豁然开朗，炼丹之路，似乎有了些盼头。</p>\n<p>欣喜之余，我也丝毫不敢懈怠。靠着几年的游历经验，白嫖到了一些炼丹器具，也习得一门奇门遁术python，更是窥得仙术tensorflow和pytorch几分奥秘，python大法从入门到入坑，深度学习从入门到放弃，从删库到跑路，我虽自认为资质平庸，在江湖掀不起大风大浪，却也勤勤恳恳苦心修炼，也是到了初识境界。</p>\n<p>Github，无数修仙能人术士炫技圣地，在这里果然找到了秘籍之中提到的各种原料以及使用说明书（大神们愿称之为 ‘瑞德密’）。</p>\n<p>于是开始每天起早贪黑，备药材，烧丹炉，研究秘籍。按照瑞德密一步一步修炼，但是依旧失败了一次又一次。深感才疏学浅带来的无力，莫不是修为尚浅，无法领略其中奥义。夜不能寐，辗转反侧，我仍百思不得其解。</p>\n<p>偶然间，看到到瑞德密后面部分，再次豁然开朗：</p>\n<blockquote>\n<p>欲修此术修此丹药，需备八个丹炉，外部容器非四百G不可。</p>\n</blockquote>\n<p>感觉像是吃了闭门羹，无数人对修仙炼丹之术趋之若鹜，但真正修得正果的，基本是各大财大气粗的门派的人。而对于资质平凡，财力有限的小作坊而言，这条路似乎走的异常艰辛。曾无数次阅读各路大神秘籍，但因为各种苛刻的修炼条件望而却步。</p>\n","categories":[],"tags":[]},{"title":"一篇论文解读","url":"https://hahally.github.io//articles/论文解读/","content":"<p>那就告一段落吧。</p>\n<blockquote>\n<p> 在平衡内心与周遭的过程中，缝缝补补自己眼中千疮百孔的世界……</p>\n</blockquote>\n<p>很多事都不一样了，在表示同意赞赏nb还行的同时，其实内心也在保持着一些最后的倔强甚至不屑的态度。向往诗和远方的同时，也在吐槽当下糟糕的境况。这是暂时的妥协，而不是最后的结果。</p>\n<p>可以预见的是，有一天，我也会被一块大饼圈住，为别人给的蛋糕沾沾自喜，因为天上掉的馅饼开始信奉神明，在推杯换盏中周旋，吃饱了面包然后驻足休息，养老等死，我的墓志铭大概就是我的第一个”hello world”代码。这是我最后的倔强，而不是暂时的妥协。</p>\n<p>技术无罪，资本作祟的时代，人人都好像鬼怪，争夺面包，吸食人xie，手捧圣经，说着抱歉，最后还不忘总结，口感似乎差了点……</p>\n<p>二十一岁我还在对自己说：<em>管他三七二十一，先做自己想做的事，说自己想说的话，走自己想走的路……</em></p>\n<p>时过境迁，我才意识到，这是原来是叫愤青啊。在深感无力的同时，我也只能长叹一口气（很长很长，用英文就是long long long…）。我还以为我在做自己认为对的事情，我在做自己能做到的事情。</p>\n<p>每一次成长，都是和自己谈判的过程，而每次妥协都是在塑造新的自己。over!!!</p>\n<hr>\n<p>真的不是在吐槽，有认认真真研读！！！</p>\n<p>有关：<code>Integrating Linguistic Knowledge to Sentence Paraphrase Generation</code> 论文解读。</p>\n<blockquote>\n<p>模型框架</p>\n</blockquote>\n<p>典型<code>Transformer-based</code>  结构，编码器和解码器都是多个<code>Multi-head Attention</code> 组成。如图：</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006132242262.png\" alt=\"image-20211006132242262\"></p>\n<p>大概分为三个部分：<code>Sentence Encoder</code>、<code>Paraphrase Decoder</code>、<code>Synonym Labeling</code></p>\n<p>按照论文里的思路，模型训练包含了一个辅助任务即：<code>Synonym Labeling</code> 。先用encoder部分做辅助任务训练模型，然后整体训练做生成任务。但其实也是可以一起训练的。</p>\n<p>下面结合作者开源的代码进行一些分析。</p>\n<blockquote>\n<p>数据处理</p>\n</blockquote>\n<ol>\n<li>第一步</li>\n</ol>\n<p>执行 <code>data_processing.py</code>  脚本，生成一个字典文件 <code>vocab</code> 文件。</p>\n<figure class=\"highlight tex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;pad&gt;</span><br><span class=\"line\">&lt;unk&gt;</span><br><span class=\"line\">&lt;s&gt;</span><br><span class=\"line\">&lt;/s&gt;</span><br><span class=\"line\">的</span><br><span class=\"line\">，</span><br><span class=\"line\">。</span><br><span class=\"line\">···</span><br><span class=\"line\">&lt;eos&gt;</span><br><span class=\"line\">&lt;sos&gt;</span><br></pre></td></tr></table></figure>\n<p>神奇的是首行的<code>&lt;pad&gt;</code> 并不是脚本添加的，需要自己手动添加，这是运行后面程序发现的。而且多出的<code>&lt;eos&gt;,&lt;sos&gt;</code> 也并没有用到。</p>\n<ol>\n<li>第二步</li>\n</ol>\n<p>执行<code>prepro_dict.py</code> 脚本，生成数据集对应的同义词对文件：<code>train_paraphrased_pair.txt</code>、<code>dev_paraphrased_pair.txt</code> 、<code>test_paraphrased_pair.txt</code> 。</p>\n<p><code>train_paraphrased_pair.txt</code> 为例：</p>\n<figure class=\"highlight angelscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">年景-&gt;<span class=\"number\">1</span> 或者-&gt;<span class=\"number\">2</span> 年景-&gt;<span class=\"number\">4</span> 或-&gt;<span class=\"number\">2</span> 抑或-&gt;<span class=\"number\">2</span> 要么-&gt;<span class=\"number\">2</span> 要-&gt;<span class=\"number\">2</span> 抑-&gt;<span class=\"number\">2</span></span><br><span class=\"line\">······</span><br></pre></td></tr></table></figure>\n<p>对应<code>train</code>数据集中的第一行是：</p>\n<figure class=\"highlight basic\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">1929 </span>年 还是 <span class=\"number\">1989</span> 年 ？</span><br></pre></td></tr></table></figure>\n<p>具体对应论文中<code>Synonym Pairs Representation</code> 部分：同义词位置对<code>Synonym-position paris</code> 。</p>\n<p><code>词-&gt;pos</code> ：其中<code>pos</code>表示<code>sentence</code> 中的位置，<code>词</code> 是指同义词，即句子中<code>pos</code>位置上词对应的同义词。<code>年景-&gt;1</code> 中位置<code>1</code>处的词为<code>年</code> ，<code>年景</code> 即是<code>年</code>的同义词。</p>\n<ol>\n<li>总结</li>\n</ol>\n<p>数据处理这一步两个脚本，生成需要的数据文件有：一个词表文件<code>.vocab</code> ，三个同义词位置对文件<code>_paraphrased_pair.txt</code> 。</p>\n<p>整个实验还需要五个数据集文件：train{.src,.tgt}，dev{.src,.tgt}，test{.src}</p>\n<p>最后还想提一下：</p>\n<p>tcnp.train.src 第268178行竟然是空行！！！对应同义词对为：<code>&lt;unk&gt;-&gt;&lt;unk&gt;</code> ！！！<br><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># tcnp.train.src</span></span><br><span class=\"line\"><span class=\"attr\">268179</span> <span class=\"string\">阿富汗 肯定 存在 错误 。</span></span><br><span class=\"line\"><span class=\"attr\">268180</span> <span class=\"string\">在 阿富汗 肯定 有 错误 。</span></span><br><span class=\"line\"><span class=\"attr\">268181</span> <span class=\"string\">事实上 ， 我们 确实 在 阿富汗 犯 了 许多 错误 。</span></span><br><span class=\"line\"><span class=\"comment\"># tcnp.train.tgt</span></span><br><span class=\"line\"><span class=\"attr\">268179</span> <span class=\"string\">事实上 ， 我们 确实 在 阿富汗 犯 了 许多 错误 。</span></span><br><span class=\"line\"><span class=\"attr\">268180</span> <span class=\"string\">阿富汗 肯定 存在 错误 。</span></span><br><span class=\"line\"><span class=\"attr\">268181</span> <span class=\"string\">在 阿富汗 肯定 有 错误 。</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>train 训练</p>\n</blockquote>\n<p>训练部分代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self, xs, ys, x_paraphrased_dict, synonym_label=None)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># forward</span></span><br><span class=\"line\">    memory, sents1 = self.encode(xs)</span><br><span class=\"line\">    _, _, synonym_label_loss = self.labeling(synonym_label, memory)</span><br><span class=\"line\">    logits, preds, y, sents2 = self.decode(ys, x_paraphrased_dict, memory)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># train scheme</span></span><br><span class=\"line\">    <span class=\"comment\"># generation loss</span></span><br><span class=\"line\">    y_ = label_smoothing(tf.one_hot(y, depth=self.hp.vocab_size))</span><br><span class=\"line\">    ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)</span><br><span class=\"line\">    nonpadding = tf.to_float(tf.not_equal(y, self.token2idx[<span class=\"string\">\"&lt;pad&gt;\"</span>]))  <span class=\"comment\"># 0: &lt;pad&gt;</span></span><br><span class=\"line\">    loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + <span class=\"number\">1e-7</span>)</span><br><span class=\"line\">    <span class=\"comment\"># multi task loss</span></span><br><span class=\"line\">    tloss = self.hp.l_alpha * loss + (<span class=\"number\">1.0</span>-self.hp.l_alpha) * synonym_label_loss</span><br><span class=\"line\"></span><br><span class=\"line\">    global_step = tf.train.get_or_create_global_step()</span><br><span class=\"line\">    lr = noam_scheme(self.hp.lr, global_step, self.hp.warmup_steps)</span><br><span class=\"line\">    optimizer = tf.train.AdamOptimizer(lr)</span><br><span class=\"line\">    train_op = optimizer.minimize(tloss, global_step=global_step)</span><br><span class=\"line\"></span><br><span class=\"line\">    tf.summary.scalar(<span class=\"string\">'lr'</span>, lr)</span><br><span class=\"line\">    tf.summary.scalar(<span class=\"string\">\"loss\"</span>, loss)</span><br><span class=\"line\">    tf.summary.scalar(<span class=\"string\">\"tloss\"</span>, tloss)</span><br><span class=\"line\">    tf.summary.scalar(<span class=\"string\">\"global_step\"</span>, global_step)</span><br><span class=\"line\"></span><br><span class=\"line\">    summaries = tf.summary.merge_all()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss, train_op, global_step, summaries</span><br></pre></td></tr></table></figure>\n<p>大概流程就是: <code>encoder -&gt; labeing and decoder</code> 。</p>\n<blockquote>\n<p>Sentence Encoder</p>\n</blockquote>\n<p>输入：</p>\n<ul>\n<li>sentence x token [x1,x2,x3,…,xn]</li>\n</ul>\n<p>输出：</p>\n<ul>\n<li>memory： 经过多个Multi-head Attention后的输出</li>\n</ul>\n<p>和常规的<code>transformer encoder</code>一样。先是对句子<code>token</code>向量进行<code>embedding</code>，然后添加位置编码<code>positional_encoding</code>。</p>\n<p>对应代码在：<code>model.py</code> ：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encode</span><span class=\"params\">(self, xs, training=True)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> tf.variable_scope(<span class=\"string\">\"encoder\"</span>, reuse=tf.AUTO_REUSE):</span><br><span class=\"line\">        x, seqlens, sents1 = xs</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># embedding</span></span><br><span class=\"line\">        enc = tf.nn.embedding_lookup(self.embeddings, x) <span class=\"comment\"># (N, T1, d_model)</span></span><br><span class=\"line\">        enc *= self.hp.d_model**<span class=\"number\">0.5</span> <span class=\"comment\"># scale</span></span><br><span class=\"line\"></span><br><span class=\"line\">        enc += positional_encoding(enc, self.hp.maxlen1)</span><br><span class=\"line\">        enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">## Blocks</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.hp.num_blocks):</span><br><span class=\"line\">            <span class=\"keyword\">with</span> tf.variable_scope(<span class=\"string\">\"num_blocks_&#123;&#125;\"</span>.format(i), reuse=tf.AUTO_REUSE):</span><br><span class=\"line\">                <span class=\"comment\"># self-attention</span></span><br><span class=\"line\">                enc = multihead_attention(queries=enc,</span><br><span class=\"line\">                                          keys=enc,</span><br><span class=\"line\">                                          values=enc,</span><br><span class=\"line\">                                          num_heads=self.hp.num_heads,</span><br><span class=\"line\">                                          dropout_rate=self.hp.dropout_rate,</span><br><span class=\"line\">                                          training=training,</span><br><span class=\"line\">                                          causality=<span class=\"literal\">False</span>)</span><br><span class=\"line\">                <span class=\"comment\"># feed forward</span></span><br><span class=\"line\">                enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])</span><br><span class=\"line\">    memory = enc</span><br><span class=\"line\">    <span class=\"keyword\">return</span> memory, sents1</span><br></pre></td></tr></table></figure>\n<p>不过有趣的是论文中关于<code>Encoder</code>的部分貌似有些问题：</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006182655905.png\" alt=\"image-20211006182655905\"></p>\n<p>这是论文中的式子。在transformer中是这样的：<code>Block(Q,K,V) = LNorm(FFN(m)+m)、m=LNorm(MultiAttn(Q,K,V)+Q)</code> 。先add再norm啊！！！</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006183417409.png\" alt=\"image-20211006183417409\"></p>\n<p>不知道是不是排版错误的原因。主要作者开源的代码里<code>multihead_attention</code>部分还有<code>ffn</code> 部分是先add再norm的。</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006184128806.png\" alt=\"image-20211006184128806\"></p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006184216337.png\" alt=\"image-20211006184216337\"></p>\n<p>属实给整蒙了。</p>\n<blockquote>\n<p>Paraphrase Decoder</p>\n</blockquote>\n<p>输入：</p>\n<ul>\n<li>sentence y token [y1,y2,y3,…,ym] 对应tgt中的句子的token</li>\n<li>x_paraphrased_dict 引入的外部知识，也就是同义词位置对：<code>synonyms-\nposition pairs</code></li>\n<li>memory: 编码器的输出</li>\n</ul>\n<p>输出：</p>\n<ul>\n<li>logits, y_hat： <code>logits</code>是最后一层的输出，<code>y_hat</code>是预测值 </li>\n</ul>\n<p>这部分有self-attention 、vanilla attention、paraphrased dictionary attention。</p>\n<p>self-attention部分和原本的transformer中一样，key=value=query。vanilla attention其实就是key与value相同，query不一样。paraphrased dictionary attention就是引入外部同义词字典知识的部分。这部分主要是计算论文中<code>ct</code> ，按论文中公式来即可。</p>\n<p>对应代码在：<code>model.py</code> ：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decode</span><span class=\"params\">(self, ys, x_paraphrased_dict, memory, training=True)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">with</span> tf.variable_scope(<span class=\"string\">\"decoder\"</span>, reuse=tf.AUTO_REUSE):</span><br><span class=\"line\">            decoder_inputs, y, seqlens, sents2 = ys</span><br><span class=\"line\">            x_paraphrased_dict, paraphrased_lens, paraphrased_sents = x_paraphrased_dict</span><br><span class=\"line\">            <span class=\"comment\"># embedding</span></span><br><span class=\"line\">            dec = tf.nn.embedding_lookup(self.embeddings, decoder_inputs)  <span class=\"comment\"># (N, T2, d_model)</span></span><br><span class=\"line\">            dec *= self.hp.d_model ** <span class=\"number\">0.5</span>  <span class=\"comment\"># scale</span></span><br><span class=\"line\"></span><br><span class=\"line\">            dec += positional_encoding(dec, self.hp.maxlen2)</span><br><span class=\"line\">            dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)</span><br><span class=\"line\"></span><br><span class=\"line\">            batch_size = tf.shape(decoder_inputs)[<span class=\"number\">0</span>] <span class=\"comment\"># (N, T2, 2)</span></span><br><span class=\"line\">            seqlens = tf.shape(decoder_inputs)[<span class=\"number\">1</span>]  <span class=\"comment\"># (N, T2, 2)</span></span><br><span class=\"line\">            paraphrased_lens = tf.shape(x_paraphrased_dict)[<span class=\"number\">1</span>]  <span class=\"comment\"># (N, T2, 2)</span></span><br><span class=\"line\"></span><br><span class=\"line\">            x_paraphrased_o, x_paraphrased_p = x_paraphrased_dict[:,:,<span class=\"number\">0</span>], x_paraphrased_dict[:,:,<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">            x_paraphrased_o_embedding = tf.nn.embedding_lookup(self.embeddings, x_paraphrased_o)  <span class=\"comment\"># N, W2, d_model</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.hp.paraphrase_type == <span class=\"number\">0</span>:</span><br><span class=\"line\">                x_paraphrased_p_embedding = tf.nn.embedding_lookup(self.embeddings, x_paraphrased_p)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                x_paraphrased_p_embedding = paraphrased_positional_encoding(x_paraphrased_p, self.hp.maxlen2, self.hp.d_model)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Blocks</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.hp.num_blocks):</span><br><span class=\"line\">                <span class=\"keyword\">with</span> tf.variable_scope(<span class=\"string\">\"num_blocks_&#123;&#125;\"</span>.format(i), reuse=tf.AUTO_REUSE):</span><br><span class=\"line\">                    <span class=\"comment\"># Masked self-attention (Note that causality is True at this time)</span></span><br><span class=\"line\">                    dec = multihead_attention(queries=dec,</span><br><span class=\"line\">                                              keys=dec,</span><br><span class=\"line\">                                              values=dec,</span><br><span class=\"line\">                                              num_heads=self.hp.num_heads,</span><br><span class=\"line\">                                              dropout_rate=self.hp.dropout_rate,</span><br><span class=\"line\">                                              training=training,</span><br><span class=\"line\">                                              causality=<span class=\"literal\">True</span>,</span><br><span class=\"line\">                                              scope=<span class=\"string\">\"self_attention\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"comment\"># Vanilla attention</span></span><br><span class=\"line\">                    dec = multihead_attention(queries=dec,</span><br><span class=\"line\">                                              keys=memory,</span><br><span class=\"line\">                                              values=memory,</span><br><span class=\"line\">                                              num_heads=self.hp.num_heads,</span><br><span class=\"line\">                                              dropout_rate=self.hp.dropout_rate,</span><br><span class=\"line\">                                              training=training,</span><br><span class=\"line\">                                              causality=<span class=\"literal\">False</span>,</span><br><span class=\"line\">                                              scope=<span class=\"string\">\"vanilla_attention\"</span>)</span><br><span class=\"line\">                    <span class=\"comment\">### Feed Forward</span></span><br><span class=\"line\">                    dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># add paraphrased dictionary attention</span></span><br><span class=\"line\">            h = tf.fill([batch_size, seqlens, paraphrased_lens, self.hp.d_model], <span class=\"number\">1.0</span>) * tf.expand_dims(dec, axis=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            o_embeding = tf.fill([batch_size, seqlens, paraphrased_lens, self.hp.d_model], <span class=\"number\">1.0</span>) * tf.expand_dims(x_paraphrased_o_embedding, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">            W_a_o = tf.get_variable(<span class=\"string\">\"original_word_parameter_w\"</span>, [<span class=\"number\">2</span>*self.hp.d_model], initializer=tf.initializers.random_normal(</span><br><span class=\"line\">          stddev=<span class=\"number\">0.01</span>, seed=<span class=\"literal\">None</span>))</span><br><span class=\"line\">            V_a_o = tf.get_variable(<span class=\"string\">\"original_word_parameter_v\"</span>, [<span class=\"number\">2</span>*self.hp.d_model], initializer=tf.initializers.random_normal(</span><br><span class=\"line\">          stddev=<span class=\"number\">0.01</span>, seed=<span class=\"literal\">None</span>))</span><br><span class=\"line\">            h_o_concat = tf.concat([h, o_embeding], <span class=\"number\">-1</span>) <span class=\"comment\"># N, T2, W2, 2*d_model</span></span><br><span class=\"line\">            score_tem_o = tf.tanh(W_a_o * h_o_concat) <span class=\"comment\"># N, T2, W2, 2*d_model</span></span><br><span class=\"line\">            score_o = tf.reduce_sum(V_a_o * score_tem_o, axis=<span class=\"number\">-1</span>) <span class=\"comment\"># N, T2, W2</span></span><br><span class=\"line\">            a = tf.nn.softmax(score_o) <span class=\"comment\"># N, T2, W2</span></span><br><span class=\"line\">            c_o = tf.matmul(a, x_paraphrased_o_embedding) <span class=\"comment\"># (N, T2, W2) * (N, W2, d_model) --&gt; N, T2, d_model</span></span><br><span class=\"line\"></span><br><span class=\"line\">            p_embeding = tf.fill([batch_size, seqlens, paraphrased_lens, self.hp.d_model], <span class=\"number\">1.0</span>) * tf.expand_dims(x_paraphrased_p_embedding, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">            W_a_p = tf.get_variable(<span class=\"string\">\"paraphrased_word_parameter_w\"</span>, [<span class=\"number\">2</span>*self.hp.d_model], initializer=tf.initializers.random_normal(</span><br><span class=\"line\">          stddev=<span class=\"number\">0.01</span>, seed=<span class=\"literal\">None</span>))</span><br><span class=\"line\">            V_a_p = tf.get_variable(<span class=\"string\">\"paraphrased_word_parameter_v\"</span>, [<span class=\"number\">2</span>*self.hp.d_model], initializer=tf.initializers.random_normal(</span><br><span class=\"line\">          stddev=<span class=\"number\">0.01</span>, seed=<span class=\"literal\">None</span>))</span><br><span class=\"line\">            h_p_concat = tf.concat([h, p_embeding], <span class=\"number\">-1</span>) <span class=\"comment\"># N, T2, W2, 2*d_model</span></span><br><span class=\"line\">            score_tem_p = tf.tanh(W_a_p * h_p_concat) <span class=\"comment\"># N, T2, W2, 2*d_model</span></span><br><span class=\"line\">            score_p = tf.reduce_sum(V_a_p * score_tem_p, axis=<span class=\"number\">-1</span>) <span class=\"comment\"># N, T2, W2</span></span><br><span class=\"line\">            a = tf.nn.softmax(score_p) <span class=\"comment\"># N, T2, W2</span></span><br><span class=\"line\">            c_p = tf.matmul(a, x_paraphrased_p_embedding) <span class=\"comment\"># (N, T2, W2) * (N, W2, d_model) --&gt; N, T2, d_model</span></span><br><span class=\"line\"></span><br><span class=\"line\">            c_t = tf.concat([c_o, c_p], axis=<span class=\"number\">-1</span>) <span class=\"comment\"># N, T2, d_model --&gt; N, T2, 2*d_model</span></span><br><span class=\"line\">            out_dec = tf.layers.dense(tf.concat([dec, c_t], axis=<span class=\"number\">-1</span>), self.hp.d_model, activation=tf.tanh, use_bias=<span class=\"literal\">False</span>, kernel_initializer=tf.initializers.random_normal(</span><br><span class=\"line\">          stddev=<span class=\"number\">0.01</span>, seed=<span class=\"literal\">None</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Final linear projection (embedding weights are shared)</span></span><br><span class=\"line\">        weights = tf.transpose(self.embeddings) <span class=\"comment\"># (d_model, vocab_size)</span></span><br><span class=\"line\">        logits = tf.einsum(<span class=\"string\">'ntd,dk-&gt;ntk'</span>, out_dec, weights) <span class=\"comment\"># (N, T2, vocab_size)</span></span><br><span class=\"line\">        y_hat = tf.to_int32(tf.argmax(logits, axis=<span class=\"number\">-1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> logits, y_hat, y, sents2</span><br></pre></td></tr></table></figure>\n<p>在最后的输出层部分:论文中的 <code>softmax layer</code>:</p>\n<script type=\"math/tex; mode=display\">\ny_t = softmax(W_yConcat[y_t^*,c_t]))</script><p>代码中很巧妙的使用 <code>tf.transpose(self.embeddings)</code> 来表示<code>Wy</code> 从而将输出映射到vocab输出。 </p>\n<p>这里需要注意的是<code>x_paraphrased_dict</code>的表示。论文中叫做<code>synonyms-\nposition pairs</code>,使用<code>P</code> 表示。</p>\n<script type=\"math/tex; mode=display\">\nP = {(si,pi)}_{i=1}^M</script><p><code>si</code> 表示同义词，<code>pi</code> 表示同义词对应sentence中的位置。训练时，会将<code>si</code> 进行<code>embedding</code> ，<code>pi</code> 进行位置编码<code>positional_encoding</code> 。这里的<code>embedding</code> 与<code>positional_encoding</code> 和<code>encoder</code>部分共享。</p>\n<blockquote>\n<p><code>Synonym Labeling</code> </p>\n</blockquote>\n<p>输入：</p>\n<ul>\n<li>synonym_label：同义词标签</li>\n<li>memory： encoder的输出</li>\n</ul>\n<p>输出：</p>\n<ul>\n<li>logits, y_hat, loss: 一个全连接层的输出、一个预测值、一个损失</li>\n</ul>\n<p>synonym_label：[True,False,…]，对于给定的一个句子，如果句中词对应位置有同义词这对应label为True,否者为False。这一部分的loss对应论文中的loss2。</p>\n<p>这是一个辅助任务，目的是确定给定句子中每个词是否有对应的同义词。有助于更好地定位同义词的位置，结合短语和同义词在原句中的语言关系。可以肯定的是，这是一个二分类任务，并且两个任务共用一个encoder。描述如下：</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006192852063.png\" alt=\"image-20211006192852063\"></p>\n<p>对应代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">labeling</span><span class=\"params\">(self, x, menmory)</span>:</span></span><br><span class=\"line\">    synonym_label, seqlens, sents1 = x</span><br><span class=\"line\">    logits = tf.layers.dense(menmory, <span class=\"number\">2</span>, activation=tf.tanh, use_bias=<span class=\"literal\">False</span>,</span><br><span class=\"line\">                              kernel_initializer=tf.initializers.random_normal(stddev=<span class=\"number\">0.01</span>, seed=<span class=\"literal\">None</span>))</span><br><span class=\"line\">    y_hat = tf.to_int32(tf.argmax(logits, axis=<span class=\"number\">-1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Synonym Labeling loss</span></span><br><span class=\"line\">    y = tf.one_hot(synonym_label, depth=<span class=\"number\">2</span>)</span><br><span class=\"line\">    ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y)</span><br><span class=\"line\">    nonpadding = tf.to_float(tf.not_equal(sents1, self.token2idx[<span class=\"string\">\"&lt;pad&gt;\"</span>]))  <span class=\"comment\"># 0: &lt;pad&gt;</span></span><br><span class=\"line\">    loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + <span class=\"number\">1e-7</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> logits, y_hat, loss</span><br></pre></td></tr></table></figure>\n<p>需要注意的是代码中还有<code>def train_labeling(self, xs, synonym_label=None):</code> 的地方。按照论文中的描述：</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006194032511.png\" alt=\"image-20211006194032511\"></p>\n<p>这个函数是用来单独做<code>Synonym Labeling</code> 任务的。按论文中的原意，应该是先<code>model.train_labeling(xs, synonym_label)</code> 然后 在进行<code>model.train(xs, ys, x_paraphrased_dict, synonym_label)</code>。</p>\n<p>但是<code>train.py</code> 代码中并没有这样做。而是直接进行<code>train</code> 。</p>\n<blockquote>\n<p>有关细节问题</p>\n</blockquote>\n<ol>\n<li>vocab</li>\n</ol>\n<p>前面提到过，生成的词表中多出<code>&lt;sos&gt;,&lt;eos&gt;</code> 两个没有用到的词，少了用于填充的<code>&lt;pad&gt;</code>，并且需要自己在首行手动插入<code>&lt;pad&gt;</code> 。猜测可能是课题组的祖传代码。</p>\n<p>而且在生成同义词位置对的文件的代码那里，将不在vocab中的同义词统统过滤掉了。</p>\n<ol>\n<li>Synonym Pairs Representation</li>\n</ol>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006200116402.png\" alt=\"image-20211006200116402\"></p>\n<p>论文里提到如果对应同义词是一个短语，那么就将短语中词嵌入向量求和来表示该短语的向量表示。但是！！！有意思的是，代码中并未体现。而细挖他的数据会发现，给定的数据已是分好词的按空格分隔的。而且是中文数据。中文分好的词，如果是短语，分词后，还是表示一个词。而英文如：abandon同义词give up。give up分词后就是两个单词。也就出现上述情况。</p>\n<p>因此，得出结论，作者开源的代码是不完整的。如果换成英文的数据，那么需要考虑的复杂一些了。当然也可以选择把短语同义词过滤掉，那么和中文上处理就是一样的了。</p>\n<ol>\n<li>paraphrase_type</li>\n</ol>\n<p>paraphrase_type这个是代码中的一个配置参数，默认为1。</p>\n<p><code>parser.add_argument(&#39;--paraphrase_type&#39;, default=1, type=int)</code></p>\n<p>这是所有参数中为数不多没有help提示信息的参数。并且我相信这也是唯一一个没有help整不明白的参数。释义类型？</p>\n<p>在<code>data_load.py</code> 中找到了蛛丝马迹：</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006202356732.png\" alt=\"image-20211006202356732\"></p>\n<p>而这一段代码，也属实有些魔幻。</p>\n<p>首先<code>parser.add_argument(&#39;--paraphrase_type&#39;, default=1, type=int)</code> 这里使用的是1。而取值为1时，对应代码中esle：部分。不用很仔细就可以看出：0时，使用word_set，1时，使用pos_set。我们在观察后面的<code>synonym_label</code> 那一句：</p>\n<p><code>synonym_label = [i in word_set if paraphrase_type else w in word_set for i, w in enumerate(in_words)]</code></p>\n<p>码字到这里，我掐了以下人中，方才缓过神来。接着分析，为0的情况下，那么考虑<code>w in word_set</code> 的真值情况。而<code>for i, w in enumerate(in_words)</code> ，w属于in_words。word_set是什么呢？<code>word_set.add(tem1)</code> 哇哦，是同义词集合诶，in_words是什么？<code>in_words=sent1.split()+[&#39;&lt;/s&gt;&#39;]</code> 欸，那w难道不是don’t exit in word_set forever？</p>\n<p>离谱的是，这里讨论的是paraphrase_type = 1的情况，也就是关word_set屁事的情况。word_set这时都是空的。所以synonym_label 难道不是<em>always be False</em> 。</p>\n<p><code>Are you kidding me? %$*#@&gt;?*&amp;。。。。</code> </p>\n<p>而且<code>x_paraphrase_dic</code> 那里也是有问题的。</p>\n<p><code>x_paraphrase_dict.append([token2idx.get(tem1, token2idx[&quot;&lt;unk&gt;&quot;]), token2idx.get(tem2, token2idx[&quot;&lt;unk&gt;&quot;])])</code></p>\n<p>这个<code>token2idx.get(tem2, token2idx[&quot;&lt;unk&gt;&quot;])</code> 就很有问题，tem2表示的是pos啊，句子中词的位置，直接给我<code>token2idx</code> 我是无法理解的。</p>\n<p><code>x_paraphrase_dict.append([token2idx.get(tem1, token2idx[&quot;&lt;unk&gt;&quot;]), int(tem2 if tem2!=&quot;&lt;unk&gt;&quot; else 0)])</code></p>\n<p><code>int(tem2 if tem2!=&quot;&lt;unk&gt;&quot; else 0)</code> 也就是说tem2为<code>&lt;unk&gt;</code> 时，大概就是tem2取0的意思。而tem2出现<code>&lt;unk&gt;</code> 的情况时，tem1也是<code>&lt;unk&gt;</code> 。此时x_paraphrase_dict添加的就是<code>[1, 0]</code> （token2idx[“<unk>“] = 1）。举个例子：</unk></p>\n<figure class=\"highlight llvm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">句子<span class=\"keyword\">x</span>: <span class=\"keyword\">x</span><span class=\"number\">1</span> <span class=\"keyword\">x</span><span class=\"number\">2</span> <span class=\"keyword\">x</span><span class=\"number\">3</span> <span class=\"keyword\">x</span><span class=\"number\">4</span> 。</span><br><span class=\"line\">对应同义词位置对： &lt;unk&gt;-&gt;&lt;unk&gt;</span><br></pre></td></tr></table></figure>\n<p>这表示这个句子中没有词含有同义词。这个时候x_paraphrase_dict添加<code>[1, 0]</code>，就相当于<code>&lt;unk&gt;</code> 为 x1的同义词。这怎么可能？<em>It’s impossible！！！</em>  而且就很不<em>reasonable</em> 。简直离谱！！！离了个大谱。</p>\n<p>甚至这段代码的第二个for循环后面的那部分代码逻辑都是有问题的。synonym labeling 任务我认为是有问题的。而将<code>&lt;unk&gt;</code> 与位置0处单词绑定，本身就引入了一些噪声，甚至可能增加<code>&lt;unk&gt;</code> 释义的潜在可能性。至于模型能work，我想，synonym labeling本身作为辅助任务，其loss权值占比为0.1，影响应该是很小的。</p>\n<p>这里大胆揣测以下paraphrase_type 意图：</p>\n<ul>\n<li>paraphrase_type  = 0时：x_paraphrase_dict包含句子中的词以及对应同义词，不含位置信息。</li>\n<li>paraphrase_type = 1 时： x_paraphrase_dict其实包含同义词以及对应位置pos。</li>\n</ul>\n<p>无论哪一种情况，其实都是在做一件事：就是将词与其对应的同义词之间进行绑定。第一种更像是比较直接的方式，第二种则略显委婉点。殊途同归！！！</p>\n<p>不管是不是这两种情况，其实那块代码都是有问题的。</p>\n<ol>\n<li>有关 paddings</li>\n</ol>\n<p><code>data_load.py</code> 中有段这样的代码，用来获取一个batch size 数据的dataset函数：</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211007104255666.png\" alt=\"image-20211007104255666\"></p>\n<p>其中关于<code>paddings</code> 处的地方自认为还是有些不妥的，对于src、tgt句子进行0填充是正常操作，但是对于<code>x_paraphrased_dict</code>也进行0填充是欠考虑的。</p>\n<p>对于<code>x_paraphrased_dict</code> ，0填充，就会出现一部分<code>[[0,0],[0,0],…]</code>的情况 默认将[pad]字符与位置0的词对应了。</p>\n<blockquote>\n<p>总结</p>\n</blockquote>\n<p>这篇论文做的是: <strong>Sentence Paraphrase Generation</strong> 。</p>\n<p>其中真正核心地方在于 <strong>Knowledge-Enhanced</strong> ,知识增强。主要就是通过引入外部信息（这里是同义词字典信息）来指导模型生成更加多样性的句子。关于知识增强的方式还是有很多的，这篇论文采用的应该是词表注意力机制，得到外部信息特征表示<strong>ct</strong> 。 </p>\n<p>代码开源了，貌似有没有完全开源！！！</p>\n<p>开源的代码基于python3.6 + tensorflow-gpu==1.12。调试起来真的好麻烦。看不惯tf1.x的代码风格，然后用tf2.x复现了下。</p>\n<ul>\n<li>对于 paraphrase_type 的两种情况按上述理解做了调整。</li>\n<li>对<code>&lt;unk&gt;</code> 匹配第一个单词的情况进行纠正，将<code>tme2 = &lt;unk&gt;</code> 时(句子中没有词存在同义词的情况)，用第一个词与第一个词匹配。即将x1的同义词匹配为x1，这样还是比较妥当的。</li>\n<li>过滤了空行，减少不必要的噪声。（空行对应的同义词对为 \\<unk\\>-&gt;\\<unk\\>）</unk\\></unk\\></li>\n<li>synonym_label中使用2进行padding。训练时，是需要对其进行padding的保证输入的数据工整的。比如句子idx会使用0进行填充直到maxlen，而idx=0对应词为 <code>&lt;pad&gt;</code> 。显然，<code>&lt;pad&gt;</code> 和其他词一样，是一个单独的类别了。所以，为了区分，synonym_label的padding_value设置为2。最后做成一个三分类任务。无伤大雅，主要是为了适配句子的填充。</li>\n<li>为了适应<code>x_paraphrased_dict</code> 的0填充，对输入句子src的首位置引入一个填充符<code>&lt;pad&gt;</code> 。这一点与第二点先呼应。</li>\n</ul>\n<p>不知道效果如何，小作坊，资源有限，训练完要很久很久。敬佩所有敢于开源代码的科研人员，也希望所有开源代码可读性越来越好吧。也希望所有开源代码都能复现结果。至少把种子固定了吧！！！</p>\n<p>————————————–——–———-———10月16日更——————-–—-———————————————</p>\n<blockquote>\n<p>一点思考</p>\n</blockquote>\n<p>兜兜转转，模型训练了好几遍，从训练指标来看，loss有下降，acc有升高，但是推理的时候，预测的起始符号\\<s>后一个词总是结束符\\</s> 。debug无数遍，优化了一些细节上的小问题，还是出现那样的情况。最后将问题锁定在了<strong>padding_mask</strong> 和<strong>look_ahead_mask</strong> 上，其实最可能猜到就是<strong>look_ahead_mask</strong> 有问题。此处的mask都是0和1填充的，代码中使用了<code>tf.keras.layers.MultiHeadAttention</code> 接口，对于0、1矩阵的mask，在里面并没有进行<code>mask*-1e9</code> 的掩码操作，这也导致了训练时出现了数据穿越/泄露问题。所以在推理时，输入的起始字符进行预测时不能得到正确结果，至于为什么是结束符，可能是起止符在词表中相邻的缘故。</p>\n<p>今天改好后，重新训练，十多个小时过去了，还没训练完（3090，24G显存）。</p>\n<p>如今的顶会基本被财大气粗的大公司大实验室的团队承包，小作坊式实验室夹缝求生。顶会期刊也不乏滥竽充数者，实验结果复现难，开源名存实亡……等等一系列骚操作。现有大环境下，一言难尽。</p>\n<p>————————————–——–———-———10月17日更——————-–—-———————————————</p>\n<p>开完组会，被老师叫停，没有硬件资源，也只好先放弃，把其他事情提上日程。</p>\n<p><img src=\"/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006221538257.png\" alt=\"image-20211006221538257\"></p>\n","categories":[],"tags":[]},{"title":"TS-Transformer","url":"https://hahally.github.io//articles/TS-Transformer/","content":"<blockquote>\n<p>前言</p>\n</blockquote>\n<p>最近，老师让复现几篇论文中的方法。打开一篇有关<code>cnn</code> 的论文，初略一看，这个模型结构不就是<code>textcnn</code> 吗？！论文中改头换面变成了<code>LS-CNN</code>，着实有些摸不着头脑。那就仔细看看模型说明吧，看看到底有什么神奇之处。</p>\n<p>十多分钟后······，大概懂了，<code>LS-CNN = TextCNN(w*stack(A,B))</code>  。A、B分别表示layer embedding特征、Google word2vec 词向量特征，*表示卷积，stack表示堆叠（两个大小维度相同的矩阵，堆叠后，通道变成2），通过一维卷积操作进行降维（融合两个嵌入特征）。</p>\n<blockquote>\n<p>I know nothing but my ignorance……</p>\n</blockquote>\n<p>2017年谷歌一篇<code>Attention is all you need</code> 在自然语言处理领域炸开了锅。此后<code>transformer</code> 成为了许多人发paper密码 。之后的<code>bert</code> 更是在各大nlp任务上霸榜。各种魔改层出不求。至此，如果不了解<code>transfomer</code> ，不会微调<code>bert</code> 都不好意思说自己是一个 <code>nlper</code> 。不仅如此，隔壁的<code>cv</code>圈都要沾一下光（<code>VIT</code>）。要我说以后投稿就喊一句：<em>哦斯，喊出我的名字吧！transformer.</em> 或者 <em>构筑未来，希望之光，特利迦，transformer type/bert type</em> 。颇有一股新生代奥特曼借力量的趣味（滑稽）。</p>\n<p>距离<code>transformer</code>发布已经过去4年，这一波热潮何时褪去，或者下一次革命性的模型什么时候出现，这似乎很难预测。<code>self-attention</code> 的尽头是什么？在这急功近利的时代，各大<code>AI Lab</code> 又有几个愿意沉下心来思考研究呢？毕竟资本家只在乎短期能不能变现。</p>\n<p>有意思的是，<code>transformer</code> 又名变形金刚，这也预示这它花里胡哨的各式变形成为可能。</p>\n<blockquote>\n<p>方兴未艾</p>\n</blockquote>\n<p>基于自己有限的认知，随便瞎扯了一下。</p>\n<p>回归正题，自然语言处理技术在其他领域的应用正在悄悄进行中，就像开头提到的那个团队所做的工作一样。仔细一想，他们似乎也是在填充这一块空白，为后继者提供一个新的基线，这是有利于领域发展的。这是一个十分优秀的团队，有责任有担当。</p>\n<p>而作为新入行者的我或者其他人，应该也是倍感压力的。眼下借助自然语言处理技术发光发热的路子似乎并没有那么简单了。</p>\n<blockquote>\n<p>班门弄斧</p>\n</blockquote>\n<p>所以，在此，不妨大胆预测一下，他们接下来会不会对<code>transformer</code> 那一大家子动手呢，又或者另辟蹊径采用<code>GNN(GCN)</code> 来建模呢？这两种可能性还是很大的。</p>\n<p>哈哈哈哈哈哈。在这里挖个坑，献丑提名个 <code>TS-Transformer</code> 来做隐写分析。</p>\n<p>采用<code>Transformer</code> 的<code>encoder</code> 部分提取句子中词与词之间的关系特征和甚至句子的语义特征，然后进行<code>max-pool</code>及<code>avg-pool</code>，然后<code>concat</code> 两个pool特征进行融合，在通过最后全连接进行分类。当然对于词嵌入向量也使用两种embedding，即<code>word2vec</code>和<code>layer embedding</code> 。基于此实现的<code>TS-Transformer</code> 已经在训练了。事实证明这是可以work的。至于效果，留个悬念，暂不公布，代码暂不开源（就图一乐，/滑稽.jpg）。</p>\n<p>【后续补个模型图】</p>\n<p>【后续补个实验结果】</p>\n<p>似乎使用大规模预训练<code>bert</code>模型来代替<code>word2vec</code> 效果应该更好吧。毕竟<code>word2vec</code> 还是属于浅层特征表示吧。【又挖个坑】</p>\n<p>按照这个路子，<code>TS-bert、TS-GNN、TS-GCN......</code> 都是可能work的。</p>\n<blockquote>\n<p>有空在更</p>\n</blockquote>\n<p>然后……中秋放假了。</p>\n<p>哦斯，喊出我的名字吧！<code>TS-Transformer</code> 。构筑未来，希望之光，<code>transformer</code>，<code>TS type</code> 。</p>\n<p>【高开低走的特特利迦竟然试图让泽塔串场来拯救低迷的收视率以及低到可怜的评分，笑死】</p>\n","categories":[],"tags":[]},{"title":"当我遇到tensorflow2.x时","url":"https://hahally.github.io//articles/当我遇到tensorflow2-x时/","content":"<blockquote>\n<p>前言</p>\n</blockquote>\n<p>近日，使用tensorflow的频率比较高，使用过程中也是遇到了一些大大小小的问题。有些着实让人脑瓜子疼。此刻借着模型训练的时间，开始码码字。本来标题想取：</p>\n<ul>\n<li>什么？2021了，还有人用Tensorflow?</li>\n<li>震惊！代码练习生竟然在用······Tensorflow?</li>\n<li>我和tensorflow不共戴天</li>\n<li>我想给tensorflow来一大嘴巴子</li>\n<li>······</li>\n</ul>\n<p>最后，用了这个<code>当我遇到tensorflow2.x时</code> 。无论学习还是生活中，我们都会遇到各种各样的人或事或物。当我们遇到时，会发生什么？我们是会充满期待的。<code>当我遇到···时，我会···</code> 。这个句式是我喜欢的，大部分人习惯在前半部分大胆设想，后半句夸下豪言壮语。这里取前半句，是因为已经发生了，而省去后半句，恰恰是因为豪言壮语很容易翻车。</p>\n<p>这是一篇记录使用tensorflow过程中遇到的一些小而折磨人的问题的博文。但我预言这也将是一篇持久的对tensorflow的血泪吐槽文。</p>\n<blockquote>\n<p>如何看待Keras正式从TensorFlow中分离？</p>\n</blockquote>\n<p>不知道为什么想到了这个知乎话题。六月份的某天，Keras 之父 Francois Chollet宣布将 Keras 的代码从 TensorFlow 代码库中分离出来，移回到了自己的 repo。乍一看，还以为以后tensorflow的keras接口用不了了。但人家只是把keras代码搬回了属于自己的repo。原本的<code>tf.keras</code> 还是能用的。</p>\n<blockquote>\n<p>For you as a user, absolutely nothing changes, now or in the future.</p>\n</blockquote>\n<p>底下全是一片叫好，天下苦tensorflow久已。而我也并不看好这对情侣或者说组合。各自单飞，独自美丽不好吗？keras何必委曲求全做别人的嫁衣。</p>\n<blockquote>\n<p>抛开keras，tensorflow还剩什么？</p>\n</blockquote>\n<p>我想这应该是吐槽后，该冷静思考的问题。而回答这个问题，是需要去阅读官方文档以及实践的。所以，那个句式的后半句也可以是下面的记录。才疏学浅，当厚积薄发。</p>\n<p>言归正传，之后遇到的bug都记录在下面部分。</p>\n<p>—————————————–———-—-————分割线——————-—————————————————–—</p>\n<blockquote>\n<p><code>tf.config.run_functions_eagerly(True)</code></p>\n</blockquote>\n<p>有关<code>Eager Execution</code> <a href=\"https://www.tensorflow.org/guide/eager\" target=\"_blank\" rel=\"noopener\">戳这里</a> </p>\n<p>然后以下是我粗俗的理解：</p>\n<p>这是即时运行和计算图运行相关的概念。即时运行可以让你的程序立马返回结果，计算图运行会先构建计算图（记录你的程序执行行为及顺序），在最后按照构建的图进行计算。</p>\n<p>有些晦涩难理解。</p>\n<p>模型训练时一般有：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@tf.function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_step</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> tf.GradientTape() <span class=\"keyword\">as</span> tape:</span><br><span class=\"line\">        ···train model code···</span><br></pre></td></tr></table></figure>\n<p>这在模型训练过程中是会构建计算图的（具体参考<a href=\"https://www.tensorflow.org/guide/intro_to_graphs\" target=\"_blank\" rel=\"noopener\">戳这里</a>），构建计算图可以，这时如果在代码中<code>print(x)</code> 一下，就会发现这是没有具体值的，而且没有<code>.numpy()</code> 属性。返回的即<code>计算图中节点的符号句柄</code> 。所以我为什么要在这里<code>print</code>呢？当然是为了调试代码(/滑稽.jpg)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Tensor(<span class=\"string\">\"x:0\"</span>, shape=(<span class=\"number\">32</span>, <span class=\"number\">32</span>), dtype=int32)</span><br></pre></td></tr></table></figure>\n<p>官网提到tensorflow2.x是默认开启<code>Eager Execution</code> 的，然而代码中(如上)使用了<code>@tf.function</code> 装饰器，默认以图的方式执行。</p>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">The</span> <span class=\"meta\">code</span> in a <span class=\"meta\">Function</span> can <span class=\"keyword\">be </span>executed <span class=\"keyword\">both </span>eagerly <span class=\"keyword\">and </span>as a graph. <span class=\"keyword\">By </span>default, <span class=\"meta\">Function</span> executes <span class=\"keyword\">its </span><span class=\"meta\">code</span> as a graph.</span><br></pre></td></tr></table></figure>\n<p>要关闭默认方式，可以通过设置：<code>tf.config.run_functions_eagerly(True)</code> 来实现。或者干脆不要加这个装饰器。</p>\n<p>最后，<code>Eager Execution</code> 增强了开发和调试的交互性，而<code>@tf.function</code> 计算图执行在分布式训练、性能优化和生产部署方面具有优势。简而言之，<code>Eager Execution</code>适合开发过程中调试，<code>@tf.function</code>适合线上部署。</p>\n<p>——————-2021.9.17更新———————</p>\n<blockquote>\n<p>自定义</p>\n</blockquote>\n<p>参考-&gt;<a href=\"https://www.tensorflow.org/guide/basic_training_loops\" target=\"_blank\" rel=\"noopener\">这里</a></p>\n<p><strong>定义模型</strong></p>\n<p>抛开keras的<code>sequential</code>, 使用 tensorflow定义模型时，可以有两种继承选择：<code>tf.keras.Model</code> 和 <code>tf.Module</code></p>\n<p>例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TFModel</span><span class=\"params\">(tf.Module)</span>:</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, **kwargs)</span>:</span></span><br><span class=\"line\">    super().__init__(**kwargs)</span><br><span class=\"line\">    self.w = tf.Variable(<span class=\"number\">5.0</span>)</span><br><span class=\"line\">    self.b = tf.Variable(<span class=\"number\">0.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__call__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> self.w * x + self.b</span><br><span class=\"line\">tf_model = TFModel()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KerasModel</span><span class=\"params\">(tf.keras.Model)</span>:</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, **kwargs)</span>:</span></span><br><span class=\"line\">    super().__init__(**kwargs)</span><br><span class=\"line\">    self.w = tf.Variable(<span class=\"number\">5.0</span>)</span><br><span class=\"line\">    self.b = tf.Variable(<span class=\"number\">0.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, x, **kwargs)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> self.w * x + self.b</span><br><span class=\"line\">keras_model = KerasModel()</span><br></pre></td></tr></table></figure>\n<p>定义训练循环：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(x_train,y_train,x_valid,y_valid,model,epochs = <span class=\"number\">5</span>,batch_size = <span class=\"number\">64</span>, lr =<span class=\"number\">0.001</span>, print_freq = <span class=\"number\">10</span>)</span>:</span></span><br><span class=\"line\">    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class=\"line\">    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">    train_loss = tf.keras.metrics.Mean(name=<span class=\"string\">'train_loss'</span>)</span><br><span class=\"line\">    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class=\"string\">'train_accuracy'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    test_loss = tf.keras.metrics.Mean(name=<span class=\"string\">'test_loss'</span>)</span><br><span class=\"line\">    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class=\"string\">'test_accuracy'</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(epochs):</span><br><span class=\"line\">        <span class=\"comment\"># 在下一个epoch开始时，重置评估指标</span></span><br><span class=\"line\">        train_loss.reset_states()</span><br><span class=\"line\">        train_accuracy.reset_states()</span><br><span class=\"line\">        test_loss.reset_states()</span><br><span class=\"line\">        test_accuracy.reset_states()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> step <span class=\"keyword\">in</span> range(int(len(x_train)/batch_size)):</span><br><span class=\"line\">            rand_id = np.asarray(random.sample(range(len(x_train)), batch_size))</span><br><span class=\"line\">            bs_x_train = x_train[rand_id]</span><br><span class=\"line\">            bs_y_train = y_train[rand_id]</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># train step</span></span><br><span class=\"line\">            <span class=\"keyword\">with</span> tf.GradientTape() <span class=\"keyword\">as</span> tape:</span><br><span class=\"line\">                predictions = model(bs_x_train)</span><br><span class=\"line\">                loss = loss_object(bs_y_train, predictions)</span><br><span class=\"line\">            gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class=\"line\">            optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class=\"line\">            train_loss(loss)</span><br><span class=\"line\">            train_accuracy(bs_y_train, predictions)</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># test step</span></span><br><span class=\"line\">            predictions = model(x_valid)</span><br><span class=\"line\">            t_loss = loss_object(y_valid, predictions)</span><br><span class=\"line\">            test_loss(t_loss)</span><br><span class=\"line\">            test_accuracy(y_valid, predictions)</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># print info</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> step%<span class=\"number\">10</span>==<span class=\"number\">0</span>:</span><br><span class=\"line\">                template = <span class=\"string\">'Epoch &#123;&#125;,step &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;'</span></span><br><span class=\"line\">                print(template.format(epoch+<span class=\"number\">1</span>,</span><br><span class=\"line\">                            step,</span><br><span class=\"line\">                            train_loss.result(),</span><br><span class=\"line\">                            train_accuracy.result()*<span class=\"number\">100</span>,</span><br><span class=\"line\">                            test_loss.result(),</span><br><span class=\"line\">                            test_accuracy.result()*<span class=\"number\">100</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        template = <span class=\"string\">'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;'</span></span><br><span class=\"line\">        print(template.format(epoch+<span class=\"number\">1</span>,</span><br><span class=\"line\">                    train_loss.result(),</span><br><span class=\"line\">                    train_accuracy.result()*<span class=\"number\">100</span>,</span><br><span class=\"line\">                    test_loss.result(),</span><br><span class=\"line\">                    test_accuracy.result()*<span class=\"number\">100</span>))</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<p>若是继承自<code>tf.keras.Model</code> 则可以使用 <code>model.compile()</code> 去设置参数, 使用<code>model.fit()</code> 进行训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">keras_model = KerasModel()</span><br><span class=\"line\">keras_model.compile(</span><br><span class=\"line\">    <span class=\"comment\"># 默认情况下，fit()调用tf.function()。</span></span><br><span class=\"line\">    <span class=\"comment\"># Debug时你可以关闭这一功能，但是现在是打开的。</span></span><br><span class=\"line\">    run_eagerly=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    optimizer=tf.keras.optimizers.SGD(learning_rate=<span class=\"number\">0.1</span>),</span><br><span class=\"line\">    loss=tf.keras.losses.mean_squared_error,</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>——————-2021.9.18更———————</p>\n<blockquote>\n<p>种子</p>\n</blockquote>\n<p>为了确保每次运行结果的稳定，设置固定种子是有必要的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random.seed(<span class=\"number\">2021</span>)</span><br><span class=\"line\">np.random.seed(<span class=\"number\">2021</span>)</span><br><span class=\"line\">tf.random.set_seed(<span class=\"number\">2021</span>)</span><br></pre></td></tr></table></figure>\n<p>——————-2021.9.19更———————</p>\n<p>当自定义模型时，继承<code>tf.keras.Model</code> 则需要实现<code>call</code> 方法而不是<code>__call__</code> 。如果是<code>tf.Module</code> 就实现<code>__call__</code> 。尽量使用<code>tf.keras.Model</code> ，因为真的很方便。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KerasModel</span><span class=\"params\">(tf.keras.Model)</span>:</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, **kwargs)</span>:</span></span><br><span class=\"line\">    super().__init__(**kwargs)</span><br><span class=\"line\">    self.w = tf.Variable(<span class=\"number\">5.0</span>)</span><br><span class=\"line\">    self.b = tf.Variable(<span class=\"number\">0.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, x, **kwargs)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> self.w * x + self.b</span><br><span class=\"line\">model = KerasModel()</span><br><span class=\"line\">bst_model_path = <span class=\"string\">\"./best.model.h5\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">early_stopping = tf.keras.callbacks.EarlyStopping(monitor=<span class=\"string\">'val_accuracy'</span>, patience=<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model_checkpoint = tf.keras.callbacks.ModelCheckpoint(bst_model_path, save_best_only=<span class=\"literal\">True</span>, save_weights_only=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class=\"literal\">False</span>),</span><br><span class=\"line\">              optimizer=tf.keras.optimizers.Adam(<span class=\"number\">1e-3</span>),</span><br><span class=\"line\">              metrics=[<span class=\"string\">'accuracy'</span>])</span><br><span class=\"line\">model.fit(x_train, train_y, </span><br><span class=\"line\">          batch_size=<span class=\"number\">16</span>, </span><br><span class=\"line\">          validation_data=(x_valid,valid_y),</span><br><span class=\"line\">          epochs=<span class=\"number\">200</span>,</span><br><span class=\"line\">          callbacks=[early_stopping,model_checkpoint]</span><br><span class=\"line\">         )</span><br></pre></td></tr></table></figure>\n<p>——————-2021.10.4更———————</p>\n<blockquote>\n<p>从python生成器中加载数据</p>\n</blockquote>\n<p>官方文档：<a href=\"https://www.tensorflow.org/guide/data?hl=zh_cn#consuming_python_generators\" target=\"_blank\" rel=\"noopener\">consuming_python_generators</a></p>\n<p>code template:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_dataset</span><span class=\"params\">(args, batch_size,shuffle=False)</span>:</span></span><br><span class=\"line\">    output_types = (tf.int32, tf.int32, tf.int32, tf.int32)</span><br><span class=\"line\">    output_shapes = ((<span class=\"literal\">None</span>,),</span><br><span class=\"line\">                     (<span class=\"literal\">None</span>,),</span><br><span class=\"line\">                     (<span class=\"literal\">None</span>,<span class=\"number\">2</span>),</span><br><span class=\"line\">                     (<span class=\"literal\">None</span>,)</span><br><span class=\"line\">                     )</span><br><span class=\"line\">    dataset = tf.data.Dataset.from_generator(generator_fn, </span><br><span class=\"line\">                                              args=args, </span><br><span class=\"line\">                                              output_types= output_types, </span><br><span class=\"line\">                                              output_shapes = output_shapes</span><br><span class=\"line\">                                              )</span><br><span class=\"line\">    <span class=\"keyword\">if</span> shuffle:</span><br><span class=\"line\">        dataset = dataset.shuffle(buffer_size = <span class=\"number\">1000</span>*batch_size)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\"># dataset = dataset.repeat() 这行代码有毒</span></span><br><span class=\"line\">    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=output_shapes, padding_values=(<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    dataset = dataset.prefetch(<span class=\"number\">1</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> dataset</span><br></pre></td></tr></table></figure>\n<p><code>output_types</code> 是必须的，<code>buffer_size</code> 一般取大于等于数据集大小。<code>generator_fn</code> 为生成器函数。如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span><span class=\"params\">(stop)</span>:</span></span><br><span class=\"line\">  i = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> i&lt;stop:</span><br><span class=\"line\">    <span class=\"keyword\">yield</span> i</span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>——————-2021.10.5更———————</p>\n<blockquote>\n<p>有关<code>call()</code></p>\n</blockquote>\n<p>子类化<code>tf.keras.Model</code>时，在实现<code>call()</code> 函数需要注意的是接受的参数一般只能是两个：<code>inputs</code> ，<code>training</code></p>\n<p><code>training</code> 一般给用户自定义训练模式提供一定的自由度，<code>training</code> 为布尔类型。当然，<code>training</code> 是非必须的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KerasModel</span><span class=\"params\">(tf.keras.Model)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, **kwargs)</span>:</span></span><br><span class=\"line\">        super().__init__(**kwargs)</span><br><span class=\"line\">        self.w = tf.Variable(<span class=\"number\">5.0</span>)</span><br><span class=\"line\">        self.b = tf.Variable(<span class=\"number\">0.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.w * x + self.b</span><br></pre></td></tr></table></figure>\n<p>如果模型需要多个输入时：可以通过 <code>inputs = (x1,x2,x3,...)</code> 将<code>inputs</code> 传入</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">··省略··</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, inputs)</span>:</span></span><br><span class=\"line\">        x1,x2,x3,... = inputs</span><br></pre></td></tr></table></figure>\n<p>当然也可以通过<code>**kwargs</code> 将其他数据传入。而不是像这样：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, x1,x2,x3,...)</span>:</span></span><br><span class=\"line\">    ···</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>类型转换</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tf.cast(x, dtype=tf.int64)</span><br></pre></td></tr></table></figure>\n<p>——————-2021.10.16更———————</p>\n<blockquote>\n<p><code>tf.keras.layers.MultiHeadAttention</code></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">multi = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">out = multi(v,k,q,mask*<span class=\"number\">-1e9</span>)</span><br></pre></td></tr></table></figure>\n<p>如果mask是0、1矩阵，记得乘以<strong>-1e9</strong> ，否者掩码无效。</p>\n<blockquote>\n<p>TODO</p>\n</blockquote>\n","categories":[],"tags":["tensorflow"]},{"title":"蛋白质结构预测","url":"https://hahally.github.io//articles/蛋白质结构预测/","content":"<p>赛题：<a href=\"https://challenge.xfyun.cn/topic/info?type=protein\" target=\"_blank\" rel=\"noopener\">蛋白质结构预测挑战赛</a></p>\n<p>数据集一共包含245种折叠类型，11843条蛋白质序列样本，其中训练集中有9472个样本，测试集中有2371个样本。</p>\n<p>继上次<a href=\"https://hahally.github.io/articles/蛋白质结构预测之lgb的baseline/\">lgb的base模型</a> 后，尝试过word2vec + 神经网络的方法，最后效果甚微。今天尝试了一下双向GRU模型，相比之前，有几个百分点的提高。</p>\n<p>代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.utils <span class=\"keyword\">import</span> shuffle</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据加载</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read_fa</span><span class=\"params\">(file, mode=<span class=\"string\">'train'</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> mode <span class=\"keyword\">in</span> &#123;<span class=\"string\">'train'</span>,<span class=\"string\">'test'</span>&#125;</span><br><span class=\"line\">    labels = []</span><br><span class=\"line\">    seqs_info = []</span><br><span class=\"line\">    cates_id = []</span><br><span class=\"line\">    seq = <span class=\"string\">''</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file,mode=<span class=\"string\">'r'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        line = f.readline().strip()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> line:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> line[<span class=\"number\">0</span>]==<span class=\"string\">'&gt;'</span>:</span><br><span class=\"line\">                info = line[<span class=\"number\">1</span>:].split(<span class=\"string\">' '</span>)</span><br><span class=\"line\">                cates_id.append(info[<span class=\"number\">0</span>])</span><br><span class=\"line\">                <span class=\"keyword\">if</span> mode == <span class=\"string\">'train'</span>:</span><br><span class=\"line\">                    label = <span class=\"string\">''</span>.join(info[<span class=\"number\">1</span>].split(<span class=\"string\">'.'</span>)[:<span class=\"number\">2</span>]</span><br><span class=\"line\">                    label = label[<span class=\"number\">0</span>]+<span class=\"string\">'.'</span>+label[<span class=\"number\">1</span>:]</span><br><span class=\"line\">                    labels.append(label)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> seq:</span><br><span class=\"line\">                    seqs_info.append(seq)</span><br><span class=\"line\">                    seq = <span class=\"string\">''</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                seq += line</span><br><span class=\"line\">            line = f.readline().strip()</span><br><span class=\"line\">        seqs_info.append(seq)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> cates_id,seqs_info,labels</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    train_file = <span class=\"string\">'/kaggle/input/textfiles/astral_train.fa'</span></span><br><span class=\"line\">    test_file = <span class=\"string\">'/kaggle/input/textfiles/astral_test.fa'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    train_sample_id, train_seqs_info, train_labels = read_fa(train_file, mode=<span class=\"string\">'train'</span>)</span><br><span class=\"line\">    test_sample_id, test_seqs_info, _ = read_fa(test_file, mode=<span class=\"string\">'test'</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    train_data = &#123;</span><br><span class=\"line\">    <span class=\"string\">'sample_id'</span>: train_sample_id,</span><br><span class=\"line\">    <span class=\"string\">'seq_info'</span>: train_seqs_info,</span><br><span class=\"line\">    <span class=\"string\">'label'</span>: train_labels</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    test_data = &#123;</span><br><span class=\"line\">        <span class=\"string\">'sample_id'</span>: test_sample_id,</span><br><span class=\"line\">        <span class=\"string\">'seq_info'</span>: test_seqs_info,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">     </span><br><span class=\"line\">    train = pd.DataFrame(data=train_data)</span><br><span class=\"line\">    train = shuffle(train,random_state=<span class=\"number\">2021</span>).reset_index(drop=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    test = pd.DataFrame(data=test_data)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> train,test</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 滑窗分词</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">split_windows</span><span class=\"params\">(sentence,w = <span class=\"number\">3</span>)</span>:</span></span><br><span class=\"line\">    new_sentence = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(sentence)-w+<span class=\"number\">1</span>):</span><br><span class=\"line\">        new_sentence.append(sentence[i:i+w])</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> new_sentence</span><br><span class=\"line\">     </span><br><span class=\"line\">data = pd.concat(load_data(),ignore_index=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># label to idx</span></span><br><span class=\"line\">label2idx = &#123; l:idx <span class=\"keyword\">for</span> idx, l <span class=\"keyword\">in</span> enumerate(data[~data[<span class=\"string\">'label'</span>].isna()][<span class=\"string\">'label'</span>].unique().tolist())&#125;</span><br><span class=\"line\">idx2label = &#123; idx:l <span class=\"keyword\">for</span> l,idx <span class=\"keyword\">in</span> label2idx.items()&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">data[<span class=\"string\">'label'</span>] = data[<span class=\"string\">'label'</span>].map(label2idx)</span><br><span class=\"line\">data[<span class=\"string\">'new_seq_info'</span>] = data[<span class=\"string\">'seq_info'</span>].apply(<span class=\"keyword\">lambda</span> x:split_windows(x,w = <span class=\"number\">1</span>))</span><br><span class=\"line\">train,test = data[~data[<span class=\"string\">'label'</span>].isna()].reset_index(drop=<span class=\"literal\">True</span>),data[data[<span class=\"string\">'label'</span>].isna()].reset_index(drop=<span class=\"literal\">True</span>)</span><br><span class=\"line\">max_features= <span class=\"number\">1000</span></span><br><span class=\"line\">max_len= <span class=\"number\">256</span></span><br><span class=\"line\">embed_size=<span class=\"number\">128</span></span><br><span class=\"line\">batch_size = <span class=\"number\">24</span></span><br><span class=\"line\">epochs = <span class=\"number\">50</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.preprocessing.text <span class=\"keyword\">import</span> Tokenizer</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.preprocessing <span class=\"keyword\">import</span> sequence</span><br><span class=\"line\"></span><br><span class=\"line\">tokens = Tokenizer(num_words = max_features)</span><br><span class=\"line\">tokens.fit_on_texts(list(data[<span class=\"string\">'new_seq_info'</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">x_data = tokens.texts_to_sequences(data[<span class=\"string\">'new_seq_info'</span>])</span><br><span class=\"line\">x_data = sequence.pad_sequences(x_data, maxlen=max_len)</span><br><span class=\"line\">x_train = x_data[:<span class=\"number\">9472</span>]</span><br><span class=\"line\">y_train = data[<span class=\"string\">'label'</span>][:<span class=\"number\">9472</span>]</span><br><span class=\"line\">x_test = x_data[<span class=\"number\">9472</span>:]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D<span class=\"comment\"># Keras Callback Functions:</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.callbacks <span class=\"keyword\">import</span> Callback</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.callbacks <span class=\"keyword\">import</span> EarlyStopping,ModelCheckpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> initializers, regularizers, constraints, optimizers, layers, callbacks</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.optimizers <span class=\"keyword\">import</span> Adam</span><br><span class=\"line\"><span class=\"keyword\">import</span> keras</span><br><span class=\"line\">sequence_input = Input(shape=(max_len, ))</span><br><span class=\"line\">x = Embedding(max_features, embed_size, trainable=<span class=\"literal\">True</span>)(sequence_input)</span><br><span class=\"line\">x = SpatialDropout1D(<span class=\"number\">0.2</span>)(x)</span><br><span class=\"line\">x = Bidirectional(GRU(<span class=\"number\">128</span>, return_sequences=<span class=\"literal\">True</span>,dropout=<span class=\"number\">0.1</span>,recurrent_dropout=<span class=\"number\">0.1</span>))(x)</span><br><span class=\"line\">x = Conv1D(<span class=\"number\">64</span>, kernel_size = <span class=\"number\">3</span>, padding = <span class=\"string\">\"valid\"</span>, kernel_initializer = <span class=\"string\">\"glorot_uniform\"</span>)(x)</span><br><span class=\"line\">avg_pool = GlobalAveragePooling1D()(x)</span><br><span class=\"line\">max_pool = GlobalMaxPooling1D()(x)</span><br><span class=\"line\">x = concatenate([avg_pool, max_pool]) </span><br><span class=\"line\">preds = Dense(<span class=\"number\">245</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">model = Model(sequence_input, preds)</span><br><span class=\"line\">model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class=\"literal\">True</span>),</span><br><span class=\"line\">              optimizer=keras.optimizers.Adam(<span class=\"number\">1e-3</span>),</span><br><span class=\"line\">              metrics=[<span class=\"string\">'accuracy'</span>])</span><br><span class=\"line\">model.fit(x_train, y_train, </span><br><span class=\"line\">          batch_size=batch_size, </span><br><span class=\"line\">          validation_split=<span class=\"number\">0.2</span>,</span><br><span class=\"line\">          epochs=epochs)</span><br></pre></td></tr></table></figure>\n<p>提交结果：目前【39/130(提交团队数)】</p>\n<p><img src=\"/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B/image-20210730140527003.png\" alt=\"image-20210730140527003\"></p>\n","categories":[],"tags":["BDC"]},{"title":"基于注意力机制的神经机器翻译的有效方法","url":"https://hahally.github.io//articles/基于注意力机制的神经机器翻译的有效方法/","content":"<blockquote>\n<p><a href=\"https://aclanthology.org/D15-1166/\" target=\"_blank\" rel=\"noopener\">Effective Approaches to Attention-based Neural Machine Translation</a></p>\n<p>基于注意力机制的神经机器翻译的有效方法</p>\n</blockquote>\n<p><strong>Bib TeX</strong></p>\n<blockquote>\n<p>@inproceedings{luong-etal-2015-effective,<br> title = “Effective Approaches to Attention-based Neural Machine Translation”,<br> author = “Luong, Thang  and<br>   Pham, Hieu  and<br>   Manning, Christopher D.”,<br> booktitle = “Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing”,<br> month = sep,<br> year = “2015”,<br> address = “Lisbon, Portugal”,<br> publisher = “Association for Computational Linguistics”,<br> url = “<a href=\"https://aclanthology.org/D15-1166\" target=\"_blank\" rel=\"noopener\">https://aclanthology.org/D15-1166</a>“,<br> doi = “10.18653/v1/D15-1166”,<br> pages = “1412—1421”,<br>}</p>\n</blockquote>\n<h3 id=\"abstract\"><a href=\"#abstract\" class=\"headerlink\" title=\"abstract\"></a>abstract</h3><p>在神经机器翻译中引入注意力机制(Attention)，使模型在翻译过程中选择性的关注句子中的某一部分。本文研究了两种简单有效的注意力机制。</p>\n<ul>\n<li>a global approach which always attends to all source words【全局方法，每次关注所有源词】</li>\n<li>a local one that only looks at a subset of source words at a time【局部方法，每次关注原词的一个子集】</li>\n</ul>\n<p><em>global attention</em> 类似方法<strong>[1]</strong>，但架构上更加简单。<em>local attention</em> 更像是 <em>hard and soft attention</em> <strong>[2]</strong>的结合。两种方法在英德语双向翻译任务中取得了不错的成绩。与已经结合了已知技术（例如 dropout）的非注意力系统相比，高了5.0个BLEU点。在WMT’15英语到德语的翻译任务中表现 SOTA（state-of-the-art）。</p>\n<blockquote>\n<p>With local attention, we achieve a significant gain of　5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.</p>\n</blockquote>\n<h3 id=\"Neural-Machine-Translation\"><a href=\"#Neural-Machine-Translation\" class=\"headerlink\" title=\"Neural Machine Translation\"></a>Neural Machine Translation</h3><p>模型结构：</p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712173309790.png\" alt=\"image-20210712173309790\"></p>\n<p>采用堆叠的 <em>LSTM</em>结构<strong>[3]</strong>。其目标函数为：</p>\n<script type=\"math/tex; mode=display\">\nJ_t = \\sum_{(x,y)\\in D}-logP(y|x)</script><p>D为训练的语料。x 表示源句子，y表示翻译后的目标句子。</p>\n<h3 id=\"Attention-based-Models\"><a href=\"#Attention-based-Models\" class=\"headerlink\" title=\"Attention-based Models\"></a>Attention-based Models</h3><p>这部分包括两种注意力机制：global 和 local。两种方式在解码阶段，将使用堆叠LSTM顶层的隐藏状态 $h_t$ 作为输入。区别在于获取上下文向量表示$c_t$方法不同。然后通过一个 简单的 <em>concatenate layer</em> 获得一个注意力隐藏状态$\\hat h_t$:</p>\n<script type=\"math/tex; mode=display\">\n\\hat h_t = tanh(W_c[c_t;h_t])</script><p>最后通过 <em>softmax layer</em> 得出预测概率分布:</p>\n<script type=\"math/tex; mode=display\">\np(y_t|y<t,x)=softmax(W_s\\hat h_t)</script><p><strong>Global Attention</strong></p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712174342526.png\" alt=\"image-20210712174342526\"></p>\n<p>主要思想是通过编码器的所有隐藏状态(hidden state)来获取上下文向量(context vector)表示 $c_t$。可变长度对齐向量$a_t$通过比较当前目标隐藏状态$h_t$和每个源隐藏状态$\\overline h_s$得到：</p>\n<script type=\"math/tex; mode=display\">\na_t(s) = align(h_t,\\overline h_s)=\\frac{exp(score(h_t,\\overline h_s))}{\\sum_{s'}exp(h_t,\\overline h_{s^{'}})}</script><p>score被称为 <em>content-based</em> 函数：</p>\n<script type=\"math/tex; mode=display\">\nscore(h_t,\\overline h_s)=\\begin{cases}\nh_t^{T}\\overline h_s, dot\\\\\nh_t^{T}W_a\\overline h_s, general\\\\\nW_a[h_t;\\overline h_s], concat\n\\end{cases}</script><p>与<strong>[1]</strong>的区别在于：</p>\n<ul>\n<li>只在编码器和解码器的顶部使用隐藏状态</li>\n<li>计算路径更加简单：$h_t-&gt;a_t-&gt;c_t-&gt;\\hat h_t$</li>\n</ul>\n<p><strong>Local Attention</strong></p>\n<p>global 模式下，模型需要关注全局信息，其代价是非常大的。因此也就出现了 local attention。让注意力机制只去关注其中的一个子集部分。<em>其灵感来自于</em> <strong>[2]</strong>。</p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712185709989.png\" alt=\"image-20210712185709989\"></p>\n<p>对比两张模型图来看，其中的局部对齐权重$a_t$由一部分局部隐藏状态计算得到，其长度变成了固定的，并且还多了一个Aligned position $p_t$ ，然后上下文向量(context vector) $c_t$ 由窗口$[p_t-D,p_t+D]$内的隐藏状态集合的加权平均得到。<em>其中D根据经验所得</em>。</p>\n<p>考虑两种变体：</p>\n<ul>\n<li><p>Monotonic alignment (local-m)</p>\n<p>即简单设置 $p_t = t$ ，认为源序列于目标序列是单调对齐的，那么$a_t$ 其实就和公式（4）计算方法一样了。</p>\n</li>\n<li><p>Predictive alignment (local-p)</p>\n<p>$p_t=S·sigmoid(v_p^{T}tanh(W_ph_t))$ ，$v_p$和$W_p$是预测$p_t$ 的模型参数。S为源句子长度。最后$p_t\\in [0,S]$ 。同时为了使对齐点更靠近$p_t$，设置一个以$p_t$为中心 的高斯分布，即$a_t$ 为：$a_t(s)=align(h_t,\\overline h_s)exp(-\\frac{(s-p_t)^2}{2\\sigma^2}),\\sigma=\\frac{D}{2}$，s为高斯分布区间内的一个整数。</p>\n</li>\n</ul>\n<h3 id=\"Input-feeding-Approach\"><a href=\"#Input-feeding-Approach\" class=\"headerlink\" title=\"Input-feeding Approach\"></a>Input-feeding Approach</h3><p>这一部分，主要是为了捕获在翻译过程中哪些源单词已经被翻译过了。对齐决策应当综合考虑过去对齐的信息。该方法将注意力向量$\\hat h_t$ 作为下一个时间步的输入。主要有两个作用：</p>\n<ul>\n<li>希望模型充分关注到先前的对齐信息</li>\n<li>创建一个在水平和垂直方向上都很深的网络</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>整篇论文看下来，大概就是在别人的baseline中引入注意力机制（global and local），然后使用<em>Input-feeding</em> 方法将过去的对齐信息考虑进来（大概就是加入了一个先验知识吧）。【PS：震惊！这些创新的点的灵感都来自其让人的论文中的方法。】</p>\n<p>最后手动滑稽：</p>\n<blockquote>\n<p>Attention is all you need!</p>\n</blockquote>\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><p>[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.  2015. Neural machine translation by jointly learning to align and translate. InICLR.</p>\n<p>[2] Kelvin Xu,  Jimmy Ba,  Ryan Kiros,  Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show,attend and tell: Neural image caption generation with visual attention. InICML.</p>\n<p>[3] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2015. Recurrent neural network regularization. InICLR.</p>\n","categories":[],"tags":["paper reading"]},{"title":"sequence-to-sequence-learning-with-neural-networks","url":"https://hahally.github.io//articles/sequence-to-sequence-learning-with-neural-networks/","content":"<blockquote>\n<p><a href=\"https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf\" target=\"_blank\" rel=\"noopener\">Sequence to Sequence Learning with Neural Networks</a></p>\n<p>基于神经网络的seq2seq</p>\n</blockquote>\n<p>很好的解决了序列到序列之间的映射问题，在语音识别和机器翻译这种长度未知且具有顺序的问题能够得到很好的解决。该模型应用到英语到法语的翻译任务，数据集来自WMT’14，在整个测试集上的BLEU得到达到34.8。</p>\n<p>模型结构：</p>\n<p><img src=\"/articles/sequence-to-sequence-learning-with-neural-networks/image-20210710154526137.png\" alt=\"image-20210710154526137\"></p>\n<p>这里使用了两个串联的 lstm 网络，前一个用于读取输入序列，产生一个大的固定维度的向量表示，然后再用一个lstm 网络从向量表示中提取输出序列。一个编码器(Encoder)，一个解码器(Decoder)。</p>\n<p>值得注意的是，论文提到在实现时，有三点与 lstm 不一样。【Our actual models differ from the above description in three important ways.】</p>\n<ul>\n<li>使用两个不同的lstm</li>\n<li>4层 lstm</li>\n<li>将输入序列进行反转【a,b,c —&gt; c,b,a】【PS： 这就是传说中的反向操作吗？！莫名其妙的trick，滑稽.jpg】</li>\n</ul>\n<p>目标函数：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{|S|}\\sum_{(T,S)\\in S}logP(T|S)</script><p>S为训练集，T表示正确的翻译。训练结束后，进行翻译时，在模型产生的多个翻译结果中找到最可能正确的翻译：</p>\n<script type=\"math/tex; mode=display\">\n\\hat T = \\mathop{argmax}_{T}P(T|S)</script><p>通过一个简单的 <em>left-to-right beam search</em> 解码器(decoder)搜索最可能的翻译。</p>\n<blockquote>\n<p>We search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some translation. At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the model’s log probability. As soon as the “<EOS>“ symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam search(Table 1).</EOS></p>\n</blockquote>\n<p><strong>一个小的总结</strong></p>\n<p>总的来说， <em>Reversing the Source Sentences</em> 这个操作给模型带来了很大的提升。</p>\n<blockquote>\n<p>the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU scores of its decoded translations increased from 25.9 to 30.6.</p>\n</blockquote>\n<p>至于为什么，论文中也没有给出很好的解释（大概是说，反转后，前面源句子的前几个词与目标句子的前几个词距离更近，模型能更好收敛。【a,b,c |w,x,y—&gt; c,b,a|w,x,y】w,x,y为a,b,c对应的翻译，反转后，a,b,c与w,x,y的平均距离不变，a离w更近了。但是c离y更远却没有影响模型精度。可能这就是玄学吧？！）。【PS：难道是因为误打误撞的尝试然后发现效果惊人，然后就发论文了？不过这篇论文确实奠定了之后的seq2seq模型的基础。】</p>\n","categories":[],"tags":["paper reading"]},{"title":"Lost-in-just-the-translation","url":"https://hahally.github.io//articles/Lost-in-just-the-translation/","content":"<blockquote>\n<p>Lost in just the translation</p>\n</blockquote>\n<h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><p>本文介绍了自然语言翻译文本信息隐藏系统的设计与实现，并给出了实验结果。与前人的工作不同，本文提出的协议<strong>只需要翻译文本就可以恢复隐藏信息</strong>。这是一个重大的改进，因为传输源文本既浪费资源又不安全。现在，系统的安全性得到了改善，这不仅是因为源文本不再对敌方有用，还因为现在可以使用更广泛的防御系统(如混合人机翻译)。</p>\n<h3 id=\"协议\"><a href=\"#协议\" class=\"headerlink\" title=\"协议\"></a>协议</h3><p><img src=\"/articles/Lost-in-just-the-translation/image-20210709214343204.png\" alt=\"image-20210709214343204\"></p>\n<ul>\n<li><p>Producing translations</p>\n<p>方法大致与论文【Translation-Based Steganography】中提到的一样</p>\n</li>\n<li><p>Tokenization</p>\n<p>双方使用相同的 Tokenization 算法，以获得相同的句子序列</p>\n</li>\n<li><p>Choosing h</p>\n<p>选择合适的 h ($h \\ge 0$)，h表示将信息隐藏在每个句子中的长度的位数。</p>\n</li>\n<li><p>Selecting translations</p>\n<p>对于所有翻译，编码器首先使用与接收方共享的密钥计算每个翻译的加密键值散列。其基本思想是在给定句子的所有译文中选择一个句子，然后对其进行适当的长度编码，并在隐藏的信息中选择合适的位置。然而，由于给定句子中的位编码数量是可变的，因此该算法在这方面有很大的自由度。</p>\n</li>\n<li><p>Optimized Handling of Hash Collisions</p>\n<p>哈希冲突的处理优化</p>\n</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>为了不传输源文本，从而，引入了哈希映射和 Tokenization以及参数 h。产生翻译文本过程中混合人机翻译结果，使得隐藏的信息更加难以检测。</p>\n","categories":[],"tags":["paper reading"]},{"title":"基于翻译的隐写术","url":"https://hahally.github.io//articles/基于翻译的隐写术/","content":"<blockquote>\n<p><a href=\"https://dl.acm.org/doi/10.1007/11558859_17\" target=\"_blank\" rel=\"noopener\">Translation-Based Steganography</a></p>\n<p>基于翻译的隐写术</p>\n</blockquote>\n<h3 id=\"abstract\"><a href=\"#abstract\" class=\"headerlink\" title=\"abstract\"></a>abstract</h3><p>这篇论文研究了用隐写术在自然语言文档自动翻译产生的噪音(“noise”)中隐藏信息的可能性。由于自然语言固有的冗余性为翻译的变化创造了足够的空间，因此机器翻译非常适合隐写。此外，因为在自动文本翻译中经常出现错误，信息隐藏机制插入的额外错误就很难检测出来，看起来就像是翻译过程中产生的正常噪音的一部分。正因如此，我们是很难确定翻译中的不准确是由隐写术的使用还是由翻译软件的缺陷造成的。</p>\n<h3 id=\"introduction\"><a href=\"#introduction\" class=\"headerlink\" title=\"introduction\"></a>introduction</h3><p>本文提出了一种用于自然语言文本中的隐蔽消息传输的新协议，为此我们有一个概念验证(proof-of-concept )实现。关键点就是将信息隐藏在自然语言翻译中经常出现的噪音中。在一对自然语言之间翻译[non-trivial]文本时，通常有许多可能的翻译结果。【大概意思应该是在不改变原文意思的情况下，翻译的结果是多种多样的】。选择这些翻译结果之一就可用于对信息进行编码。一个 <em>adversary</em> 要想检测出其中隐藏的信息，就必须明白包含隐藏信息的翻译是不可能由普通翻译生成的。由于翻译过程中本就夹杂一些噪声，这使得检测隐藏信息是十分困难的。例如，由于同义词的存在，在对原文进行翻译的过程中，使用同义词进行替换。随着翻译结果的增加，也增加了信息隐藏的可能性。</p>\n<p>本文评估了使用自动机器翻译 (MT) 的自然语言翻译中隐蔽消息传输的潜在性。为了描述在机器翻译中的哪种变化是合理的，我们研究了各种 MT 系统产生的不同类型的错误。在机器翻译中观察到的一些变化对于人工翻译也显然是合理的。除了让 <em>adversary</em> 难以检测到隐藏信息的存在之外，基于翻译的隐写术也更容易使用。与之前的基于文本、图像和声音的隐写系统不一样，基于翻译的隐写，其 <em>cover</em> 是不需要保密的【the cover does not have to be secret.】。在基于翻译的隐写术中，源语言的原始文本可以是公开的，可以从公共资源中获取，并与译文一起，在<em>adversary</em>的视线范围内，在两方之间进行交换。在传统的图像隐写术中，经常出现的问题是，随后隐藏消息的源图像必须由发送者保密并且只使用一次（否则“diff”攻击将揭示隐藏消息的存在）。这增加了用户为每条消息创建新的秘密封面(secret cover)【周杰伦的专辑《不能说的秘密》？！滑稽脸.jpg】的负担。</p>\n<blockquote>\n<p> In translation-based steganography, the original text in the source language can be publically known, obtained from public sources, and, together with the translation, exchanged between the two parties in plain sight of the adversary. In traditional image steganography, the problem often occurs that the source image in which the message is subsequently hidden must be kept secret by the sender and used only once (as otherwise a “diff” attack would reveal the presence of a hidden message). This burdens the user with creating a new, secret cover for each message.</p>\n</blockquote>\n<p>基于翻译的隐写术没有这个缺点，因为对手无法对翻译应用差异分析来检测隐藏的消息。对手可能会生成原始消息的翻译，但无论使用隐写术，翻译可能会有所不同，使得差异分析无法检测隐藏的消息。</p>\n<p>为了证明这一点，我们实现了一个隐写编码器和解码器。该系统通过以类似于在现有 MT 系统中观察到的变化和错误的方式更改机器翻译来隐藏消息。我们的网页上提供了原型的交互式版本。</p>\n<p>在本文的其余结构如下。首先，第 2 节回顾了相关工作。在第 3 节中，描述了隐写交换的基本协议。在第 4 节中，我们给出了现有机器翻译系统中产生的错误的特征。第 5 节概述了实现和一些实验结果。在第 6 节中，我们讨论了基本协议的变体，以及各种攻击和可能的防御。</p>\n<h3 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h3><h3 id=\"Protocol\"><a href=\"#Protocol\" class=\"headerlink\" title=\"Protocol\"></a>Protocol</h3><p>本文的基本隐写协议工作如下。发件人首先需要获得源语言的封面(cover)。封面不必是保密的(secret)，可以从公共来源获得 ， 例如，新闻网站。然后发送者使用隐写编码器将源文本中的句子翻译成目标语言。隐写编码器本质上为每个句子创建多个翻译，并选择其中之一来对隐藏消息中的位进行编码。然后将翻译后的文本连同足以获得源文本的信息一起发送给接收者。这可以是源文本本身或对源的引用。然后接收者还使用相同的隐写编码器配置执行源文本的翻译。通过比较结果句子，接收者重建隐藏消息的比特流。图 1 说明了基本协议。</p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E7%BF%BB%E8%AF%91%E7%9A%84%E9%9A%90%E5%86%99%E6%9C%AF/image-20210709163311687.png\" alt=\"image-20210709163311687\"></p>\n<h4 id=\"Producing-translations\"><a href=\"#Producing-translations\" class=\"headerlink\" title=\"Producing translations\"></a>Producing translations</h4><p>获取源文本后，发送方和接收方的第一步是使用相同的算法生成源文本的多个翻译。此步骤的目标是确定性地生成源文本的多个不同翻译。实现这一目标的最简单方法是在源文本中的每个句子上应用所有可用 MT 系统的（子集）。如果各方可以完全访问统计 MT 系统的代码，他们可以通过使用不同的语料库训练，从同一代码库生成多个 MT 系统。</p>\n<p>除了使用多个翻译系统生成不同的句子外，还可以对结果翻译应用后处理以获得额外的变化。这种后处理包括模拟任何（MT）翻译中固有噪声的转换。例如，后处理器可以插入常见的翻译错误（如第 4 节所述）。</p>\n<p>由于不同引擎之间的翻译质量不同，并且还取决于应用了哪些后处理器来处理结果，因此翻译系统使用启发式方法为每个翻译分配一个概率，描述其与其他翻译相比的相对质量。启发式可以基于生成器的经验和基于语言模型对句子质量进行排名的算法 。用于生成翻译及其排名的特定翻译引擎、训练语料库和后处理操作集是想要进行秘密通信的两方密钥共享的一部分。</p>\n<h4 id=\"Selecting-a-translation\"><a href=\"#Selecting-a-translation\" class=\"headerlink\" title=\"Selecting a translation\"></a>Selecting a translation</h4><p>选择翻译以对隐藏消息进行编码时，编码器首先使用生成器算法分配的概率构建可用转换的霍夫曼树。然后算法选择与要编码的位序列对应的句子。 </p>\n<p>使用霍夫曼树根据翻译质量估计选择句子可确保较少选择翻译质量较低的句子。此外，所选翻译的质量越低，传输的比特数就越高。</p>\n<p>这减少了所需的封面文本总量，从而减少了对手可以分析的文本量。编码器可以使用相对翻译质量的下限来排除估计翻译质量低于某个阈值的句子，在这种情况下，该阈值成为发送者和接收者之间共享秘密的一部分。</p>\n<h4 id=\"Keeping-the-source-text-secret\"><a href=\"#Keeping-the-source-text-secret\" class=\"headerlink\" title=\"Keeping the source text secret\"></a>Keeping the source text secret</h4><p>所提出的方案可以适用于需要对源文本保密的水印。这可以按如下方式实现。编码器计算每个翻译句子的（加密）哈希。然后它选择一个句子，使得翻译句子的散列的最后一位对应于要传输的隐藏消息中的下一位。 然后解码器只计算接收到的句子的散列码并连接相应的最低位获取隐藏信息。</p>\n<p>该方案假设句子足够长，几乎总是有足够的变化来获得具有所需最低位的散列。每当没有一个句子产生可接受的哈希码时，就必须使用纠错码来纠正错误。使用这种变化会降低编码所能达到的比特率。更多细节可以在我们的技术报告中找到。</p>\n<h3 id=\"Lost-in-Translation\"><a href=\"#Lost-in-Translation\" class=\"headerlink\" title=\"Lost in Translation\"></a>Lost in Translation</h3><p>现代 MT 系统会在翻译中产生许多常见错误。本节描述了其中一些错误的特征。虽然我们描述的错误不是可能错误的完整列表，但它们代表了我们在示例翻译中经常观察到的错误类型。翻译错误的扩展特征可以在我们的技术报告中找到（由于篇幅限制，此处省略）。这些错误中的大多数是由于当代 MT 系统对统计和句法文本分析的依赖造成的，导致缺乏语义和上下文意识。这会产生一系列错误类型，我们可以使用它们来合理地改变文本，从而产生进一步的标记可能性。</p>\n<h4 id=\"Functional-Words\"><a href=\"#Functional-Words\" class=\"headerlink\" title=\"Functional Words\"></a>Functional Words</h4><p>一类经常发生但不破坏意义的错误是功能词翻译不正确，如冠词、代词和介词。因为这些功能词通常与句子中的另一个词或短语有很强的关联，复杂的结构似乎经常会导致这些词的翻译错误。此外，不同的语言对这些词的处理方式非常不同，因此在使用未考虑这些差异的引擎时会导致翻译错误。</p>\n<p>例如，许多使用冠词的语言并不在所有名词前使用它们。这在从文章规则不同的语言翻译时会导致问题。例如，法语句子“La vie est paralysee.”在英语中翻译为“Life is paralyzed.”。然而，翻译引擎可以预见地将其翻译为“The life is paralyzed.”；“life in general”意义上的“life”并没有用出现在一篇英文文章中。这与许多不可数名词如“水”和 “钱”一样，而导致类似的错误。</p>\n<p>通常，介词的正确选择完全取决于句子的上下文。例如，法语中的 $J’habite$ $\\grave{a}$ 100 $m\\grave{e}tres$ $de$ $lui$在英语中的意思是“我住在离他100米的地方”。然而，[20] 将其翻译为“我与他一起生活 100 米”，而 [71]将其翻译为“在他的 100 米处生活”。两者都使用“$\\grave{a}$”（“with/in”）的不同翻译这完全不适合上下文。</p>\n<h4 id=\"Blatant-Word-Choice-Errors\"><a href=\"#Blatant-Word-Choice-Errors\" class=\"headerlink\" title=\"Blatant Word Choice Errors\"></a>Blatant Word Choice Errors</h4><p>不太常见的是，在翻译中选择完全不相关的单词或短语。例如，<em>I’m staying home</em>和<em>I am staying home</em>都被[20]翻译成德语为<em>Ich bleibe Haupt</em>（<em>I’m staying head</em>）而不是<em>Ich bleibe zu Hause</em>。这些不同于语义错误，反映了实际引擎或其字典中的某种缺陷，明显影响了翻译质量。</p>\n<h4 id=\"Additional-Errors\"><a href=\"#Additional-Errors\" class=\"headerlink\" title=\"Additional Errors\"></a>Additional Errors</h4><p>遇到了其他几种有趣的错误类型，由于篇幅原因，我们将只简要介绍这些错误类型。</p>\n<ul>\n<li>基本语法错误导致翻译如<em>It do not work</em></li>\n<li>逐字翻译，尤其是惯用语的翻译，会产生诸如<em>The pencils are at me.</em>这样的结构</li>\n<li>源词典中没有的单词只是不翻译</li>\n<li>语言之间反身结构的不正确映射会导致反身冠词被错误地插入目标翻译中（例如，<em>Ich kamme mich</em>变成了<em>I comb myself</em>）。</li>\n</ul>\n<h4 id=\"Translations-between-Typologically-Dissimilar-Languages\"><a href=\"#Translations-between-Typologically-Dissimilar-Languages\" class=\"headerlink\" title=\"Translations between Typologically Dissimilar Languages\"></a>Translations between Typologically Dissimilar Languages</h4><p>类型学上相距遥远的语言是指形式结构彼此完全不同的语言。这些结构差异体现在许多领域（例如句法（短语和句子结构）、语义（含义结构）和形态（词结构））。毫不奇怪，由于这些差异，在类型上相距遥远的语言（中文和英文、英文和阿拉伯文等）之间的翻译经常很糟糕，以至于不连贯或不可读。我们在这项工作中没有考虑这些语言，因为翻译质量通常很差，结果翻译的交换可能是难以置信的。</p>\n<h3 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h3><p>本节描述了实现的一些方面，重点介绍了用于获得生成的翻译变化的不同技术。</p>\n<h4 id=\"Translation-Engines\"><a href=\"#Translation-Engines\" class=\"headerlink\" title=\"Translation Engines\"></a>Translation Engines</h4><p>当前实现使用 Internet 上可用的不同翻译服务来获得初始翻译。当前的实现支持三种不同的服务，我们计划在未来添加更多服务。添加新服务只需要编写一个函数，将给定的句子从源语言翻译成目标语言。应使用可用 MT 服务的哪个子集由用户决定，但必须至少选择一个引擎。</p>\n<p>选择多个不同翻译引擎的一个可能问题是它们可能具有不同的错误特征（例如，一个引擎可能无法翻译带有缩写的单词）。知道特定机器翻译系统存在此类问题的对手可能会发现所有句子中有一半存在与这些特征匹配的错误。由于普通用户不太可能在不同的翻译引擎之间交替，这将揭示隐藏消息的存在。</p>\n<p>更好的选择是使用相同的机器翻译软件，但使用不同的语料库对其进行训练。特定语料库成为隐写编码器使用的密钥的一部分； Victor Raskin 和 Umut Topkara 之前在另一个上下文（[2] 的上下文）中讨论了这种使用语料库作为关键字的情况。因此，对手无法再检测到不同机器翻译算法导致的差异。这种方法的一个问题是获得好的语料库很昂贵。此外，划分单个语料库以生成多个较小的语料库将导致更糟糕的翻译，这可能再次导致可疑文本。也就是说，完全控制翻译引擎还可以允许翻译算法本身的微小变化。例如，GIZA++系统提供了多种计算翻译的算法[9]。这些算法的主要区别在于如何生成翻译“候选结果”。更改这些选项也有助于生成多个翻译。</p>\n<p>从翻译引擎获得一个或多个翻译后，该工具会使用各种后处理算法生成其他变体。只需使用一个高质量的翻译引擎并依靠后处理生成替代翻译，就可以避免使用多个引擎的问题。</p>\n<h4 id=\"Semantic-Substitution\"><a href=\"#Semantic-Substitution\" class=\"headerlink\" title=\"Semantic Substitution\"></a>Semantic Substitution</h4><p>语义替换是一种非常有效的 post-pass，并且已在以前的方法中用于隐藏信息 [2,5]。与以前工作的一个主要区别是，与原始文本中的语义替换相比，由语义替换引起的错误在翻译中更合理。</p>\n<p>传统语义替换的一个典型问题是需要替换列表。替换列表是由语义上足够接近的词组成的元组列表，可以在任意句子中用一个词替换另一个词。对于传统的语义替换，这些列表是手工生成的。语义替换列表中的一对单词的示例将是舒适和方便的。不仅手工构建替换列表很乏味，而且列表中包含的内容也必须是保守的。例如，一般替换列表不能包含诸如明亮和光之类的词对，因为光可以用于不同的意义（意味着轻松、不精确甚至用作名词）。</p>\n<p>翻译的语义替换没有这个问题。使用原始句子，可以自动生成语义替换，甚至可以包含上述某些情况（无法添加到一般单语替换列表中）。基本思想是在两种语言之间来回翻译以找到语义相似的单词。假设翻译是准确的，源语言中的单词可以帮助提供必要的上下文信息，以限制对当前上下文中语义接近的单词的替换。</p>\n<p>假设源语言是德语（d），翻译的目标语言是英语（e）。原始句子包含一个德语单词 d1<br>并且翻译包含一个单词 e1，它是 d1的翻译。基本算法如下，如图2所示：</p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E7%BF%BB%E8%AF%91%E7%9A%84%E9%9A%90%E5%86%99%E6%9C%AF/image-20210709175052191.png\" alt=\"image-20210709175052191\"></p>\n<ul>\n<li>找出 d1 的所有其他翻译的集合，并称这个集合为$E_{d1}$。 $E_{d1}$是语义替换的候选集。$e_1 \\in E_{d1}$。</li>\n<li>找出 e1 的所有翻译；将此集合称为 $D_{e1}$。此集合称为集合<em>witnesses</em>。</li>\n<li>对于每个单词$e \\in  E_{d1}-\\{e1\\}$找到所有的翻译 $D_{e1}$并计算$D_e \\cap D_{e1}$中元素的数量。如果该数字高于给定的阈值 t，则将 e 添加到 e1 的可能语义替代列表中。</li>\n</ul>\n<p>一个<em>witness</em>是源语言中的一个词，它也翻译成目标语言中的两个词，从而确认两个词的语义接近度。<em>witness</em>阈值 t 可用于将更多可能的替换与更高的不适当替换的可能性进行交换。</p>\n<h4 id=\"Adding-plausible-mistakes\"><a href=\"#Adding-plausible-mistakes\" class=\"headerlink\" title=\"Adding plausible mistakes\"></a>Adding plausible mistakes</h4><p>另一种可能的 post-pass 将 MT 系统常见的错误添加到翻译中。我们的实现可以使用的转换基于第 4 节中对 MT 错误的研究。当前系统支持使用手工制作的语言特定替换来更改冠词和介词，这些替换尝试模仿观察到的可能错误。</p>\n<h4 id=\"Results-from-the-Prototype\"><a href=\"#Results-from-the-Prototype\" class=\"headerlink\" title=\"Results from the Prototype\"></a>Results from the Prototype</h4><p>系统的不同配置产生不同质量的翻译，但即使质量下降也是不可预测的。有时我们的修改实际上（巧合）提高了翻译质量。</p>\n<p>应该注意的是，为简单起见，原型当前使用的引擎是公开可用的免费网络引擎，并且这不是自定义生成引擎或付费商业软件的输出的示范。为了更好地说明原型系统，给出了以下稍微更广泛的示例： 24 位字符串“lit”是在来自 Deutsche Welle 网站的电影评论部分的翻译中编码的。使用我们的原型将文本从德语翻译成英语，没有语义替换，启用冠词和介词替换，也没有“不良阈值”。源引擎是 Babelfish、Google 和 LinguaTec。德语文本是一段关于摩洛哥电影《风马》的评论的第一部分，内容如下：</p>\n<p>······省略······</p>\n<h3 id=\"Discussion\"><a href=\"#Discussion\" class=\"headerlink\" title=\"Discussion\"></a>Discussion</h3><p>本节讨论对隐写编码的各种攻击以及针对这些攻击的可能防御。讨论是非正式的，因为该系统基于 MT 的缺陷，这些缺陷很难正式分析（这也是 MT 是一个如此困难的话题的原因之一）。</p>\n<h4 id=\"Future-Machine-Translation-Systems\"><a href=\"#Future-Machine-Translation-Systems\" class=\"headerlink\" title=\"Future Machine Translation Systems\"></a>Future Machine Translation Systems</h4><p>【所提出的隐写编码在未来可能面临的一个可能问题是机器翻译的重大进展。如果机器翻译变得更加准确，那么可能出现的似是而非的错误可能会变得更小。然而，当前机器翻译错误的一大类是由于机器翻译器没有考虑到上下文。】</p>\n<p>为了显着改进现有的机器翻译系统，一个必要的功能是保存从一个句子到下一个句子的上下文信息。只有有了这些信息，才有可能消除某些错误。但是将这种上下文引入机器翻译系统也为在翻译中隐藏信息带来了新的机会。【一旦机器翻译软件开始保留上下文，使用隐写协议的两方就有可能使用这个上下文作为密钥。】通过为各自的翻译引擎植入 k 位上下文，他们可以使翻译中的偏差变得合理，迫使对手可能尝试$2^k$种可能的上下文输入，以便甚至确定使用该机制的可能性。这类似于基于密钥拆分语料库的想法，不同之处在于不会影响每句翻译的整体质量。</p>\n<h4 id=\"Repeated-Sentence-Problem\"><a href=\"#Repeated-Sentence-Problem\" class=\"headerlink\" title=\"Repeated Sentence Problem\"></a>Repeated Sentence Problem</h4><p>在翻译中隐藏消息的任何方法的一个普遍问题是，如果源语言中的文本包含两次相同的句子，它可能会被翻译成两个不同的句子，具体取决于隐藏位的值。由于机器翻译系统（不保留上下文）总是会产生相同的句子，这将允许攻击者怀疑使用了隐写术。解决这个问题的方法是不要在源文本中使用重复的句子来隐藏数据，而始终输出用于该句子第一次出现的翻译。</p>\n<p>这种攻击类似于图像隐写术中使用的攻击。如果图像经过数字化修改，图像某些不可信区域的颜色变化可能会揭示隐藏信息的存在。解决这个问题对于文本隐写术来说更容易，因为检测两个句子是否相同比检测图像中的一系列像素属于相同的数字构造形状并因此必须具有相同的颜色更容易。</p>\n<h4 id=\"Statistical-Attacks\"><a href=\"#Statistical-Attacks\" class=\"headerlink\" title=\"Statistical Attacks\"></a>Statistical Attacks</h4><p>统计攻击在击败图像、音频和视频的隐写术方面非常成功（参见，例如，[8,14,19]）。对手可能有一个统计模型（例如语言模型），所有可用 MT 系统的翻译都遵守该模型。例如，Zipf 定律 [15] 指出，一个单词的频率与其在所有单词的按频率排序的列表中的排名成反比。Zipf 定律适用于英语，事实上，甚至在名词、动词、形容词等个别类别中也适用。</p>\n<p>假设所有合理的翻译引擎通常都遵循这样的统计模型，隐写编码器必须小心不要导致与此类分布的明显偏差。一旦知道这样的统计规律，实际上很容易修改隐写编码器以消除明显偏离所需分布的翻译。例如，Golle 和 Farahat [10] 指出（在不同的加密上下文中）可以在不明显偏离 Zipf 定律的情况下广泛修改自然语言文本。换句话说，这是一个非常易于管理的困难，只要隐写系统是“Zipf-aware”的。</p>\n<p>我们不能排除尚未发现的翻译语言模型的存在，这些模型可能会被我们现有的实现所违反。然而，我们希望发现和验证这样的模型对于对手来说是一项重要的任务。另一方面，给定这样的模型（正如我们上面指出的）修改隐写系统很容易，通过避免被标记的句子来消除偏差。</p>\n","categories":[],"tags":["paper reading"]},{"title":"基于形容词删除策略的语言隐写与密钥共享","url":"https://hahally.github.io//articles/基于形容词删除策略的语言隐写与密钥共享/","content":"<blockquote>\n<p><a href=\"https://aclanthology.org/C12-1031/\" target=\"_blank\" rel=\"noopener\">Adjective Deletion for Linguistic Steganography and Secret Sharing</a></p>\n</blockquote>\n<p><strong>概念</strong></p>\n<ul>\n<li><p>Adjective Deletion 【形容词删除】</p>\n</li>\n<li><p>Linguistic Steganography 【语言隐写术】隐写术就是将秘密信息隐藏到看上去普通的信息中进行传送。</p>\n<blockquote>\n<p>Linguistic steganography is a form of covert communication in which information is embedded in a seemly innocent cover text so that the presence of the information is imperceptible to an outside observer (human or computer).</p>\n<p>理想的 Linguistic Steganography满足两个基本要求：high imperceptibility（不易察觉） and high payload capacity（高信息承载容量）</p>\n</blockquote>\n</li>\n<li><p>Secret Sharing 【密钥共享】一种分发、保存、恢复秘密密钥的方法。</p>\n</li>\n</ul>\n<p><strong>文章所作工作</strong></p>\n<ol>\n<li><p>验证删除形容词的可行性的两种方法：「checking the acceptability of adjective deletion in noun phrases.」</p>\n<ul>\n<li>Google n-gram corpus 【谷歌语料库】「check 删除一个形容词后的 <strong>context</strong> 的流利程度」</li>\n<li>SVM模型(使用n-gram counts和其他方法训练得到) 「classify 是否在 <strong>context</strong> 删除形容词」</li>\n</ul>\n</li>\n<li><p>证明删除形容词技术可以集成到一个存在的语言系统(an existing linguistic stegosyste)</p>\n</li>\n<li><p>提出一种新的基于形容词删除技术(adjective deletion)的密钥共享(secret sharing)方法</p>\n</li>\n</ol>\n<p><strong>(t,n)-threshold scheme</strong></p>\n<p>论文中采用的 secret sharing方法是基于(2, 2)-threshold, 其中共享的必须是两个可比较的文本(two comparable texts)。通过形容词删除技术将【0s 和 1s 的加密位字符串(secret bitstring;)】嵌入到两个文本中，这两个文本可以组合起来，获得秘密位串。</p>\n<blockquote>\n<p>Hence the proposed method is a novel combination of secret sharing and linguistic steganography.</p>\n</blockquote>\n<p>一种密钥共享与语言隐写技术的新颖组合方法？！</p>\n<p><strong>Adjective Deletion</strong></p>\n<p>在不影响句子流利程度和语义的情况下，可以将一些形容词删除。在下面的例子中，删除 <em>own</em> 这个形容词后，句意并没有发生改变。</p>\n<blockquote>\n<p>he spent only his own money.</p>\n<p>he spent only his money.</p>\n</blockquote>\n<p>一种极端情况 adjective-noun ：大致可以理解为正确的废话（正确但duck不必的形容）吧。</p>\n<blockquote>\n<p>unfair prejudice</p>\n<p>horrible crime</p>\n<p>fragile glass</p>\n</blockquote>\n<p><strong>隐写术种的语言转换(Linguistic Transformations for Steganography)</strong></p>\n<p>如：词汇替换、短语意译、句子结构调整、语义转换等【PS：有种毕业论文降重的赶脚】</p>\n<p>还有一种研究通过在翻译的文本中嵌入信息。在机器翻译算法中引入水印作为参数，对带有水印的译文进行概率识别。</p>\n<p>【Watermarking the outputs of structured prediction with an application in statistical machine translation】</p>\n<blockquote>\n<p>Another recent work proposedby Venugopal et al. (2011) introduces a watermark as a parameter in the machine translation algorithm and probabilistically identifies the watermarked translation.</p>\n</blockquote>\n<p><strong>隐写系统评估</strong></p>\n<p>可以从两个方面对系统进行评估：安全性(security level)和嵌入容量( embedding capacity)</p>\n<ol>\n<li><p>security level： automatic evaluation and human evaluation.</p>\n<p>automatic evaluation 大概就是使用机器翻译评价指标 BLEU 和 NIST。计算隐藏文本与原始文本之间的距离。</p>\n<p>human evaluation 就是认为指定的一套评估标准(seven-point scale)。</p>\n</li>\n<li><p>embedding capacity</p>\n<p>将嵌入的信息按每个语言单位(每个句子或每个单词)比特进行量化。</p>\n</li>\n</ol>\n<p>隐写系统的语言转换和编码方法，以及隐写文本的选择都会影响隐写系统的安全级别和有效负载能力。</p>\n<p><strong>句子压缩</strong></p>\n<p>句子压缩，文本简化和文本摘要通常涉及删除句子中不重要的词，以使文本更简洁。论文中指出，形容词删除可以用在句子压缩之前或之后。进一步简化句子。</p>\n<blockquote>\n<p>The proposed adjective deletion methods can be applied before and/or after a sentence compression system. Deleting unnecessary adjectives before can help the system focus on other content of a sentence. Deleting unnecessary adjectives after can generate an even more concise sentence.</p>\n</blockquote>\n<p><strong>Deletable Adjective Classification</strong></p>\n<p>论文中，为了使一个形容词的删除是可以接受的，使用两个检查：语法性和自然性检查(grammaticality and naturalness checks)。</p>\n<ol>\n<li><p>N-gram Count 方法</p>\n<p>计算删除形容词前后文本的 N-gram 统计得分，通过设置一个阈值，来判断删除后的文本是否可接受。</p>\n</li>\n<li><p>Features for the SVM</p>\n<p>支持向量机的特征有：</p>\n<ul>\n<li>N-gram Counts</li>\n<li>Lexical Association Measures【确定形容词和名词之间的关联程度。】</li>\n<li>Noun and Adjective Entropy【名词和形容词熵】</li>\n<li>Contextual α-Skew Divergence【上下文的倾斜散度？】</li>\n</ul>\n</li>\n</ol>\n<p><strong>Secret Sharing Scheme</strong></p>\n<p>将一个密钥位串分成两个部分$share_0$和 $share_1$ 。若目标形容词在$share_0$ 中保留，则密钥值取0，若目标形容词在$share_1$中保留，则密钥值取1。</p>\n<blockquote>\n<p>Share0 holds secret bits as 0s and Share1 holds secret bits as 1s</p>\n</blockquote>\n<p>下面是一个密钥位串为 101 的例子：</p>\n<p><img src=\"/articles/%E5%9F%BA%E4%BA%8E%E5%BD%A2%E5%AE%B9%E8%AF%8D%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E7%9A%84%E8%AF%AD%E8%A8%80%E9%9A%90%E5%86%99%E4%B8%8E%E5%AF%86%E9%92%A5%E5%85%B1%E4%BA%AB/image-20210708213208658.png\" alt=\"image-20210708213208658\"></p>\n","categories":[],"tags":["paper reading"]},{"title":"蛋白质结构预测之lgb的baseline","url":"https://hahally.github.io//articles/蛋白质结构预测之lgb的baseline/","content":"<p>赛题：<a href=\"https://challenge.xfyun.cn/topic/info?type=protein\" target=\"_blank\" rel=\"noopener\">蛋白质结构预测挑战赛</a></p>\n<p>代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">################## utils.py #####################</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read_fa</span><span class=\"params\">(file, mode=<span class=\"string\">'train'</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> mode <span class=\"keyword\">in</span> &#123;<span class=\"string\">'train'</span>,<span class=\"string\">'test'</span>&#125;</span><br><span class=\"line\">    labels = []</span><br><span class=\"line\">    seqs_info = []</span><br><span class=\"line\">    cates_id = []</span><br><span class=\"line\">    seq = <span class=\"string\">''</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file,mode=<span class=\"string\">'r'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        line = f.readline().strip()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> line:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> line[<span class=\"number\">0</span>]==<span class=\"string\">'&gt;'</span>:</span><br><span class=\"line\">                info = line[<span class=\"number\">1</span>:].split(<span class=\"string\">' '</span>)</span><br><span class=\"line\">                cates_id.append(info[<span class=\"number\">0</span>])</span><br><span class=\"line\">                <span class=\"keyword\">if</span> mode == <span class=\"string\">'train'</span>:</span><br><span class=\"line\">                    labels.append(<span class=\"string\">''</span>.join(info[<span class=\"number\">1</span>].split(<span class=\"string\">'.'</span>)[:<span class=\"number\">2</span>]))</span><br><span class=\"line\">                <span class=\"keyword\">if</span> seq:</span><br><span class=\"line\">                    seqs_info.append(seq)</span><br><span class=\"line\">                    seq = <span class=\"string\">''</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                seq += line</span><br><span class=\"line\">            line = f.readline().strip()</span><br><span class=\"line\">        seqs_info.append(seq)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> cates_id,seqs_info,labels</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">################## main.py #####################</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> utils <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> f1_score, fbeta_score, precision_score, recall_score, roc_auc_score</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> StratifiedKFold <span class=\"keyword\">as</span> KFold</span><br><span class=\"line\"><span class=\"keyword\">import</span> lightgbm <span class=\"keyword\">as</span> lgb</span><br><span class=\"line\"></span><br><span class=\"line\">train_file = <span class=\"string\">'./训练集/astral_train.fa'</span></span><br><span class=\"line\">test_file = <span class=\"string\">'./测试集/astral_test.fa'</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">train_sample_id, train_seqs_info, train_labels = read_fa(train_file, mode=<span class=\"string\">'train'</span>)</span><br><span class=\"line\">test_sample_id, test_seqs_info, _ = read_fa(test_file, mode=<span class=\"string\">'test'</span>)</span><br><span class=\"line\">train_data = &#123;</span><br><span class=\"line\">    <span class=\"string\">'sample_id'</span>: train_sample_id,</span><br><span class=\"line\">    <span class=\"string\">'seq_info'</span>: train_seqs_info,</span><br><span class=\"line\">    <span class=\"string\">'label'</span>: train_labels</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">test_data = &#123;</span><br><span class=\"line\">    <span class=\"string\">'sample_id'</span>: test_sample_id,</span><br><span class=\"line\">    <span class=\"string\">'seq_info'</span>: test_seqs_info,</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">label_map = &#123;l:idx <span class=\"keyword\">for</span> idx,l <span class=\"keyword\">in</span> enumerate(set(train_labels))&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">rev_label_map = &#123;v:k <span class=\"keyword\">for</span> k,v <span class=\"keyword\">in</span> label_map.items()&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># print(label_map)</span></span><br><span class=\"line\"></span><br><span class=\"line\">train = pd.DataFrame(data=train_data)</span><br><span class=\"line\">test = pd.DataFrame(data=test_data)</span><br><span class=\"line\"></span><br><span class=\"line\">train[<span class=\"string\">'label'</span>] = train[<span class=\"string\">'label'</span>].map(label_map)</span><br><span class=\"line\"></span><br><span class=\"line\">alp = list(set(<span class=\"string\">''</span>.join(train_seqs_info + test_seqs_info)))</span><br><span class=\"line\"></span><br><span class=\"line\">train[<span class=\"string\">'seq_len'</span>] = train[<span class=\"string\">'seq_info'</span>].apply(<span class=\"keyword\">lambda</span> x:len(x))</span><br><span class=\"line\">test[<span class=\"string\">'seq_len'</span>] = test[<span class=\"string\">'seq_info'</span>].apply(<span class=\"keyword\">lambda</span> x:len(x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> alp:</span><br><span class=\"line\">    train[<span class=\"string\">'count_'</span>+s] = train[<span class=\"string\">'seq_info'</span>].apply(<span class=\"keyword\">lambda</span> x:x.count(s))</span><br><span class=\"line\">    train[<span class=\"string\">'freq_'</span>+s] = train[<span class=\"string\">'seq_info'</span>].apply(<span class=\"keyword\">lambda</span> x:x.count(s)/len(x))</span><br><span class=\"line\">    </span><br><span class=\"line\">    test[<span class=\"string\">'count_'</span>+s] = test[<span class=\"string\">'seq_info'</span>].apply(<span class=\"keyword\">lambda</span> x:x.count(s))</span><br><span class=\"line\">    test[<span class=\"string\">'freq_'</span>+s] = test[<span class=\"string\">'seq_info'</span>].apply(<span class=\"keyword\">lambda</span> x:x.count(s)/len(x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">feats = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> train.columns <span class=\"keyword\">if</span> i <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> [<span class=\"string\">'label'</span>,<span class=\"string\">'sample_id'</span>,<span class=\"string\">'seq_info'</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># print(feats)</span></span><br><span class=\"line\"></span><br><span class=\"line\">x_train = train[feats]</span><br><span class=\"line\">y_train = train[<span class=\"string\">'label'</span>]</span><br><span class=\"line\">x_test = test[feats]</span><br><span class=\"line\"></span><br><span class=\"line\">   </span><br><span class=\"line\">params = &#123; </span><br><span class=\"line\">    <span class=\"string\">'boosting_type'</span>: <span class=\"string\">'gbdt'</span>,  </span><br><span class=\"line\">    <span class=\"string\">'objective'</span>: <span class=\"string\">'multiclass'</span>,  </span><br><span class=\"line\">    <span class=\"string\">'num_class'</span>: <span class=\"number\">245</span>,  </span><br><span class=\"line\">    <span class=\"string\">'metric'</span>: <span class=\"string\">'multi_error'</span>,  </span><br><span class=\"line\">    <span class=\"string\">'num_leaves'</span>: <span class=\"number\">300</span>,  </span><br><span class=\"line\">    <span class=\"string\">'min_data_in_leaf'</span>: <span class=\"number\">500</span>,  </span><br><span class=\"line\">    <span class=\"string\">'learning_rate'</span>: <span class=\"number\">0.007</span>,  </span><br><span class=\"line\">    <span class=\"string\">'max_depth'</span>: <span class=\"number\">8</span>,</span><br><span class=\"line\">    <span class=\"string\">'feature_fraction'</span>: <span class=\"number\">0.8</span>,  </span><br><span class=\"line\">    <span class=\"string\">'bagging_fraction'</span>: <span class=\"number\">0.8</span>,  </span><br><span class=\"line\">    <span class=\"string\">'bagging_freq'</span>: <span class=\"number\">5</span>,  </span><br><span class=\"line\">    <span class=\"string\">'lambda_l1'</span>: <span class=\"number\">0.4</span>,  </span><br><span class=\"line\">    <span class=\"string\">'lambda_l2'</span>: <span class=\"number\">0.5</span>,  </span><br><span class=\"line\">    <span class=\"string\">'min_gain_to_split'</span>: <span class=\"number\">0.2</span>,  </span><br><span class=\"line\">    <span class=\"string\">'verbose'</span>: <span class=\"number\">-1</span>,</span><br><span class=\"line\">    <span class=\"string\">'num_threads'</span>:<span class=\"number\">2</span>,</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 五折交叉验证</span></span><br><span class=\"line\">folds = KFold(n_splits=<span class=\"number\">5</span>, shuffle=<span class=\"literal\">True</span>, random_state=<span class=\"number\">2021</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">oof = np.zeros([len(x_train),<span class=\"number\">245</span>])</span><br><span class=\"line\">predictions = np.zeros([len(x_test),<span class=\"number\">245</span>])</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">for</span> fold_, (trn_idx, val_idx) <span class=\"keyword\">in</span> enumerate(folds.split(x_train, y_train)):</span><br><span class=\"line\">    print(<span class=\"string\">\"fold n°&#123;&#125;\"</span>.format(fold_+<span class=\"number\">1</span>))</span><br><span class=\"line\">    trn_data = lgb.Dataset(x_train.iloc[trn_idx], y_train.iloc[trn_idx])</span><br><span class=\"line\">    val_data = lgb.Dataset(x_train.iloc[val_idx], y_train.iloc[val_idx])</span><br><span class=\"line\"> </span><br><span class=\"line\">    num_round = <span class=\"number\">1000</span></span><br><span class=\"line\">    clf = lgb.train(params, </span><br><span class=\"line\">                    trn_data, </span><br><span class=\"line\">                    num_round, </span><br><span class=\"line\">                    valid_sets = [trn_data, val_data], </span><br><span class=\"line\">                    verbose_eval = <span class=\"number\">100</span>, </span><br><span class=\"line\">                    early_stopping_rounds = <span class=\"number\">50</span>)</span><br><span class=\"line\">    oof[val_idx] = clf.predict(x_train.iloc[val_idx][feats], num_iteration=clf.best_iteration)    </span><br><span class=\"line\">    predictions += clf.predict(x_test, num_iteration=clf.best_iteration) / folds.n_splits</span><br><span class=\"line\">    <span class=\"comment\">#print(predictions)</span></span><br><span class=\"line\"></span><br><span class=\"line\">x_test[<span class=\"string\">'sample_id'</span>] = test[<span class=\"string\">'sample_id'</span>]</span><br><span class=\"line\">x_test[<span class=\"string\">'category_id'</span>] = [rev_label_map[list(x).index(max(x))] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> predictions]</span><br><span class=\"line\">x_test[<span class=\"string\">'category_id'</span>] = x_test[<span class=\"string\">'category_id'</span>].apply(<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">0</span>]+<span class=\"string\">'.'</span>+x[<span class=\"number\">1</span>:])</span><br><span class=\"line\">x_test[[<span class=\"string\">'sample_id'</span>, <span class=\"string\">'category_id'</span>]].to_csv(<span class=\"string\">'base_sub.csv'</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_pre = oof.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"F1 score: &#123;&#125;\"</span>.format(f1_score(y_train, y_pre,average=<span class=\"string\">'micro'</span>)))</span><br><span class=\"line\">print(<span class=\"string\">\"Precision score: &#123;&#125;\"</span>.format(precision_score(y_train, y_pre,average=<span class=\"string\">'micro'</span>)))</span><br><span class=\"line\">print(<span class=\"string\">\"Recall score: &#123;&#125;\"</span>.format(recall_score(y_train, y_pre,average=<span class=\"string\">'micro'</span>)))</span><br></pre></td></tr></table></figure>\n<p>提交结果：目前【14/27(提交团队数)】</p>\n<p><img src=\"/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B%E4%B9%8Blgb%E7%9A%84baseline/image-20210703193646587.png\" alt=\"image-20210703193646587\"></p>\n<p>主要是提取了氨基酸组成(AAC)特征，即一些简单的统计特征。没有考虑氨基酸之间的相对位置信息，也没有必要调参，最后预测结果也很是拉跨。</p>\n<p>下一步直接尝试<code>nlp</code> 相关模型。</p>\n","categories":[],"tags":["BDC"]},{"title":"Problem-Container","url":"https://hahally.github.io//articles/Problem-Container/","content":"<blockquote>\n<p>前言</p>\n</blockquote>\n<p>这里收纳遇到的一些<code>Problem</code> ，包含一些生活中遇到的人或事。是泛指，而绝不是某类问题。想法来源于<code>SCP</code> 基金会，也和一首歌有关《瓶与抛光者》。而起因却是迪迦与伍六七的下架。</p>\n<p><em>我感觉我们正失去一些美好的东西，善良、温柔、体贴，这些属于我们人类美好的本质，好像正慢慢淡化！— 新城</em></p>\n<p><em>不拼尽全力试一下又怎么会知道啊！ —阿七</em></p>\n<p>随着时间推移，愈发感觉对于很多事情都很无力。<em>被矫枉过正的灵魂，嘴硬蛮横</em>。</p>\n<p>没有那么果敢（三思而后行/胆怯），没有那么坦诚（冷暖自知/尔虞我诈）、没有那么期待（活在当下/躺平）……然后成熟稳重。</p>\n<p>要藏起一些光芒（谦逊/自卑），收敛一些个性（低调/普通），寻找一些共性（合群/跟风）……然后求同存异。</p>\n<p>大道理听了很多，依旧过不好。可总是有人唠叨，也总是有人听。以至于我们活的越来越像别人……</p>\n<p>当大人不在苦口婆心耐心教导小孩明辨是非时，他们开始试图将自认为不适合小孩的东西封杀雪藏。可谓另辟蹊径，偷教育的懒。似乎想达到<em>同步教育</em> 的目的。</p>\n<p><em>同步教育： 并不是所有家长都能管好自家小孩，譬如打游戏，看电视。于是举报游戏，举报动画片，以此达到所有小孩都玩不了游戏，看不了动画片的目的，这样看似省心省事。</em> </p>\n<p>几十载的一生，活着的意义又是什么呢。以前什么都不懂，过一天是一天，现在懂的多了，过一天算一天，还要总结昨天，规划明天，才能过好今天。</p>\n<p>夜深人静，当我们思考这一天，或最近的状况时，都企图得出一些人生大道理，悟出生活中的真谛，寻求内心的平静。自己到底是一个什么样的人？对于这个问题，好像每天都有不同的答案。</p>\n<p>于是在百无聊赖的生活里，寻找明天继续的理由……</p>\n<p>最后感恩！迪迦又上架了。</p>\n<p><code>Problem Container</code> 将作为理由之一，在互联网的大海里漂流。以下是收容的一些<code>Problem</code> 。</p>\n<blockquote>\n<p>布丁</p>\n</blockquote>\n<p>——2020.8.21 ——</p>\n<p>布丁是妹妹过年时带回家的宠物狗。全身白色，眼睛很大，个子却很小，年龄有一年多了吧。听说是花五十块买的。</p>\n<p>记得刚到家时，它很是胆小，只敢跟在妹妹脚边转悠。布丁这个名字只有我妹妹叫，才有用，其他人不行。后来慢慢熟悉了，它也开始放的开了。每天都会发疯一样来回窜。布丁好像对一切充满了好奇，能吃的，都要尝一尝，以至于给它的狗粮，成了它最不济的选择。</p>\n<p>家里有只猫，布丁很怕猫，每次布丁想去接近猫的时候，猫都会用爪子示意。猫有时会去偷吃狗粮，布丁看见了，只能在旁边看着干着急，然后发出委屈的声音。有次它跑过来挠我的脚，示意我把猫弄走。我把猫赶走后，布丁就开始吃狗粮了。虽然，它不喜欢吃狗粮，但也不许别人动它的东西。</p>\n<p>妹妹回深圳上班了，特殊时期，只能把狗留在家里了。那天晚上布丁没有见到妹妹，它就在妹妹房间门口等，眼睛泪汪汪的，身体蜷缩着，压在妹妹的拖鞋上面。我打开房间门，布丁立马起身跑进去，然后扒拉床沿，看妹妹在不在上面。确认没有后，它失望地走了出来。</p>\n<p>从此，布丁就交给我了。每天早上布丁会早早地起来扒拉床沿，把我弄醒，示意我开门下楼放它出去上厕所。刚开始，看着它越走越远时，还是有些担心的，怕它不知道回来。庆幸的是，每次在我焦急等着喊它名字的时候，它都兴冲冲地跑回来了。</p>\n<p>布丁好像很孤独。那段时间，明天都上网课，然后它就趴在地上，在旁边陪着我。久而久之，布丁已经摸清我的习惯了。什么时候下课，什么时候吃饭，什么时候睡觉，什么时候在房间里。我也知道每天早上七点下楼开门放它出去玩半个小时，然后回来陪我上网课，中午大概十二点和我一起下楼去吃午饭，吃完后，我会先上楼，布丁会在一点多的时候，上楼来挠我房间门，示意我开门。开门后，布丁会先喝水，然后走到窝里睡午觉。布丁晚上睡觉很早，但是我每天都会熬到十二点多才睡，开着灯，有些影响它。它的耳朵很灵，晚上我关灯躺床上刷手机，一点点声音，它就会过来床边，两只爪子扒在床上，然后抓蚊帐，示意我关掉。有段时间，它可以和周边其它狗打成一片了，甚至成了团宠。我感到有些欣慰，但也有些忧虑。其他狗都是散养的，乡下那种土狗。布丁是个娇生惯养的宠物狗，和土狗打成一片，不可避免要惹上一些虫子。每天给布丁洗澡，成了我每天最烦恼的事情。有那么几次，被朋友约出去玩。上午出门，傍晚才回家。每次回家刚把钥匙插进去，布丁就知道我回来了，马上从楼上跑下来，汪汪地叫着。我一进去，布丁就开始扒拉我的脚，时不时发出一种憋屈地要哭了的声音。看得出，布丁在用尽它所有的动作或表情来欢迎我。它大概是以为我不回来了，把它抛弃了吧。</p>\n<p>布丁是会哭的。妹妹回来过一次，布丁开心得活蹦乱跳的，两只小脚不停的扑妹妹。好像在倾其所有表达它有多爱妹妹。妹妹没让布丁去她房间里，依旧把它留在我房间。布丁不停的用爪子挠门，想要出去，进妹妹房间。见我不开门，布丁开始急了，扑我身上，用一只小爪子挠我，发出委屈的声音，两眼泪汪汪的，眼泪都出来了。我开门后，它又不停的去挠妹妹房间的门。最后累了就躺在妹妹拖鞋上面，蜷缩着身子，等着。</p>\n<p>几个月的时光，就这样患得患失地过来了。很快到了返校时间。一大早就急匆匆的离开了，布丁应该以为我会像以前一样，到了晚上就会回来吧。它大概在楼上阳台观望，等待，直到夜幕降临。</p>\n<p>陪我半年的布丁，无疑是我这半年来最大的收获。朝夕相处的那段时光，算是我最开心的时候吧。</p>\n","categories":[],"tags":[]},{"title":"九品炼丹师","url":"https://hahally.github.io//articles/九品炼丹师/","content":"<blockquote>\n<p>前言</p>\n</blockquote>\n<p>第一次参加<code>cv</code> 赛事，由清华举办的一场<code>AI 挑战赛</code> , 旨在推广 <code>jittor</code>框架的吧。<a href=\"https://www.educoder.net/competitions/index/Jittor-2\" target=\"_blank\" rel=\"noopener\">传送门</a></p>\n<p>共有两个赛道，一个细分类，一个目标检测。由于有一个毕设与目标检测相关，于是毫不犹豫的报名参加了。算是入坑<code>DL</code> 了。前期在<code>tensorflow</code> 、<code>pytorch</code> 、<code>jittor</code> 三大框架之间反复横跳，最后还是抛弃了<code>tf</code>。主要是服务器上的<code>tf</code>用不了显卡的算力。环境问题懒得去倒腾了。<code>pytorch</code> 上手也很快，而且与<code>jittor</code> 相似。</p>\n<p>选着狗细分类这个赛道试水，结果差点每淹死在水里面。查阅了许多细分类的论文，一个个提到说效果达到<code>SOTA</code> ，结果到自己手里就废了。</p>\n<blockquote>\n<p>在好的配方，也能被炼废。 </p>\n</blockquote>\n<p>拿到配方，丹炉架好，药材就绪，大力按下回车键后，看着进度条缓缓加载，epoch 1,2,3,…</p>\n<p>这是一个漫长的过程，睡一觉第二天醒来，观察各项指标变化，没有预期那么好，却也差强人意。点击提交后，果然，依旧没有好的效果。2021.2.19，在尝试好几种配方，反复炼丹数十余日后，最终还是以失败告终。</p>\n<blockquote>\n<p>高端的食材往往只需要简单的烹饪。</p>\n</blockquote>\n<p>按照 <code>baseline</code>的方法，仅仅只是使用了一个简单的<code>resnet50</code>分类网络而已，最后的效果却要高于我各种花里胡哨的方法好几个百分点。开源的基线已是我望尘莫及的极限了。着实有些颓废。</p>\n<blockquote>\n<p>I know nothing but my ignorance.</p>\n</blockquote>\n<p>炼丹之路注定是布满荆棘的坎坷之路。才疏学浅，当厚积薄发才是。</p>\n","categories":[],"tags":[]},{"title":"2020","url":"https://hahally.github.io//articles/2020/","content":"<p>说来搞笑，翻看去年的记录这样写道：</p>\n<blockquote>\n<p>2020.12.31 总结</p>\n<p>想总结来着，但仔细一想，实在不知道如何说起。那就祝自己少活几十年吧，哈哈哈哈哈。</p>\n</blockquote>\n<p>2020我记录了很多天发生的事情。在入睡前，回想一下近况如何。</p>\n<hr>\n<p>关于过年，除夕夜自己一个人在房间里敲着代码，跑着深度学习的第一个demo，想来马上就是自己入坑深度学习一年了。第二天春节，和往常一样待在房间里追剧、打游戏、敲代码。好像和平常没什么两样。我会说：当我不去在乎节日里的仪式感，节日也就是平常，我也就把每天都当作节日一样过着了。春节快乐！</p>\n<p>2020.2.15，有关考研，自己也很纠结。可能是对自己实力的不自信，也可能是对自己本科四年的失望吧。细想过去的三年，无非在应付无关紧要的作业，参加可有可无的活动，平衡泛泛之交的同学关系，权衡尔虞我诈中的利弊。很是失败，也很无奈。我说：我在百无聊赖的生活里，寻找明天继续的理由……</p>\n<p>关于五一，记得前一天四月三十号，组建了第一个大数据比赛队伍，队伍名字就叫430。这一天回想了过去几个月的状态。本以为2020年是个好兆头，会是一个对我来说，重新开始的一年。我以为所有的不快、压抑、不良情绪都抛在了2019年。可是并没什么改变，我发现我依然无法和过去和解，尽管我几乎不记得发生了什么。当我开始搜索抑郁症相关信息时，我发现自己的状况好像并没有那么乐观。我无法确定，自己在互联网里寻找到的蛛丝马迹是为了得到什么样的答案。如果有一天，自己去医院就诊，然后被医生判定重度抑郁时，我大概在想：是我把医生骗了，还是医生在骗我。总之花了不少钱。我会说：我无数次把房间门反锁，这个时候，我总觉得我还算快乐……</p>\n<p>2020.6.23，记录中这是熬夜的一天，月初开始的大数据挑战赛初赛即将结束，月初开始的专业综合实训也接近尾声。昨天开始写实训报告，在今天凌晨三点左右凑够了9000+字数。睡一觉接着补15天的实训日记，第一次觉得200字的日记也是真的难写。BDC初赛结束也只有四天了，每天看着榜单上的名次往下掉，自己却一点法子都没有。询问老师，得到的回复总是“你用的什么模型”，或者抛来几个博文链接给我。值得庆幸的是目前还没有掉出榜单前100，还是有一丝机会的。最后几天还能挣扎一下。我会说：所有不能打败你的，都将使你变得更强……</p>\n<p>2020.7.20，这一天是和自己谈判的一天。逃离家庭，却终将要与自己和解。和平相处，peace and love……。这天我会说：最后这故里我想我是不会回来的……</p>\n<p>2020.7.30，靠着一点运气，以51的名次苟进了复赛。新建无数次的notebook，处理了无数次的数据、尝试了各种模型，效果依旧不如人意。最后的nn模型是我最后的倔强了，经过接近十个小时的模型训练，最终以收敛失败告终。我开始思考，自己这十几天的思路与付出是否真的是错的。思索良久，决定重新开始。重新清洗数据，重新做特征工程……我安慰自己：不是所有努力都会有回报，最终收不收敛，还得看你努力的方向是否正确……</p>\n<p>2020.8.10，比赛截至，苟进复赛，止步于此，34名。算是满意吧。从六月初到现在也熬过两个月了。期间有过很多次想过放弃，一方面指导老师没怎么指导，另一方面自己还是个入门两三个月的机器学习练习生而已。但是不知道为什么，还是抱着忍一忍的心态，挺了过去。算是在老师那刷刷存在感吧，毕竟考研可能要报他那。漫长黑夜，对着电脑，笨拙的手指在黑白机械键盘上敲出清脆的声音，这是我喜欢的音乐，这是我喜欢的世界。我会想：所有的坚持，都会是为未来埋下来的伏笔吧……</p>\n<p>2020.8.15，这天生日，已然苟活整整21年。几十载的一生，活着的意义又是什么呢。小时候什么都不懂，过一天是一天，现在懂得多了，过一天算一天，还要总结昨天、规划明天，才能过好今天。一个普通人的一生该是什么样的？二十到三十，这十年该如何度过，我也只能走一步看一步了。我很庄严的告诉自己：管他三七二十一，先做自己想做的事，说自己想说的话，走自己想走的路……</p>\n<p>2020.8.21，这天是回学校的日子。记录了我和布丁相处的半年多的时间的一些事情。它大概以为我会晚上到点就回来，但是这次真的好久都不会回来了。若是重来，我想对它说：你好，布丁，招待不周，还望见谅。</p>\n<p>2020.9.26，很多事都不一样了，在表示同意赞赏nb还行的同时，其实内心也在保持着一些最后的倔强甚至不屑的态度。向往诗和远方的同时，也在吐槽当下糟糕的境况。这是暂时的妥协，而不是最后的结果。可以预见的是，有一天，我也会被一块大饼圈住，为别人给的蛋糕沾沾自喜，因为天上掉的馅饼开始信奉神明，在推杯换盏中周旋，吃饱了面包然后驻足休息，养老等死，我的墓志铭大概就是我的第一个”hello world”代码。这是我最后的倔强，而不是暂时的妥协。技术无罪，资本作祟的时代，人人都好像鬼怪，争夺面包，吸食人xie，手捧圣经，说着抱歉，最后还不忘总结，口感似乎差了点……</p>\n<p>2020.10.4，国庆假期，这天我在学校。记录里这样写着：不可否认的是，大学里，我尝试过很多方法去改变我那自卑内敛的性格。参加社团，参加义务家教，学着别人的口吻说话，练习不让人讨厌的微笑……努力让自己看起来有些自信。十八九岁总带有一些理想的冲动，二十几岁了相对来说多了些理性的思考和对未知的焦虑。不可否认的是，很多很多困扰我的问题，对于上大学的我，依旧难以给出理智的回答。就连一日三餐吃什么，都要犹豫半天。我会说：大道理听了很多，依旧过不好。可总是有人唠叨，也总是有人听。以至于我们活的越来越像别人……</p>\n<p>2020.10.5，长沙13℃，风很大。去图书馆的路上，遍地桂花。一场雨，一阵风，桂花落，好像宣告秋天结束，冬天来了。然而现在才10月份开头。到图书馆已经是八点半了，进自习室，发现差不多坐满了，剩下些零零散散的位置。索性经常坐的位置没有人占。平常一般六点半起床，大概七点半到。最近放假，放松了几天，生物钟还在慢慢调回去。十一点多已经快饿得不行了。早上买的肉夹馍和豆浆，只是吃了几口就扔了。找到适合我的早餐或许到毕业都不太可能吧。自己已经感觉到明显的头晕想睡觉，之前一直以为是休息不好，直到最近才意识到自己这是低血糖的症状。食堂只开张了寥寥几家店，但是留校的人却很多。选择了一家人少的馄饨面。结果忘记叫他不要放葱了。打包回寝室，边看剧边挑着馄饨面里的葱。又是一个多小时的午餐。寝室睡一觉，接着去图书馆学习。拿出历年英语真题开始做阅读理解，到目前为止，正确率依旧只有百分之五十左右。很是崩溃，单词记得也不多。做完已经快五点，好像又开始低血糖了。于是玩了一下手机。五点过后开始收拾东西走了。晚饭去西门吃了，顺便剪个头发。接着逛超市，买了一些零食和糖果。晚上睡觉前，有关智齿的问题纠结了我大半个小时，第一次长，害怕拔牙的痛苦。逮到人就问长了智齿没有。这颗智齿好像是最近开始长的，没有多大。算是给我这平平淡淡的生活徒增了一些烦恼，又或者埋下了一个小小的伏笔……智齿好像每个人都会长，但不是每个人都需要拔，除非它发炎。所以真特么刺激，这可爱的智齿……</p>\n<p>2020.10.7，憨憨从望城那边赶了回来。一进寝室就说快祝我回归单身。于是开始聊了起来。他说其实分开很久了，九月份的时候就分了。什么原因，他说很复杂。现在他们唯一的共同财产就是那条狗了。反正我是不太相信的，坐等他们复合请吃饭吧。如果他们都走不到最后，那我觉得所谓爱情也不过如此了。二十几岁的我，现在也没有特别想谈恋爱的冲动。现在的我，还有很多问题无法回答，很现实，也很必要。我常常想：我们好像都在换位思考，理智分析各自的问题，却依旧无法做出满意的决定……</p>\n<p>2020.10.11，这一天有感而发，层出不穷的社会丑闻，每天都在发生。在事情没有结果的时候，就已经有人抢占道德高地，开始火力全开，声张自己的正义，字里行间表达的都是我是对的。舆论一边倒，好像已经板上钉钉了。当结果出乎意料时，所有人惊呼人心叵测，却少有人思考其中的前因后果。我们只在乎吃瓜时的快感，那股正义凛然的劲。指尖在键盘上轻轻舞动，就能让互联网的另一头引发轩然大波。我明白：人言可畏，在互联网中，表现的如此淋漓尽致。</p>\n<p>2020.10.14，最近被大连理工研究生自杀的新闻刷屏了。是啊，好好的一个人怎么就这样了。作为一名大四考研生，看到这个新闻时，第一想到的不是自己研究生的情况怎么样，而是在想，一个平常看起来挺正常的人，到底积攒了多少的糟糕的心情，才突然爆发了呢。有时候我也会觉得自己就是那样的人，对于很多事无奈的时候，就干脆做一个别人眼中认为的那个人。在不同角色之间来回切换，如同推杯换盏中达成一致。我说：在平衡内心与周遭的过程中，缝缝补补自己眼中那千疮百孔的世界……</p>\n<p>2020.10.15，记录着这样一段话：寻求一些期许，满足对未来的幻想，提醒今天的自己，你要加油啊，社畜……</p>\n<p>2020.10.18，早早洗完澡，就躺床上去了。刷着手机，思绪突然凌乱。胡思乱想中，脑海里浮现了布丁的身影。我无法想象在未来某一天，布丁生命终结，我该如何面对。这是我一开始就害怕面对的问题。与它待的每一天，我好像都会害怕眼前美妙的情景在未来哪天突然消失。我不擅长面对这样的场面，我应该也很很难走出来。至少生命中，某些时刻，会时不时浮现脑海里，就像现在这样。有些珍贵的情感，不需要触景而生，就在生活中的某些时刻突然想起，无需刻意回忆。</p>\n<p>2020.10.19，这一天记录了一篇小说偷影子的人。这是一部小说，以童年的故事作为铺垫，以成人后的追忆把小说推向高潮。小说不长，却很抓人。小说里那种细腻的感触，总是不经意间出现。我们自以为忘记了过去，但生活自会把我们引进回忆，然后拾起一段又一段珍贵的情感……”随着时间的流逝，有些事自会迎刃而解。”，”你偷走了我的影子，不论你在哪里，我都会一直想着你。”最近总是半夜醒来，说不清楚什么原因。可能是睡得早了，可能是做了噩梦。近来确实容易做梦，梦见布丁去世，梦见与爸妈吵架，梦见一些生活的一些事的后续，事情发展越发荒唐……我开始害怕自己的影子被别人偷走，我不太能确定它是否可靠，是不是轻易就能说出我不愿说的心事。我不相信我的影子。我不太愿意回忆过去了，甚至开始选择性的去忘记一些事情。我希望一切往前看，即使过去一团糟，我依旧觉得明天是我可以逃避一些事的地方，在那里前途一片光明。不问过去：我尽量准备好明天的到来，尽管今天过得并不怎么样……</p>\n<p>2020.10.25，这一天的标题是寻人启事。考研让我的生活变得规律起来。早上六点半起床，七点吃早饭，七点半到图书馆学习，然后十一点吃午饭，十一点半回寝室睡觉，十二点半返回图书馆学习，然后直到下午五点，吃完晚饭大概六点，接着学到九点多，然后回寝室洗澡洗衣服刷牙，最后睡觉。在路上常常有种行尸走肉般的感觉，机械的，按部就班的，过着每一天。这样也挺好，忙起来，把生活填满，就不必去思考其他的东西了。可又好像不是这样，我考虑半天三餐吃什么，我可能会思考吃哪一家的，我会计较哪家喜欢放葱，我会犹豫半天吃饭还是吃粉，炒饭还是炒粉，明天什么天气，是否有雨，温度多高，要复习什么内容……我每天都在纠结着这些好像微不足道的小事。夜深了，躺在床上，打着字，思考这一天，或最近的状况。企图得出一些人生大道理，悟出生活中的真谛，寻求内心的平静。自己到底是一个什么样的人？对于这个问题，好像每天都有不同的答案。我想写一份寻人启事，寻找真正的自己……</p>\n<hr>\n<p>往后的时间，在刷卷子中度过。自习室里，笔在一张张A3纸上沙沙作响，垫在桌子上的纸一张一张的叠着，慢慢的，越铺越厚。考完那天，回到自习室，抽出那一摞纸张，然后重重地扔进了垃圾堆里，宣告结束了。2020年也结束了。</p>\n<hr>\n<p>2021了啊！</p>\n<p>2021.1.10，记录了一次失眠。今晚又失眠了。莫名其妙的情绪低落，时不时流眼泪。明明睡前还有说有笑的啊。真是奇怪。不知道从什么时候开始，我好像更希望有个认识的能够说得上话的人在旁边。算了，写不下去了，找个时间看医生吧。受够了……</p>\n<hr>\n<p>后来没有去看医生，好像也还能够忍受。</p>\n<p>开学准备复试。。。</p>\n","categories":[],"tags":[]},{"title":"单片机实习日记","url":"https://hahally.github.io//articles/单片机实习日记/","content":"<blockquote>\n<p><em>I know nothing but my ignorance.</em></p>\n</blockquote>\n<h3 id=\"5月18日\"><a href=\"#5月18日\" class=\"headerlink\" title=\"5月18日\"></a>5月18日</h3><p>今天是单片机实习的第一天，因为疫情原因变成了线上开展。</p>\n<p>上午，老师把六个选题都一一讲解了一遍，并告诉我们一些在实习过程中可能会出现的一些问题和注意事项。老师讲完后，我们就开始组队和选题了。</p>\n<p>在经过一番并不激烈的讨论后，我们组选择了第二个项目：<strong>数字频率计设计</strong> </p>\n<p>一、基本要求：</p>\n<p>测量待测 <code>TTL</code> 电平信号的频率</p>\n<ol>\n<li>频率范围：10 Hz ~ 50kHz，全测量范围误差不大于 1%；</li>\n<li>用液晶屏 <code>LCD1602</code> 显示数值和单位，可显示频率及周期；</li>\n<li>数据刷新率每秒三次以上；</li>\n<li>在同一台单片机上自行设计测试用信号源；</li>\n<li>通过串口通信在远程单片机（仿真环境）显示测量结果。</li>\n</ol>\n<p>二、扩展要求</p>\n<ol>\n<li>占空比测量</li>\n<li>在保证数据刷新率和精度的基础上拓展测量频率范围</li>\n</ol>\n<p>下午，我们组四个人开始查文献讨论总体方案和分工。又是一番并不激烈的讨论后，我们确定好了分工。我选择了串口通信功能的实现。</p>\n<h3 id=\"5月19日\"><a href=\"#5月19日\" class=\"headerlink\" title=\"5月19日\"></a>5月19日</h3><p>上午确定分工合作的一些细节部分的设计，资源的分配，引脚的功能等等。然后，听老师讲一些知识点。</p>\n<p>下午，我开始查找串口通信的资料文献。最终确定采用异步通信的方式实现串口通信。接着开始着重异步通信方面的资料查询。经过一番并不困难的翻阅后，大致了解了异步通信的 4 种方式以及定时器的知识。</p>\n<p>52单片机有三个定时器/计数器，与串口通信有关的就是 T1定时器了。定时器/计数器的实质是加 1 计数器(高 8 位 和低 8 位 两个寄存器组成)。<code>TMOD</code> 是确定工作方式和功能的寄存器；<code>TCON</code> 是控制 T0、T1 的启动和停止及设置溢出标志的寄存器。</p>\n<p><strong><code>TMOD</code></strong>:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">位序号</th>\n<th style=\"text-align:center\">D7</th>\n<th style=\"text-align:center\">D6</th>\n<th style=\"text-align:center\">D5</th>\n<th style=\"text-align:center\">D4</th>\n<th style=\"text-align:center\">D3</th>\n<th style=\"text-align:center\">D2</th>\n<th style=\"text-align:center\">D1</th>\n<th style=\"text-align:center\">D0</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">位符号</td>\n<td style=\"text-align:center\">$GATE$</td>\n<td style=\"text-align:center\">$C/\\overline{\\text{T}}$</td>\n<td style=\"text-align:center\">$M1$</td>\n<td style=\"text-align:center\">$M0$</td>\n<td style=\"text-align:center\">$GATE$</td>\n<td style=\"text-align:center\">$C/\\overline{\\text{T}}$</td>\n<td style=\"text-align:center\">$M1$</td>\n<td style=\"text-align:center\">$M0$</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><code>D7~D4</code> 为 <code>T1</code> 定时器，<code>D3~D0</code> 为 <code>T0</code> 定时器。$C/\\overline{\\text{T}}$ 为 0 时为定时器模式，为 1 时为计数器模式，<code>M1</code> 和 <code>M0</code> 是工作方式选择位。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">M1</th>\n<th style=\"text-align:center\">M0</th>\n<th style=\"text-align:center\">工作方式</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">方式0，为13位定时器/计数器</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">方式1，为16位定时器/计数器</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">方式2，8位初值自动重装的8位定时器/计数器</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">方式3，仅适用于 T0，分成两个8位计数器，T1停止计数</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong><code>TCON</code></strong></p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">位序号</th>\n<th style=\"text-align:center\">D7</th>\n<th style=\"text-align:center\">D6</th>\n<th style=\"text-align:center\">D5</th>\n<th style=\"text-align:center\">D4</th>\n<th style=\"text-align:center\">D3</th>\n<th style=\"text-align:center\">D2</th>\n<th style=\"text-align:center\">D1</th>\n<th style=\"text-align:center\">D0</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">位符号</td>\n<td style=\"text-align:center\">$TF1$</td>\n<td style=\"text-align:center\">$TR1$</td>\n<td style=\"text-align:center\">$TF0$</td>\n<td style=\"text-align:center\">$TR0$</td>\n<td style=\"text-align:center\">$IE1$</td>\n<td style=\"text-align:center\">$IT1$</td>\n<td style=\"text-align:center\">$IE0$</td>\n<td style=\"text-align:center\">$IT0$</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"5月20日\"><a href=\"#5月20日\" class=\"headerlink\" title=\"5月20日\"></a>5月20日</h3><p>经过昨天一天的查阅学习，已经对串口通信有了比较全面的了解。所以，今天就是先把串口通信的一些必要环境准备好。下载串口助手和虚拟串口工具，然后测试串口助手。在一番并不花里胡哨的操作后，基本操作都会了。接着就是准备仿真下的串口测试了。在 <code>protues</code> 上，点来点去，又是一番行云流水且不花里胡哨的操作后，仿真环境也搭建好了。然后开始敲代码，在 <code>keil</code> 这个外表朴实无华但功能强大的软件上，敲下第一个简单串口通信的代码。然后结合串口助手观察效果。这样一个发送单个字符的例子就测试成功了。</p>\n<h3 id=\"5月21日\"><a href=\"#5月21日\" class=\"headerlink\" title=\"5月21日\"></a>5月21日</h3><p>今天是周四，上午老师照常开了小小的线上例会。交我们一些小小的调试技巧之类的。在昨天的基础上，今天继续修改代码，实现发送和接收功能。上午把整个代码框架写了出来，本以为会是顺顺利利的一天。但是下午的调试，着实不太顺利。先是，接收的数据并不是期望的数据，在一顿微操后，发现是接收数据的数组出现了溢出现象。调好后，又发现接收的数据有些错位了。又是一顿 <code>Debug</code> 后，自己定义一个简单通信协议，在数据头尾加上标志位，以确保接收方正确接收到期望数据。来来回回这样折腾后，仿真串口通信是没有问题了。</p>\n<h3 id=\"5月22日\"><a href=\"#5月22日\" class=\"headerlink\" title=\"5月22日\"></a>5月22日</h3><p>一觉醒来还是周五，是的，昨晚熬到了十二点。精力显然有些不充沛的我，还是打开了电脑，接着干了。队友那边的频率测量 和<code>LCD</code> 显示功能已经就绪了。今天可以整一块测试一下了。拿到他们的代码后，便开始调试。红红火火恍恍惚惚，就这样一顿朴实无华而实用的 <code>CV</code>大法过后，加以小小微操作为辅助，就调好了。</p>\n<h3 id=\"5月25日\"><a href=\"#5月25日\" class=\"headerlink\" title=\"5月25日\"></a>5月25日</h3><p>短暂的周末一晃就过去了。新的一周，继续干。现在只剩下信号源了。查资料，敲代码，仿真。陷入循环状态。进度十分缓慢。浑浑噩噩，一天就这样结束了。</p>\n<h3 id=\"5月26日\"><a href=\"#5月26日\" class=\"headerlink\" title=\"5月26日\"></a>5月26日</h3><p>距离实习结束，迫在眉睫。但是，实在有些精神疲惫了。打开和昨天一样的软件一样的文件，开始改代码。一番操作，以为成了。结果，一到高频信号，就不受调制了，无法准确控制信号频率的改变。</p>\n<h3 id=\"5月27日\"><a href=\"#5月27日\" class=\"headerlink\" title=\"5月27日\"></a>5月27日</h3><p>今天开始在开发板上进行真机测试，果然不出所料，仿真和真机不一样。一天的疯狂调试，勉强调好，差强人意的样子。</p>\n","categories":["Diary"],"tags":["单片机"]},{"title":"CSRent","url":"https://hahally.github.io//articles/CSRent/","content":"<blockquote>\n<p>交互式Python爬虫分析实例小项目</p>\n</blockquote>\n<p>先抛出项目地址吧： <a href=\"http://dblab.xmu.edu.cn/blog/2355/#more-2355\" target=\"_blank\" rel=\"noopener\">厦门大学数据库实验室</a></p>\n<h3 id=\"项目简述\"><a href=\"#项目简述\" class=\"headerlink\" title=\"项目简述\"></a>项目简述</h3><p>实现一个简单的交互式的租房信息分析展示 $web$ 平台。</p>\n<p>数据来源 ： <a href=\"http://www.xhj.com/zufang/\" target=\"_blank\" rel=\"noopener\">http://www.xhj.com/zufang/</a></p>\n<p><strong>技术栈</strong> ：</p>\n<ul>\n<li>$python$ 爬虫</li>\n<li>$pyspark$ 数据分析</li>\n<li>$flask$  $web$ 后端</li>\n<li>$pyecharts$  可视化</li>\n</ul>\n<p><strong>最终呈现效果</strong> ：</p>\n<p><img src=\"/articles/CSRent/image-20200517220605897.png\" alt=\"image-20200517220605897\"></p>\n<p><img src=\"/articles/CSRent/image-20200517220645343.png\" alt=\"image-20200517220645343\"></p>\n<h3 id=\"租房信息爬取\"><a href=\"#租房信息爬取\" class=\"headerlink\" title=\"租房信息爬取\"></a>租房信息爬取</h3><p>地址： <a href=\"http://www.xhj.com/zufang/\" target=\"_blank\" rel=\"noopener\">http://www.xhj.com/zufang/</a></p>\n<p><img src=\"/articles/CSRent/image-20200517222251986.png\" alt=\"image-20200517222251986\"></p>\n<p><strong>网页分析</strong></p>\n<h4 id=\"地址分析\"><a href=\"#地址分析\" class=\"headerlink\" title=\"地址分析\"></a>地址分析</h4><p>在 <em>区域找房</em>  一栏找到长沙的各个区域。这里选取了长沙的六个区：【天心区、芙蓉区、开福区、岳麓区、雨花区、望城】</p>\n<p>逐个点击 六个区可以观察到每个区域都对应有 $40$ 页 ，而且地址可以简单按下面这样方式拼接：</p>\n<figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http:<span class=\"regexp\">//</span>www.xhj.com<span class=\"regexp\">/zufang/</span> + 区域+<span class=\"regexp\">/pg + 页码/</span></span><br></pre></td></tr></table></figure>\n<p>所以代码中可以这样构造地址：</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">base_url</span> = <span class=\"string\">'http://www.xhj.com/zufang/'</span></span><br><span class=\"line\"><span class=\"attr\">param</span> = [<span class=\"string\">'tianxinqu'</span>,<span class=\"string\">'furongqu'</span>,<span class=\"string\">'kaifuqu'</span>,<span class=\"string\">'yueluqu'</span>,<span class=\"string\">'yuhuaqu'</span>,<span class=\"string\">'wangcheng'</span>]</span><br><span class=\"line\"><span class=\"comment\"># region in param</span></span><br><span class=\"line\"><span class=\"attr\">url</span> = base_url + region + <span class=\"string\">'/pg%d/'</span>%i <span class=\"comment\"># region为区域，i 为页码</span></span><br></pre></td></tr></table></figure>\n<p>这样很容易就可以通过两个循环来爬取我们需要的数据了。</p>\n<h4 id=\"源码分析\"><a href=\"#源码分析\" class=\"headerlink\" title=\"源码分析\"></a>源码分析</h4><p>通过开发者工具查看源码：</p>\n<p><img src=\"/articles/CSRent/image-20200517223429820.png\" alt=\"image-20200517223429820\"></p>\n<p>显然，这个网站的前端非常的给力，结构一目了然，而且没有动态加载。要是能改为 $ajax$ 的请求方式加载数据或许会更友好。</p>\n<p>不用仔细观察都可以看见，每个租房信息都被一个 <code>&lt;div​ class=&quot;lp_wrap&quot;&gt;...&lt;/div&gt;​</code> 包裹着。</p>\n<p>我们很容易就可以通过 $xpath$ 定位到每个租房信息，像这样：</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># html is the source code of webpage.</span></span><br><span class=\"line\"><span class=\"attr\">div</span> = html.xpath(<span class=\"string\">'//*/div[@class=\"lp_wrap\"]'</span>)</span><br></pre></td></tr></table></figure>\n<p>这样提取的 <code>div​</code> 对象为一个 $list$ 。最后将结果保存在 <code>rent_info.csv</code> 中。</p>\n<h4 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h4><p>经过这么一波并不花里胡哨的简单操作和分析，基本上可以写出对应的代码了。但是，这看似普普通通的网站，还是会封你的 $ip$ 的。所以，一般的加 <code>headers[&#39;User-Agent&#39;]​</code>  已经不行了。这里，我选择了添加代理来绕过它的反爬机制。【备注：很久之前爬过免费高匿代理存放在 <code>mongodb</code> 中】</p>\n<p>(此处省略个几百字)一顿花里胡哨的操作后，数据库中的代理 $ip$ 果然已经基本失效了。毕竟一年多了。</p>\n<p>后面发现，这个网站只会封你半分钟不到好像(应该是的，被禁后，刷新了好几下网页，然后刷回来了)。所以说，代码中是不是可以通过设置休眠时间来降低访问速度呢。三思过后，放弃的这个想法，这样的做法好像一点都不干脆利落。还是决定自己做个代理池算了。</p>\n<p>于是开启了免费代理的寻找之路，又是一顿的花里胡哨操作后(此处省略几百字)。很多 <strong>西刺代理</strong> 这样的免费代理网站已经迭代升级了，不在是曾经那个亚子了。它也开始封我 $ip$ 了。差点当场炸裂开来……因为它不是封你一两分钟酱紫玩玩。</p>\n<p>不过没关系，多爬几个这样的网站就可以有比较多得代理了。如果不想爬也不打紧，不妨逛一逛这里<a href=\"https://ip.ihuan.me/\" target=\"_blank\" rel=\"noopener\">小幻http代理</a>  </p>\n<p>支持批量提取，十分友好，可以帮我们省十几行代码了。</p>\n<p><strong>一点建议</strong></p>\n<p>记得用我们的目标网站测试一下这些免费代理是否失效。</p>\n<p><strong>下面是我选出来的比较好的</strong></p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">proxies = [</span><br><span class=\"line\">    &#123;<span class=\"symbol\">'https</span>':<span class=\"string\">\"https://221.6.201.18:9999\"</span>&#125;,</span><br><span class=\"line\">    &#123;<span class=\"symbol\">'http</span>': <span class=\"symbol\">'http</span>:<span class=\"comment\">//39.137.69.9:80'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'https</span>': <span class=\"symbol\">'https</span>:<span class=\"comment\">//221.122.91.64:80'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'http</span>': <span class=\"symbol\">'http</span>:<span class=\"comment\">//39.137.69.8:8080'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'http</span>': <span class=\"symbol\">'http</span>:<span class=\"comment\">//125.59.223.27:8380'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'http</span><span class=\"string\">':'</span>http:<span class=\"comment\">//118.212.104.22:9999'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'https</span><span class=\"string\">':'</span>https:<span class=\"comment\">//47.106.59.75:3128'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'http</span><span class=\"string\">':'</span>http:<span class=\"comment\">//221.180.170.104:8080'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'http</span>': <span class=\"symbol\">'http</span>:<span class=\"comment\">//113.59.99.138:8910'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'http</span><span class=\"string\">':'</span>http:<span class=\"comment\">//123.194.231.55:8197'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'https</span><span class=\"string\">':'</span>https:<span class=\"comment\">//218.60.8.99:3129'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'http</span>': <span class=\"symbol\">'http</span>:<span class=\"comment\">//218.58.194.162:8060'&#125;,</span></span><br><span class=\"line\">    &#123;<span class=\"symbol\">'https</span>': <span class=\"symbol\">'https</span>:<span class=\"comment\">//221.122.91.64:80'&#125;</span></span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<h3 id=\"pyspark-数据分析\"><a href=\"#pyspark-数据分析\" class=\"headerlink\" title=\"pyspark 数据分析\"></a>pyspark 数据分析</h3><p>这一步主要使用 <code>pyspark.sql.SparkSession</code> 来操作。从 <code>rent_info.csv</code> 中读取数据获得一个 <code>DataFrame</code> 对象，然后通过一系列动作(过滤筛选，聚合，统计)完成简单分析。</p>\n<h3 id=\"flask-后端\"><a href=\"#flask-后端\" class=\"headerlink\" title=\"flask 后端\"></a>flask 后端</h3><p>使用 <code>flask_socketio.SocketIO</code>  来注册一个 <code>flask app</code> 对象。调用 <code>run</code> 方法启动服务。</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">app = Flask(__name__)</span><br><span class=\"line\">app.config[<span class=\"string\">'SECRET_KEY'</span>] = <span class=\"string\">'xmudblab'</span></span><br><span class=\"line\">socketio = SocketIO(app)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    socketio.<span class=\"builtin-name\">run</span>(app, <span class=\"attribute\">debug</span>=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>剩下的就是一些简单的路由配置(通过装饰器来实现)：</p>\n<figure class=\"highlight ruby\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 客户端访问 http://127.0.0.1:5000/，可以看到index界面</span></span><br><span class=\"line\">@app.route(<span class=\"string\">\"/\"</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">handle_mes</span><span class=\"params\">()</span></span><span class=\"symbol\">:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> render_template(<span class=\"string\">\"index.html\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对客户端发来的start_spider事件作出相应</span></span><br><span class=\"line\">@socketio.on(<span class=\"string\">\"start_spider\"</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start_spider</span><span class=\"params\">(message)</span></span><span class=\"symbol\">:</span></span><br><span class=\"line\">    print(message)</span><br><span class=\"line\">    run_spider()</span><br><span class=\"line\">    socketio.emit(<span class=\"string\">'get_result'</span>, &#123;<span class=\"string\">'data'</span>: <span class=\"string\">\"请获取最后结果\"</span>&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对客户端发来的/Get_result事件作出相应</span></span><br><span class=\"line\">@app.route(<span class=\"string\">\"/Get_result\"</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Get_result</span><span class=\"params\">()</span></span><span class=\"symbol\">:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> render_template(<span class=\"string\">\"result.html\"</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"socketio-补充\"><a href=\"#socketio-补充\" class=\"headerlink\" title=\"socketio 补充\"></a>socketio 补充</h3><p>使用 <code>socketio</code> 可以轻松实现 <code>web</code> 后台和前端的信息交互，这种连接是基于 <code>websocket</code> 协议的全双工通信。</p>\n<p>前端 <code>socketio</code> 库</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">src</span>=<span class=\"string\">\"static/js/socket.io.js\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>未完待续…..</p>\n</blockquote>\n<h3 id=\"改进空间\"><a href=\"#改进空间\" class=\"headerlink\" title=\"改进空间\"></a>改进空间</h3><p>整个项目中，<code>spark</code> 的强大好像并没有发挥出来。毕竟 <code>spark</code> 在实时数据处理方面可是碾压 <code>mapreduce</code>的，好像一套组合拳，只使出了一点花拳绣腿。不妨大点想象一下，能不能实现一个实时房租信息交互系统，通过可视化工具在地图上直观的显示租房信息，每隔一小段时间更新数据，同时发送邮件提醒。甚至结合微信小程序在移动端也能查看。</p>\n<p>嗯，想一想，挺好的。但是，这里的数据来源的可信度还有待考察。或许应该去 <a href=\"https://cs.zu.ke.com/zufang/changshaxian/\" target=\"_blank\" rel=\"noopener\">贝壳找房</a> 看看(当事人非常后悔)。怎么开始就没想到去贝壳找。【不是打广告/手动滑稽】</p>\n<hr>\n","categories":["projdemo"],"tags":["爬虫"]},{"title":"Scrapy","url":"https://hahally.github.io//articles/Scrapy/","content":"<h3 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h3><p>在很久之前就已经学过了爬虫。那时还是懵懵懂懂的小白，学了一点基础，就买来一本书，然后就开干。代码倒是写了不少，但是没有什么拿的出手的。之后，便又匆匆忙忙的转战 web ，学起了 Django 。这一入坑，不知不觉差不多快一年了。最后发现自己知道的依旧凤毛麟角。没有基础的计算机网络知识，没有良好的代码编写规范……</p>\n<p>意识到问题后，开始试着阅读官方文档，去看协议，看源码。这些天看了 http 协议，计算机网络基础，python 文档，以及 Scrapy 文档。不得不说，看完后虽然记住的不多，但是大致是怎么一回事，多多少少还是了解了。比如，当初的爬虫程序，为什么要设置 <code>header</code> 、<code>cookie</code> 、<code>session</code> 什么的。还有 <code>request</code> 和 <code>response</code> 的含义。</p>\n<p>这些天看了一下 Scrapy 的 <a href=\"https://doc.scrapy.org/en/latest/intro/overview.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>，对这个框架有了一些了解。正如文档中所提到的，scrapy 框架很大程度上借鉴了 Django ，这也是为什么现在的我重新来看待它时，比之前要轻松太多了。</p>\n<h3 id=\"关于-Scrapy\"><a href=\"#关于-Scrapy\" class=\"headerlink\" title=\"关于 Scrapy\"></a>关于 Scrapy</h3><blockquote>\n<p>Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.</p>\n</blockquote>\n<p>学习一个框架，得明白，它是什么？怎么做？更深入为什么要这样做？</p>\n<h5 id=\"是什么？\"><a href=\"#是什么？\" class=\"headerlink\" title=\"是什么？\"></a>是什么？</h5><p>简而言之,就是一个支持分布式的，可扩展的，用于批量爬取网站并提取结构化数据的异步应用程序框架。值得一提的是，Scrapy 是用 <a href=\"https://twistedmatrix.com/trac/\" target=\"_blank\" rel=\"noopener\">Twisted</a> 编写的，<a href=\"https://twistedmatrix.com/trac/\" target=\"_blank\" rel=\"noopener\">Twisted</a> 是一种流行的 Python 事件驱动的网络框架。因此，它是使用非阻塞（又称为异步）代码并发实现的。</p>\n<p>Scrapy 有着丰富的命令行工具，交互式控制台，内置支持以多种格式(json、xml、csv)等。</p>\n<h5 id=\"怎么做？\"><a href=\"#怎么做？\" class=\"headerlink\" title=\"怎么做？\"></a>怎么做？</h5><p>要使用 Scrapy ，我们不得不先安装它。文档为我们提供的良好的 <a href=\"https://doc.scrapy.org/en/latest/intro/install.html\" target=\"_blank\" rel=\"noopener\">安装指南</a> 。</p>\n<p>我们只需要这样做：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install Scrapy</span><br></pre></td></tr></table></figure>\n<p>不过我们不得不知道下面文档中提到的：</p>\n<blockquote>\n<p>Scrapy is written in pure Python and depends on a few key Python packages (among others)</p>\n</blockquote>\n<p>Scrapy 需要一些依赖包：</p>\n<ul>\n<li><a href=\"http://lxml.de/\" target=\"_blank\" rel=\"noopener\">lxml</a>，高效的XML和HTML解析器</li>\n<li><a href=\"https://pypi.python.org/pypi/parsel\" target=\"_blank\" rel=\"noopener\">parsel</a>，是在lxml之上编写的HTML / XML数据提取库</li>\n<li><a href=\"https://pypi.python.org/pypi/w3lib\" target=\"_blank\" rel=\"noopener\">w3lib</a>，用于处理URL和网页编码的多功能帮助器</li>\n<li><a href=\"https://twistedmatrix.com/\" target=\"_blank\" rel=\"noopener\">twisted</a>，异步网络框架</li>\n<li><a href=\"https://cryptography.io/\" target=\"_blank\" rel=\"noopener\">cryptography</a> 和 <a href=\"https://pypi.python.org/pypi/pyOpenSSL\" target=\"_blank\" rel=\"noopener\">pyOpenSSL</a> ，以处理各种网络级安全需求</li>\n</ul>\n<p>其中还有一些版本要求：</p>\n<ul>\n<li>Twisted 14.0</li>\n<li>lxml 3.4</li>\n<li>pyOpenSSL 0.14</li>\n</ul>\n<p>如果你没有这些依赖包，那你不得不考虑先安装依赖。在此建议使用清华源下载，这样可以避免不必要的 Time out 。如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install [example_modul] -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple&#x2F;</span><br></pre></td></tr></table></figure>\n<p>安装完成后，就可以开始按接下来的<a href=\"https://doc.scrapy.org/en/latest/intro/tutorial.html\" target=\"_blank\" rel=\"noopener\">教程</a> 学习了。</p>\n<p>像这样创建一个项目：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\"> scrapy startproject tutorial</span></span><br></pre></td></tr></table></figure>\n<p>编写自己的爬虫类：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">QuotesSpider</span><span class=\"params\">(scrapy.Spider)</span>:</span></span><br><span class=\"line\">    name = <span class=\"string\">\"quotes\"</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start_requests</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        urls = [</span><br><span class=\"line\">            <span class=\"string\">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class=\"line\">            <span class=\"string\">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class=\"line\">        ]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> url <span class=\"keyword\">in</span> urls:</span><br><span class=\"line\">            <span class=\"keyword\">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parse</span><span class=\"params\">(self, response)</span>:</span></span><br><span class=\"line\">        page = response.url.split(<span class=\"string\">\"/\"</span>)[<span class=\"number\">-2</span>]</span><br><span class=\"line\">        filename = <span class=\"string\">'quotes-%s.html'</span> % page</span><br><span class=\"line\">        <span class=\"keyword\">with</span> open(filename, <span class=\"string\">'wb'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            f.write(response.body)</span><br><span class=\"line\">        self.log(<span class=\"string\">'Saved file %s'</span> % filename)</span><br></pre></td></tr></table></figure>\n<p>运行项目：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; scrapy crawl quotes</span><br></pre></td></tr></table></figure>\n<p>至此，一个基本可以运行的 Scrapy 项目就成型了。</p>\n<h5 id=\"框架概述\"><a href=\"#框架概述\" class=\"headerlink\" title=\"框架概述\"></a><a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html\" target=\"_blank\" rel=\"noopener\">框架概述</a></h5><p>在依葫芦画瓢的完成一个 Scrapy 项目的编写后，要想明白为什么要这样编写我们的爬虫程序，就不得不了解这个框架的一些细节。</p>\n<p>Scrapy的体系结构及组件如下图所示：</p>\n<p><img src=\"https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png\"></p>\n<p>对照着 Scrapy 的项目结构：</p>\n<figure class=\"highlight livecodeserver\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tutorial/</span><br><span class=\"line\">    scrapy.cfg            <span class=\"comment\"># deploy configuration file</span></span><br><span class=\"line\"></span><br><span class=\"line\">    tutorial/             <span class=\"comment\"># project's Python module, you'll import your code from here</span></span><br><span class=\"line\">        __init__.py</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">items</span>.py          <span class=\"comment\"># project items definition file</span></span><br><span class=\"line\"></span><br><span class=\"line\">        middlewares.py    <span class=\"comment\"># project middlewares file</span></span><br><span class=\"line\"></span><br><span class=\"line\">        pipelines.py      <span class=\"comment\"># project pipelines file</span></span><br><span class=\"line\"></span><br><span class=\"line\">        settings.py       <span class=\"comment\"># project settings file</span></span><br><span class=\"line\"></span><br><span class=\"line\">        spiders/          <span class=\"comment\"># a directory where you'll later put your spiders</span></span><br><span class=\"line\">            __init__.py</span><br><span class=\"line\">            quotes_spider.py  <span class=\"comment\"># a spider written by yourself</span></span><br></pre></td></tr></table></figure>\n<p>学过 Django 就会发现，这个框架简直就是套着它的设计模式来的。全局设置的 <code>settings.py</code> 、项目的管道 <code>pipelines.py</code> 、强大可扩展的中间件 <code>middlewares.py</code> 、以及类似模型的 <code>items.py</code> 。从图中我们不难发现，spiders可以对 requests 和 response 进行处理。而中间件 middlewares还可以对 items 进行处理。 管道 pipelines 对输出的 items 进行最后的清洗。所以，在我们明白要对数据做怎样处理时，只需要在对应的地方按要求编写我们的代码来达到我们的目的即可。</p>\n<p>一个例子：如果我们需要对最后清洗的数据保存到一个文件(如：json文件)中，那么你可能就要在管道 <code>pipelines.py</code> 中编写合适代码来实现。像这样子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JsonWriterPipeline</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">    @classmethod</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">from_crawler</span><span class=\"params\">(cls, crawler)</span>:</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> cls(crawler)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">open_spider</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">        self.file = open(<span class=\"string\">'items.jl'</span>, <span class=\"string\">'w'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">close_spider</span><span class=\"params\">(self, spider)</span>:</span></span><br><span class=\"line\">        self.file.close()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_item</span><span class=\"params\">(self, item, spider)</span>:</span></span><br><span class=\"line\">        line = json.dumps(dict(item)) + <span class=\"string\">\"\\n\"</span></span><br><span class=\"line\">        self.file.write(line)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> item</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>process_item (self, item, spider)</p>\n<blockquote>\n<p>每个项目管道组件均调用此方法，返回一个 item 对象，返回 Twisted Deferred 或引发 DropItem 异常。</p>\n<p>如果要使用自己的管道，那么就不得不实现此方法。</p>\n</blockquote>\n</li>\n</ul>\n<p>  除此之外，还可以实现下面几种方法：</p>\n<ul>\n<li><p>open_spider(self, spider)</p>\n<blockquote>\n<p>This method is called when the spider is opened.</p>\n</blockquote>\n</li>\n<li><p>close_spider(self, spider)</p>\n<blockquote>\n<p>This method is called when the spider is closed.</p>\n</blockquote>\n</li>\n<li><p>from_crawler(cls, crawler)</p>\n<blockquote>\n<p>If present, this classmethod is called to create a pipeline instance from a Crawler. It must return a new instance of the pipeline. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for pipeline to access them and hook its functionality into Scrapy.</p>\n</blockquote>\n<p>编写完自己的 Item Pipeline后，我们还需要在 <code>settings.py</code> 中激活才能使用。像这样：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ITEM_PIPELINES = &#123;</span><br><span class=\"line\">    <span class=\"string\">'myproject.pipelines.JsonWriterPipeline'</span>: <span class=\"number\">800</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>需要注意的是，管道组件以字典的形式配置，并分配一整数值(0 ~ 1000)，项目将按升序方式依次执行。</p>\n</li>\n</ul>\n<hr>\n<p>补一篇关于 Scrapy 的笔记算是对很久之前的一个总结吧！</p>\n<blockquote>\n<p>路漫漫其修远兮吾将上下而求索。</p>\n<p>I know nothing but my ignorance.</p>\n</blockquote>\n","categories":["Scrapy"],"tags":["爬虫"]},{"title":"Scrapy工作流程","url":"https://hahally.github.io//articles/Scrapy工作流程/","content":"<h3 id=\"Scrapy-框架中的数据流\"><a href=\"#Scrapy-框架中的数据流\" class=\"headerlink\" title=\"Scrapy 框架中的数据流\"></a>Scrapy 框架中的数据流</h3><p><img src=\"https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png\"></p>\n<p>尽管文档中这样提到：Scrapy中的数据流由执行引擎控制，如下所示</p>\n<ol>\n<li>The <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine\" target=\"_blank\" rel=\"noopener\">Engine</a> gets the initial Requests to crawl from the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-spiders\" target=\"_blank\" rel=\"noopener\">Spider</a>.</li>\n<li>The <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine\" target=\"_blank\" rel=\"noopener\">Engine</a> schedules the Requests in the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-scheduler\" target=\"_blank\" rel=\"noopener\">Scheduler</a> and asks for the next Requests to crawl.</li>\n<li>The <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-scheduler\" target=\"_blank\" rel=\"noopener\">Scheduler</a> returns the next Requests to the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine\" target=\"_blank\" rel=\"noopener\">Engine</a>.</li>\n<li>The <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine\" target=\"_blank\" rel=\"noopener\">Engine</a> sends the Requests to the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader\" target=\"_blank\" rel=\"noopener\">Downloader</a>, passing through the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware\" target=\"_blank\" rel=\"noopener\">Downloader Middlewares</a> (see <a href=\"https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" target=\"_blank\" rel=\"noopener\"><code>process_request()</code></a>).</li>\n<li>Once the page finishes downloading the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader\" target=\"_blank\" rel=\"noopener\">Downloader</a> generates a Response (with that page) and sends it to the Engine, passing through the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware\" target=\"_blank\" rel=\"noopener\">Downloader Middlewares</a> (see <a href=\"https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" target=\"_blank\" rel=\"noopener\"><code>process_response()</code></a>).</li>\n<li>The <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine\" target=\"_blank\" rel=\"noopener\">Engine</a> receives the Response from the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-downloader\" target=\"_blank\" rel=\"noopener\">Downloader</a> and sends it to the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-spiders\" target=\"_blank\" rel=\"noopener\">Spider</a> for processing, passing through the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-spider-middleware\" target=\"_blank\" rel=\"noopener\">Spider Middleware</a> (see <a href=\"https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\" target=\"_blank\" rel=\"noopener\"><code>process_spider_input()</code></a>).</li>\n<li>The <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-spiders\" target=\"_blank\" rel=\"noopener\">Spider</a> processes the Response and returns scraped items and new Requests (to follow) to the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine\" target=\"_blank\" rel=\"noopener\">Engine</a>, passing through the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-spider-middleware\" target=\"_blank\" rel=\"noopener\">Spider Middleware</a> (see <a href=\"https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" target=\"_blank\" rel=\"noopener\"><code>process_spider_output()</code></a>).</li>\n<li>The <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-engine\" target=\"_blank\" rel=\"noopener\">Engine</a> sends processed items to <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-pipelines\" target=\"_blank\" rel=\"noopener\">Item Pipelines</a>, then send processed Requests to the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-scheduler\" target=\"_blank\" rel=\"noopener\">Scheduler</a> and asks for possible next Requests to crawl.</li>\n<li>The process repeats (from step 1) until there are no more requests from the <a href=\"https://doc.scrapy.org/en/latest/topics/architecture.html#component-scheduler\" target=\"_blank\" rel=\"noopener\">Scheduler</a>.</li>\n</ol>\n<p>但是具体到程序中是如何体现的呢？在项目运行时，控制台中就有输出提示信息。如果要更直观的体现，不妨在每步对应的函数中打印自己设置的提示信息。例如：在自己的项目管道中可以这样做</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MyPipeline(object):</span><br><span class=\"line\">    # 在 open_spider 以及 parse 之后执行</span><br><span class=\"line\">    def process_item(self, item, spider):</span><br><span class=\"line\">        print(&quot;----- process_item -----&quot; )</span><br><span class=\"line\">        return item</span><br><span class=\"line\"></span><br><span class=\"line\">    # 在 from_crawler 之后执行</span><br><span class=\"line\">    def open_spider(self, spider):</span><br><span class=\"line\">        print(&quot;------------ open_spider --------------&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    # 在 process_item 之后执行</span><br><span class=\"line\">    def close_spider(self, spider):</span><br><span class=\"line\">        print(&quot;------------ close_spider --------------&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    # 最先执行</span><br><span class=\"line\">    @classmethod</span><br><span class=\"line\">    def from_crawler(cls, crawler):</span><br><span class=\"line\">        print(&quot;------------ from_crawler --------------&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">        return cls()</span><br></pre></td></tr></table></figure>\n<p>项目运行后，就可以看见他们的输出顺序了：</p>\n<figure class=\"highlight brainfuck\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--<span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span> <span class=\"comment\">from_crawler</span> --<span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span></span><br><span class=\"line\">--<span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span> <span class=\"comment\">open_spider</span>  --<span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span></span><br><span class=\"line\">--<span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span> <span class=\"comment\">process_item</span> --<span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span></span><br><span class=\"line\">--<span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span> <span class=\"comment\">close_spider</span> --<span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span><span class=\"literal\">-</span></span><br></pre></td></tr></table></figure>\n<p>了解框架的处理逻辑对我们编写高效代码是很有好处的。</p>\n","categories":["Scrapy"],"tags":["爬虫"]},{"title":"CopyText","url":"https://hahally.github.io//articles/CopyText/","content":"<blockquote>\n<pre><code>  auth : hahally\n\n  start : 2020.1.11\n</code></pre></blockquote>\n<h3 id=\"colab-长连接脚本\"><a href=\"#colab-长连接脚本\" class=\"headerlink\" title=\"colab 长连接脚本\"></a>colab 长连接脚本</h3><figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">ClickConnect</span>(<span class=\"params\"></span>)</span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">console</span>.log(<span class=\"string\">\"Clicked on connect button\"</span>); </span><br><span class=\"line\">    <span class=\"built_in\">document</span>.querySelector(<span class=\"string\">\"colab-connect-button\"</span>).click()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">setInterval(ClickConnect,<span class=\"number\">60000</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Django\"><a href=\"#Django\" class=\"headerlink\" title=\"Django\"></a>Django</h3><p>常用命令</p>\n<blockquote>\n<pre><code> django-admin startproject locallibrary   # 创建项目\n python manage.py startapp catalog        # 创建应用\n python manage.py runserver               # 启动服务\n python manage.py makemigrations          # 数据库迁移\n python manage.py migrate\n python manage.py createsuperuser         # 创建管理员账号\n</code></pre></blockquote>\n<pre><code>views.py\n    posts.content = markdown.markdown(\n    posts.content,\n    extensions = [\n        # 包含 缩写、表格等常用扩展\n        &#39;markdown.extensions.extra&#39;,\n        # 语法高亮扩展\n        &#39;markdown.extensions.codehilite&#39;,\n        ]\n    )\n</code></pre><h3 id=\"Scrapy-常用命令\"><a href=\"#Scrapy-常用命令\" class=\"headerlink\" title=\"Scrapy 常用命令\"></a>Scrapy 常用命令</h3><blockquote>\n<pre><code>           scrapy startproject proj     # 创建项目\n           scrapy crawl spider_name     # 运行爬虫\n</code></pre></blockquote>\n<h3 id=\"python-第三方库安装源\"><a href=\"#python-第三方库安装源\" class=\"headerlink\" title=\"python 第三方库安装源\"></a>python 第三方库安装源</h3><pre><code>清华大学镜像\nhttps://pypi.tuna.tsinghua.edu.cn/simple/\n阿里云\nhttp://mirrors.aliyun.com/pypi/simple/\n中科大镜像\nhttps://pypi.mirrors.ustc.edu.cn/simple/\n豆瓣镜像\nhttp://pypi.douban.com/simple/\n中科大镜像2\nhttp://pypi.mirrors.ustc.edu.cn/simple/\n</code></pre><hr>\n<h3 id=\"笔记\"><a href=\"#笔记\" class=\"headerlink\" title=\"笔记\"></a>笔记</h3><pre><code>      Auth       : hahally\ncreateTime       : 2019.10.26\n  abstract       : 大数据辅修学习笔记\n</code></pre><h4 id=\"jdk环境变量配置\"><a href=\"#jdk环境变量配置\" class=\"headerlink\" title=\"jdk环境变量配置\"></a><code>jdk</code>环境变量配置</h4><pre><code>    在、/etc/profile或~/.bashrc中的文件底部\n    JAVA_HOME=/usr/java/jdk1.8.0_162\n    JRE_HOME=$JAVA_HOME/jre\n    CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib\n    PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin\n    export JAVA_HOME JRE_HOME CLASS_PATH PATH\n    [root@master~]# source /etc/profile   使配置生效\n</code></pre><h4 id=\"windows-dos-命令\"><a href=\"#windows-dos-命令\" class=\"headerlink\" title=\"windows dos 命令\"></a><code>windows dos</code> 命令</h4><pre><code>    C:\\Users\\ACER&gt;netstat -aon|findstr &quot;8081&quot;      查看端口号\n    C:\\Users\\ACER&gt;taskkill /f /t /im 10144         杀掉进程\n</code></pre><h4 id=\"Linux命令\"><a href=\"#Linux命令\" class=\"headerlink\" title=\"Linux命令\"></a><code>Linux</code>命令</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master~]# tar -zxvf [*].tar.gz -C [路径]   解压</span><br><span class=\"line\">[root@master~]# yum -y remove firewalld          卸载防火墙</span><br><span class=\"line\">[root@master~]# systemctl stop/status/start firewalld 停止/查看状态/启动/防火墙服务</span><br><span class=\"line\">[root@master~]# netstat -tunlp|grep 端口号        查看端口占用情况</span><br><span class=\"line\">[root@master~]# sudo passwd root        设置root密码</span><br><span class=\"line\">[root@master~]# sudo ln -s /usr/local/jdk1.8.0_162/bin/ bin   创建软链接</span><br><span class=\"line\">[root@master~]# cp [-r] file/filedir filepath        复制文件或目录</span><br><span class=\"line\"></span><br><span class=\"line\">ubuntu ens33丢失重连</span><br><span class=\"line\">[root@master~]# sudo service network-manager stop</span><br><span class=\"line\">[root@master~]# sudo rm /var/lib/NetworkManager/NetworkManager.state</span><br><span class=\"line\">[root@master~]# sudo service network-manager start</span><br><span class=\"line\">[root@master~]# sudo gedit /etc/NetworkManager/NetworkManager.conf    #（把false改成true）</span><br><span class=\"line\">[root@master~]# sudo service network-manager restart</span><br><span class=\"line\"></span><br><span class=\"line\">centos ens33丢失重连</span><br><span class=\"line\">[root@master~]# systemctl stop NetworkManager</span><br><span class=\"line\">[root@master~]# systemctl disable NetworkManager</span><br><span class=\"line\">[root@master~]# sudo ifup ens33         重新连接ens33</span><br><span class=\"line\">[root@master~]# systemctl restart network</span><br><span class=\"line\">[root@master~]# systemctl start NetworkManager</span><br><span class=\"line\"></span><br><span class=\"line\">[root@master~]# sudo ps -e |grep ssh    查看ssh服务是否启动</span><br></pre></td></tr></table></figure>\n<h4 id=\"git-命令\"><a href=\"#git-命令\" class=\"headerlink\" title=\"git 命令\"></a><code>git</code> 命令</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git init   初始化</span><br><span class=\"line\">git add filename  将上传文件加到缓冲区</span><br><span class=\"line\">git commit [-m] [注释]</span><br><span class=\"line\">git remote add origin https://github.com/[用户名名]/[仓库名].git</span><br><span class=\"line\">git push -u origin master -f    上传到远程仓库分支</span><br><span class=\"line\">git clone https://github.com/[用户名名]/[仓库名].git        拉取代码</span><br></pre></td></tr></table></figure>\n<h4 id=\"docker命令\"><a href=\"#docker命令\" class=\"headerlink\" title=\"docker命令\"></a><code>docker</code>命令</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master~]# sudo docker run -it -v /home/hahally/myimage:/data --name slave2 -h slave2 new_image:newhadoop /bin/bash      运行容器指定共享目录</span><br><span class=\"line\">[root@master~]# sudo docker start slave2      启动容器</span><br><span class=\"line\">[root@master~]# sudo docker exec -i -t s2 /bin/bash\t\t进入容器</span><br><span class=\"line\">[root@master~]# docker commit master new_image:tag    提交容器</span><br><span class=\"line\">[root@master~]# sudo docker rm contianername          删除容器</span><br><span class=\"line\">[root@master~]# sudo docker rmi imagesname            删除镜像</span><br><span class=\"line\">[root@master~]# sudo docker rename name1 name2        重新命名容器</span><br></pre></td></tr></table></figure>\n<h4 id=\"hadoop命令\"><a href=\"#hadoop命令\" class=\"headerlink\" title=\"hadoop命令\"></a><code>hadoop</code>命令</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master~]# hadoop dfsadmin -report      命令查看磁盘使用情况</span><br><span class=\"line\">[root@master~]# hadoop jar hadoop-mapreduce-examples-2.7.5.jar wordcount /wordcount/input  /wordcount/output 运行jar包</span><br><span class=\"line\">[root@master~]# hadoop dfsadmin -safemode leave    退出安全模式</span><br><span class=\"line\">[root@master~]# hadoop jar  x.jar  MainClassName[主类名称] [inputPath] [outputPath]</span><br></pre></td></tr></table></figure>\n<h4 id=\"运行hadoop自带MapReduce程序\"><a href=\"#运行hadoop自带MapReduce程序\" class=\"headerlink\" title=\"运行hadoop自带MapReduce程序\"></a>运行<code>hadoop</code>自带<code>MapReduce</code>程序</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master hadoop-2.7.5]# hadoop fs -mkdir -p /wordcount/input              [创建一个目录]</span><br><span class=\"line\">[root@master hadoop-2.7.5]# hadoop fs -put a.txt  b.txt  /wordcount/input     [将文件上传到input文件夹中]</span><br><span class=\"line\">[root@master hadoop-2.7.5]# cd share/hadoop/mapreduce/                        [进入程序所在目录]</span><br><span class=\"line\">[root@master mapreduce]# hadoop jar hadoop-mapreduce-examples-2.7.5.jar wordcount /wordcount/input  /wordcount/output   [运行jar包]</span><br><span class=\"line\">[root@master mapreduce]# hadoop fs -cat /wordcount/output/part-r-00000     [查看输出结果]</span><br></pre></td></tr></table></figure>\n<h4 id=\"Spark\"><a href=\"#Spark\" class=\"headerlink\" title=\"Spark\"></a>Spark</h4><p>环境变量<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim ~/.bashrc</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 现在我们的环境变量配置看起来像这样</span></span><br><span class=\"line\">export HADOOP_HOME=/usr/local/hadoop</span><br><span class=\"line\">export SPARK_HOME=/usr/local/spark</span><br><span class=\"line\">export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH</span><br><span class=\"line\">export PYSPARK_PYTHON=python3</span><br><span class=\"line\">export JAVA_HOME=/usr/local/java/jdk1.8.0_171</span><br><span class=\"line\">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class=\"line\">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/jre/lib/rt.jar:$&#123;JAVA_HOME&#125;/lib/dt.jar:$&#123;JAVA_HOME&#125;/lib/tools.jar</span><br><span class=\"line\">export PATH=$PATH:$&#123;JAVA_HOME&#125;/bin</span><br><span class=\"line\">export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:/usr/local/spark/bin:/usr/local/spark/sbin:$PATH</span><br><span class=\"line\">export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PYTHON</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 使配置生效</span></span><br><span class=\"line\">source ~/.bashrc</span><br></pre></td></tr></table></figure><br><code>spark-env.sh</code><br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd /usr/local/spark</span><br><span class=\"line\">cp ./conf/spark-env.sh.template ./conf/spark-env.sh</span><br><span class=\"line\">vim ./conf/spark-env.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 在最后一行添加如下配置信息：</span></span><br><span class=\"line\"></span><br><span class=\"line\">export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)</span><br><span class=\"line\">export HADOOP_CONF_DIR=/usr/local/hadoop/ect/hadoop</span><br><span class=\"line\">export YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure><br>运行<br><figure class=\"highlight crystal\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/usr/local/spark/bin/run-example SparkPi <span class=\"comment\"># 运行例子</span></span><br><span class=\"line\">/usr/local/spark、bin/run-example SparkPi <span class=\"number\">2</span>&gt;&amp;<span class=\"number\">1</span> | grep <span class=\"string\">\"Pi is roughly\"</span></span><br><span class=\"line\">/usr/local/spark/bin/spark-submit ../examples/src/main/python/pi.py <span class=\"number\">2</span>&gt;&amp;<span class=\"number\">1</span> | grep <span class=\"string\">'Pi'</span></span><br><span class=\"line\">/usr/local/spark/bin/spark-submit --master yarn --deploy-mode cluster /usr/local/spark/examples/src/main/python/wordcount.py <span class=\"symbol\">hdfs:</span>/<span class=\"regexp\">/master:9000/words</span>.txt</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"注意点\"><a href=\"#注意点\" class=\"headerlink\" title=\"注意点\"></a>注意点</h4><p>运行<code>jar</code>包时，先删掉 <code>/output</code>文件夹，否则无法发查看输出结果</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"builtin-name\">export</span> <span class=\"attribute\">JAVA_HOME</span>=/usr/local/jdk1.8.0_162</span><br><span class=\"line\"><span class=\"builtin-name\">export</span> <span class=\"attribute\">JRE_HOME</span>=<span class=\"variable\">$&#123;JAVA_HOME&#125;</span>/jre</span><br><span class=\"line\"><span class=\"builtin-name\">export</span> <span class=\"attribute\">CLASSPATH</span>=.:$&#123;JAVA_HOME&#125;/lib:<span class=\"variable\">$&#123;JRE_HOME&#125;</span>/lib</span><br><span class=\"line\"><span class=\"builtin-name\">export</span> <span class=\"attribute\">HADOOP_HOME</span>=/usr/local/hadoop-2.7.5</span><br><span class=\"line\"><span class=\"builtin-name\">export</span> <span class=\"attribute\">PATH</span>=<span class=\"variable\">$PATH</span>:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:/usr/local/hbase-1.3.6/bin</span><br><span class=\"line\"><span class=\"builtin-name\">export</span> <span class=\"attribute\">HBASE_HOME</span>=/usr/local/hbase-1.3.6</span><br><span class=\"line\"><span class=\"builtin-name\">export</span> <span class=\"attribute\">HBASE_CLASSPATH</span>=/usr/local/hbase-1.3.6/lib/hbase-common-1.3.6.jar:/usr/local/hbase-1.3.6/lib/</span><br><span class=\"line\">hbase-server-1.3.6.jar</span><br></pre></td></tr></table></figure>","categories":["copyText"],"tags":[]},{"title":"Django中的分页","url":"https://hahally.github.io//articles/Django中的分页/","content":"<h3 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h3><p>随着自己写的博客日益增加，博客列表页的展示也逐渐变得有些力不从心。要浏览所有的博客就不得不疯狂的滑鼠标。冗长的页面带来的体验十分的差劲。这个时候不得不将他们做一下简单分页处理。分页的方式有很多，而便捷的 Django 为我们准备了十分友好的类 <code>Paginator</code> 来帮助我们进行分页。</p>\n<hr>\n<h3 id=\"Paginator-对象\"><a href=\"#Paginator-对象\" class=\"headerlink\" title=\"Paginator 对象\"></a>Paginator 对象</h3><p>先看看源码中的初始化或者说构造方法:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Paginator</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, object_list, per_page, orphans=<span class=\"number\">0</span>,allow_empty_first_page=True)</span>:</span></span><br><span class=\"line\">\t\tself.object_list = object_list</span><br><span class=\"line\">\t\tself._check_object_list_is_ordered()</span><br><span class=\"line\">\t\tself.per_page = int(per_page)</span><br><span class=\"line\">\t\tself.orphans = int(orphans)</span><br><span class=\"line\">\t\tself.allow_empty_first_page = allow_empty_first_page</span><br></pre></td></tr></table></figure>\n<p>显然，要创建一个 Paginator 对象, 就不得不提供 object_list 和 per_page 对象。</p>\n<p><code>object_list</code></p>\n<blockquote>\n<p>A list, tuple, QuerySet, or other sliceable object with a count() or <strong>len</strong>() method. For consistent pagination, QuerySets should be ordered, e.g. with an order_by() clause or with a default ordering on the model.</p>\n</blockquote>\n<p>从这段文档说明中，可以大致了解 object_list 是个这样的对象：列表、元组、QuerySet…同时，文档建议如果是 QuerySet 对象的话，应当对其进行排序，如使用 order_by() 方法，或者采用模型中默认的排序方法。</p>\n<p><code>per_page</code></p>\n<blockquote>\n<p>The maximum number of items to include on a page, not including orphans (see the orphans optional argument below).</p>\n</blockquote>\n<p>显然，per_page 指的是每页要展示的选项最大个数。例如：每页显示 5 篇文章，那么就是 <code>per_page=5</code>。同时也强调，不包括 orphans。</p>\n<p><code>orphans</code></p>\n<blockquote>\n<p>Use this when you don’t want to have a last page with very few items. If the last page would normally have a number of items less than or equal to orphans, then those items will be added to the previous page (which becomes the last page) instead of leaving the items on a page by themselves. For example, with 23 items, per_page=10, and orphans=3, there will be two pages; the first page with 10 items and the second (and last) page with 13 items. orphans defaults to zero, which means pages are never combined and the last page may have one item.</p>\n</blockquote>\n<p>orphans 顾名思义就是孤儿的意思。通俗来讲就是，在分页时，发现最后一页可能就只有一两个选项去了，如果觉得最后一页只是很少一部分，不想单独占一页，那么就可以将其添加在前一页中。而 orphans 值恰恰就是这样一个阈值，当小于它时，就可以将剩下那部分加到前一页中。假设我们有 23 个选项，每页展示是个选项，那么最多可以分成 3 页，第三页就只有 3 个选项。如果我们设置 orphans 大于等于 3，那么第一页是 10，第二页是 13 了。</p>\n<hr>\n<h3 id=\"如何使用\"><a href=\"#如何使用\" class=\"headerlink\" title=\"如何使用\"></a>如何使用</h3><p>文档中给出了一个十分详细的例子供我们参考：</p>\n<p>在我们的视图 views.py 中：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> django.core.paginator <span class=\"keyword\">import</span> Paginator</span><br><span class=\"line\"><span class=\"keyword\">from</span> django.shortcuts <span class=\"keyword\">import</span> render</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">listing</span><span class=\"params\">(request)</span>:</span></span><br><span class=\"line\">    contact_list = Contacts.objects.all()</span><br><span class=\"line\">    paginator = Paginator(contact_list, <span class=\"number\">25</span>) <span class=\"comment\"># Show 25 contacts per page</span></span><br><span class=\"line\">    page = request.GET.get(<span class=\"string\">'page'</span>)</span><br><span class=\"line\">    contacts = paginator.get_page(page)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> render(request, <span class=\"string\">'list.html'</span>, &#123;<span class=\"string\">'contacts'</span>: contacts&#125;)</span><br></pre></td></tr></table></figure>\n<p>在我们的模板 list.html 中：</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">for</span></span> contact <span class=\"keyword\">in</span> contacts %&#125;</span></span><br><span class=\"line\"><span class=\"xml\">    </span><span class=\"comment\">&#123;# Each \"contact\" is a Contact model object. #&#125;</span></span><br><span class=\"line\"><span class=\"xml\">    </span><span class=\"template-variable\">&#123;&#123; contact.full_name|<span class=\"name\">upper</span> &#125;&#125;</span><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">    ...</span></span><br><span class=\"line\"><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">endfor</span></span> %&#125;</span></span><br><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">\"pagination\"</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">    <span class=\"tag\">&lt;<span class=\"name\">span</span> <span class=\"attr\">class</span>=<span class=\"string\">\"step-links\"</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">        </span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">if</span></span> contacts.has_previous %&#125;</span></span><br><span class=\"line\"><span class=\"xml\">            <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"?page=1\"</span>&gt;</span><span class=\"symbol\">&amp;laquo;</span> first<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">            <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"?page=</span></span></span><span class=\"template-variable\">&#123;&#123; contacts.previous_page_number &#125;&#125;</span><span class=\"xml\"><span class=\"tag\"><span class=\"string\">\"</span>&gt;</span>previous<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">        </span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">endif</span></span> %&#125;</span></span><br><span class=\"line\"><span class=\"xml\">        <span class=\"tag\">&lt;<span class=\"name\">span</span> <span class=\"attr\">class</span>=<span class=\"string\">\"current\"</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">            Page </span><span class=\"template-variable\">&#123;&#123; contacts.number &#125;&#125;</span><span class=\"xml\"> of </span><span class=\"template-variable\">&#123;&#123; contacts.paginator.num_pages &#125;&#125;</span><span class=\"xml\">.</span></span><br><span class=\"line\"><span class=\"xml\">        <span class=\"tag\">&lt;/<span class=\"name\">span</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">        </span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">if</span></span> contacts.has_next %&#125;</span></span><br><span class=\"line\"><span class=\"xml\">            <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"?page=</span></span></span><span class=\"template-variable\">&#123;&#123; contacts.next_page_number &#125;&#125;</span><span class=\"xml\"><span class=\"tag\"><span class=\"string\">\"</span>&gt;</span>next<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">            <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"?page=</span></span></span><span class=\"template-variable\">&#123;&#123; contacts.paginator.num_pages &#125;&#125;</span><span class=\"xml\"><span class=\"tag\"><span class=\"string\">\"</span>&gt;</span>last <span class=\"symbol\">&amp;raquo;</span><span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\">        </span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">endif</span></span> %&#125;</span></span><br><span class=\"line\"><span class=\"xml\">    <span class=\"tag\">&lt;/<span class=\"name\">span</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p>一点点注释：</p>\n<blockquote>\n<pre><code>has_previous() 判断是否有上一页\nhas_next() 判断是否有上一页\nprevious_page_number() 返回上一个页码\ncontacts.number 一个基于 1 的页码\npaginator.num_pages() 页码的基于 1 的范围迭代器\n</code></pre></blockquote>\n<p>获取更多可以参考 <a href=\"https://docs.djangoproject.com/zh-hans/2.1/topics/pagination/#example\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<hr>\n<h3 id=\"欢迎参观\"><a href=\"#欢迎参观\" class=\"headerlink\" title=\"欢迎参观\"></a>欢迎参观</h3><p>学Django时，顺便写了简单的<a href=\"http://114.55.102.217/blog/\" target=\"_blank\" rel=\"noopener\">个人博客</a>。前端用的<a href=\"https://www.bootcss.com/\" target=\"_blank\" rel=\"noopener\">bootstrap</a>框架。笔记都会同步的。</p>\n","categories":["Django"],"tags":["后端"]},{"title":"about","url":"https://hahally.github.io/about/index.html","content":"","categories":[],"tags":[]},{"title":"","url":"https://hahally.github.io/css/personal-style.css","content":"@font-face {\n  font-family: \"Meiryo\";\n  src: url(\"/fonts/Meiryo.eot\");\n  /* IE9 */\n  src: url(\"/fonts/Meiryo.eot?#iefix\") format(\"embedded-opentype\"), /* IE6-IE8 */\n  url(\"/fonts/Meiryo.woff\") format(\"woff\"), /* chrome, firefox */\n  url(\"/fonts/Meiryo.ttf\") format(\"truetype\"), /* chrome, firefox, opera, Safari, Android, iOS 4.2+ */\n  url(\"/fonts/Meiryo.svg#Meiryo\") format(\"svg\");\n  /* iOS 4.1- */\n  font-style: normal;\n  font-weight: normal;\n}\nhtml.page-home {\n  /*background-image: url('/images/bg.jpg')*/\n  /*background: linear-gradient( #1abc9c, transparent), linear-gradient( 90deg, skyblue, transparent), linear-gradient( -90deg, coral, transparent);*/\n  /*background-blend-mode: screen;*/\n  /*background: linear-gradient(to left, #5f2c82, #49a09d);*/\n}","categories":[],"tags":[]},{"title":"category","url":"https://hahally.github.io/category/index.html","content":"","categories":[],"tags":[]},{"title":"search","url":"https://hahally.github.io/search/index.html","content":"","categories":[],"tags":[]},{"title":"tag","url":"https://hahally.github.io/tag/index.html","content":"","categories":[],"tags":[]}]
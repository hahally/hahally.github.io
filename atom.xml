<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hahally&#39;s BLOG</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://hahally.github.io/"/>
  <updated>2021-07-30T06:06:36.862Z</updated>
  <id>https://hahally.github.io/</id>
  
  <author>
    <name>hahally</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>蛋白质结构预测</title>
    <link href="https://hahally.github.io/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B/"/>
    <id>https://hahally.github.io/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B/</id>
    <published>2021-07-30T05:47:35.000Z</published>
    <updated>2021-07-30T06:06:36.862Z</updated>
    
    <content type="html"><![CDATA[<p>赛题：<a href="https://challenge.xfyun.cn/topic/info?type=protein" target="_blank" rel="noopener">蛋白质结构预测挑战赛</a></p><p>数据集一共包含245种折叠类型，11843条蛋白质序列样本，其中训练集中有9472个样本，测试集中有2371个样本。</p><p>继上次<a href="[蛋白质结构预测之lgb的baseline | Hahally&#39;s BLOG](https://hahally.github.io/articles/蛋白质结构预测之lgb的baseline/">lgb的base模型</a>) 后，尝试过word2vec + 神经网络的方法，最后效果甚微。今天尝试了一下双向GRU模型，相比之前，有几个百分点的提高。</p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_fa</span><span class="params">(file, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> mode <span class="keyword">in</span> &#123;<span class="string">'train'</span>,<span class="string">'test'</span>&#125;</span><br><span class="line">    labels = []</span><br><span class="line">    seqs_info = []</span><br><span class="line">    cates_id = []</span><br><span class="line">    seq = <span class="string">''</span></span><br><span class="line">    <span class="keyword">with</span> open(file,mode=<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        line = f.readline().strip()</span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            <span class="keyword">if</span> line[<span class="number">0</span>]==<span class="string">'&gt;'</span>:</span><br><span class="line">                info = line[<span class="number">1</span>:].split(<span class="string">' '</span>)</span><br><span class="line">                cates_id.append(info[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">                    label = <span class="string">''</span>.join(info[<span class="number">1</span>].split(<span class="string">'.'</span>)[:<span class="number">2</span>]</span><br><span class="line">                    label = label[<span class="number">0</span>]+<span class="string">'.'</span>+label[<span class="number">1</span>:]</span><br><span class="line">                    labels.append(label)</span><br><span class="line">                <span class="keyword">if</span> seq:</span><br><span class="line">                    seqs_info.append(seq)</span><br><span class="line">                    seq = <span class="string">''</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                seq += line</span><br><span class="line">            line = f.readline().strip()</span><br><span class="line">        seqs_info.append(seq)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cates_id,seqs_info,labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    train_file = <span class="string">'/kaggle/input/textfiles/astral_train.fa'</span></span><br><span class="line">    test_file = <span class="string">'/kaggle/input/textfiles/astral_test.fa'</span></span><br><span class="line"></span><br><span class="line">    train_sample_id, train_seqs_info, train_labels = read_fa(train_file, mode=<span class="string">'train'</span>)</span><br><span class="line">    test_sample_id, test_seqs_info, _ = read_fa(test_file, mode=<span class="string">'test'</span>)</span><br><span class="line">    </span><br><span class="line">    train_data = &#123;</span><br><span class="line">    <span class="string">'sample_id'</span>: train_sample_id,</span><br><span class="line">    <span class="string">'seq_info'</span>: train_seqs_info,</span><br><span class="line">    <span class="string">'label'</span>: train_labels</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    test_data = &#123;</span><br><span class="line">        <span class="string">'sample_id'</span>: test_sample_id,</span><br><span class="line">        <span class="string">'seq_info'</span>: test_seqs_info,</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    train = pd.DataFrame(data=train_data)</span><br><span class="line">    train = shuffle(train,random_state=<span class="number">2021</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    test = pd.DataFrame(data=test_data)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train,test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 滑窗分词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_windows</span><span class="params">(sentence,w = <span class="number">3</span>)</span>:</span></span><br><span class="line">    new_sentence = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sentence)-w+<span class="number">1</span>):</span><br><span class="line">        new_sentence.append(sentence[i:i+w])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> new_sentence</span><br><span class="line">     </span><br><span class="line">data = pd.concat(load_data(),ignore_index=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># label to idx</span></span><br><span class="line">label2idx = &#123; l:idx <span class="keyword">for</span> idx, l <span class="keyword">in</span> enumerate(data[~data[<span class="string">'label'</span>].isna()][<span class="string">'label'</span>].unique().tolist())&#125;</span><br><span class="line">idx2label = &#123; idx:l <span class="keyword">for</span> l,idx <span class="keyword">in</span> label2idx.items()&#125;</span><br><span class="line"></span><br><span class="line">data[<span class="string">'label'</span>] = data[<span class="string">'label'</span>].map(label2idx)</span><br><span class="line">data[<span class="string">'new_seq_info'</span>] = data[<span class="string">'seq_info'</span>].apply(<span class="keyword">lambda</span> x:split_windows(x,w = <span class="number">1</span>))</span><br><span class="line">train,test = data[~data[<span class="string">'label'</span>].isna()].reset_index(drop=<span class="literal">True</span>),data[data[<span class="string">'label'</span>].isna()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">max_features= <span class="number">1000</span></span><br><span class="line">max_len= <span class="number">256</span></span><br><span class="line">embed_size=<span class="number">128</span></span><br><span class="line">batch_size = <span class="number">24</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"></span><br><span class="line">tokens = Tokenizer(num_words = max_features)</span><br><span class="line">tokens.fit_on_texts(list(data[<span class="string">'new_seq_info'</span>]))</span><br><span class="line"></span><br><span class="line">x_data = tokens.texts_to_sequences(data[<span class="string">'new_seq_info'</span>])</span><br><span class="line">x_data = sequence.pad_sequences(x_data, maxlen=max_len)</span><br><span class="line">x_train = x_data[:<span class="number">9472</span>]</span><br><span class="line">y_train = data[<span class="string">'label'</span>][:<span class="number">9472</span>]</span><br><span class="line">x_test = x_data[<span class="number">9472</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D<span class="comment"># Keras Callback Functions:</span></span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Callback</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping,ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> initializers, regularizers, constraints, optimizers, layers, callbacks</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line">sequence_input = Input(shape=(max_len, ))</span><br><span class="line">x = Embedding(max_features, embed_size, trainable=<span class="literal">True</span>)(sequence_input)</span><br><span class="line">x = SpatialDropout1D(<span class="number">0.2</span>)(x)</span><br><span class="line">x = Bidirectional(GRU(<span class="number">128</span>, return_sequences=<span class="literal">True</span>,dropout=<span class="number">0.1</span>,recurrent_dropout=<span class="number">0.1</span>))(x)</span><br><span class="line">x = Conv1D(<span class="number">64</span>, kernel_size = <span class="number">3</span>, padding = <span class="string">"valid"</span>, kernel_initializer = <span class="string">"glorot_uniform"</span>)(x)</span><br><span class="line">avg_pool = GlobalAveragePooling1D()(x)</span><br><span class="line">max_pool = GlobalMaxPooling1D()(x)</span><br><span class="line">x = concatenate([avg_pool, max_pool]) </span><br><span class="line">preds = Dense(<span class="number">245</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(sequence_input, preds)</span><br><span class="line">model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              optimizer=keras.optimizers.Adam(<span class="number">1e-3</span>),</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train, y_train, </span><br><span class="line">          batch_size=batch_size, </span><br><span class="line">          validation_split=<span class="number">0.2</span>,</span><br><span class="line">          epochs=epochs)</span><br></pre></td></tr></table></figure><p>提交结果：目前【39/130(提交团队数)】</p><p><img src="/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B/image-20210730140527003.png" alt="image-20210730140527003"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;赛题：&lt;a href=&quot;https://challenge.xfyun.cn/topic/info?type=protein&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;蛋白质结构预测挑战赛&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;数据集一共包含245种折叠类型，1184
      
    
    </summary>
    
    
    
      <category term="BDC" scheme="https://hahally.github.io/tags/BDC/"/>
    
  </entry>
  
  <entry>
    <title>Attention-Is-All-You-Need</title>
    <link href="https://hahally.github.io/articles/Attention-Is-All-You-Need/"/>
    <id>https://hahally.github.io/articles/Attention-Is-All-You-Need/</id>
    <published>2021-07-15T10:30:21.000Z</published>
    <updated>2021-07-15T10:37:34.721Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">Attention Is All You Need</a></p><p>“懂的都懂”</p></blockquote><p><strong>Bib TeX</strong></p><blockquote><p>@inproceedings{NIPS2017_3f5ee243,<br>author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},<br>booktitle = {Advances in Neural Information Processing Systems},<br>editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},<br>pages = {},<br>publisher = {Curran Associates, Inc.},<br>title = {Attention is All you Need},<br>url = {<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}" target="_blank" rel="noopener">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}</a>,<br>volume = {30},<br>year = {2017}<br>}</p></blockquote><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><em>Attention is All you Need</em> 这篇论文是Google的又一神作。传统的 encoder-decoder序列转化模型架构主要以卷积和循环神经网络为基础。在引入 attention 后，其模型效果达到最佳。除了 <strong>cnn</strong> 或 <strong>rnn</strong> 进行编码解码，Google团队提出了一个更加简单有效的模型，即大名鼎鼎的  <strong>transformer</strong> 。正如论文标题所说，该模型的encoder和decoder都将基于attention机制实现。</p><blockquote><p> We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</p></blockquote><p>【PS：想来<em>simple</em> 一词 似乎有些凡尔赛的意味，正所谓大道至简，用在这篇论文似乎也毫不违和。相比于堆叠卷积层的 <em>cnn</em> 亦或是递归的 <em>rnn</em> 来说，确实 simple。不过标题着实有些“狂”了，可能也只有Google的名气才配的上吧，换做别人估计被喷惨了/滑稽】</p><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p><img src="/articles/Attention-Is-All-You-Need/image-20210715161152396.png" alt="image-20210715161152396"></p><p><strong>Encoder</strong></p><p>这一部分由多个(N=6)相同的层堆叠而成。每一层又包括两个子层，对应图中部分。第一个是一个多头自注意力机制[<em>a multi-head self-attention</em>]，第二个是一个全连接的前馈神经网络[a position-wise feed-forward networks]。这两个子层通过残差连接（residual connection ）。所有子层和embedding 层的输出维度都统一为512（便于进行残差连接）。</p><p><strong>Decoder</strong></p><p>同样地，解码器也由多个(N=6)相同的层堆叠而成。每层包括三个子层。在编码器的子层基础上增加了一个<em>masked multi-head self-attention</em> 。三个子层通过残差连接起来。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>论文中对 attention定义如下：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>Q, k, V即quary、key、value对应的向量。$Q\in R^{n\times d_k},K\in R^{m\times d_k},V\in R^{d_v}$。</p><p>两种常见的注意力机制是：additive attention 和 dot-product attention ，显然从公式中可见使用的是后者。其中$\sqrt{d_k}$ 起到了缩放的作用，使得softmax能更好的work。并且命名为 Scaled Dot-Product Attention：</p><p><img src="/articles/Attention-Is-All-You-Need/image-20210715171719397.png" alt="image-20210715171719397"></p><p><strong>Multi-Head Attention</strong></p><p><img src="/articles/Attention-Is-All-You-Need/image-20210715172114776.png" alt="image-20210715172114776"></p><p>Multi-Head Attention其实就是多个Scaled Dot-Product Attention并行，然后将结果 concat 拼接。从公式来看：</p><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O\\Head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script><p>其中，$W_i^Q\in R^{d_{model}\times d_k},W_i^K\in R^{d_{model}\times d_k},W_i^V\in R^{d_{model}\times d_v},W^O\in R^{hd_v\times d_{model}}$ 。本文采用的self-attention，即取Q，K，V相同。</p><h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>进行非线性变换：也就是 linear 后面加个 ReLU而已。</p><script type="math/tex; mode=display">FFN(X)=max(0,xW_1+b_1)W_2+b_2</script><blockquote><p>While the linear transformations are the same across different positions, they use different parameters from layer to layer.   Another way of describing this is as two convolutions with kernel size 1.</p></blockquote><p>【PS：Another way of describing this is as <em>two convolutions with kernel size 1</em> /惊！看来还是有借鉴cnn的卷积思想嘛。/滑稽】</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>这一部分的作用就是为了引入序列中的顺序信息，或者说位置信息。positional encodings（位置编码） 与input embeddings（如：词向量） 具有相同的维度$d_{model}$ 。通过将两向量相加，从而将对应的位置信息嵌入进去。位置编码的方式如下：</p><script type="math/tex; mode=display">PE(pos,2i)=sin(pos/10000^{2i/d_{model}})\\PE(pos,2i+1)=cos(pos/10000^{2i/d_{model}})</script><p>其中，$pos$ 表示位置，$i$ 是维度，这样，每个位置编码都对应一个正弦信号。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>抛弃cnn和rnn后，仅仅使用attention显然是不够的，为了捕获输入序列中词向量之间的前后位置信息，不得不引入Positional Encoding。不过很好解决了传统encoder-decoder无法并行训练的问题。因此，transformer 训练速度要快得多。transformer模型在nlp任务上的非凡表现，也引来了其他人将其应用在其他任务上的思考，目前在其他领域上，也出现了各种花里胡哨的transformer改版。</p><p><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">官方开源代码</a></p><p>这里给上苏神大佬对《Attention is All You Need》的解读：<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码） - 科学空间|Scientific Spaces (kexue.fm)</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A
      
    
    </summary>
    
    
    
      <category term="paper reading" scheme="https://hahally.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>基于注意力机制的神经机器翻译的有效方法</title>
    <link href="https://hahally.github.io/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/"/>
    <id>https://hahally.github.io/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/</id>
    <published>2021-07-12T12:52:30.000Z</published>
    <updated>2021-07-12T12:59:57.249Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://aclanthology.org/D15-1166/" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></p><p>基于注意力机制的神经机器翻译的有效方法</p></blockquote><p><strong>Bib TeX</strong></p><blockquote><p>@inproceedings{luong-etal-2015-effective,<br> title = “Effective Approaches to Attention-based Neural Machine Translation”,<br> author = “Luong, Thang  and<br>   Pham, Hieu  and<br>   Manning, Christopher D.”,<br> booktitle = “Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing”,<br> month = sep,<br> year = “2015”,<br> address = “Lisbon, Portugal”,<br> publisher = “Association for Computational Linguistics”,<br> url = “<a href="https://aclanthology.org/D15-1166" target="_blank" rel="noopener">https://aclanthology.org/D15-1166</a>“,<br> doi = “10.18653/v1/D15-1166”,<br> pages = “1412—1421”,<br>}</p></blockquote><h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>在神经机器翻译中引入注意力机制(Attention)，使模型在翻译过程中选择性的关注句子中的某一部分。本文研究了两种简单有效的注意力机制。</p><ul><li>a global approach which always attends to all source words【全局方法，每次关注所有源词】</li><li>a local one that only looks at a subset of source words at a time【局部方法，每次关注原词的一个子集】</li></ul><p><em>global attention</em> 类似方法<strong>[1]</strong>，但架构上更加简单。<em>local attention</em> 更像是 <em>hard and soft attention</em> <strong>[2]</strong>的结合。两种方法在英德语双向翻译任务中取得了不错的成绩。与已经结合了已知技术（例如 dropout）的非注意力系统相比，高了5.0个BLEU点。在WMT’15英语到德语的翻译任务中表现 SOTA（state-of-the-art）。</p><blockquote><p>With local attention, we achieve a significant gain of　5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.</p></blockquote><h3 id="Neural-Machine-Translation"><a href="#Neural-Machine-Translation" class="headerlink" title="Neural Machine Translation"></a>Neural Machine Translation</h3><p>模型结构：</p><p><img src="/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712173309790.png" alt="image-20210712173309790"></p><p>采用堆叠的 <em>LSTM</em>结构<strong>[3]</strong>。其目标函数为：</p><script type="math/tex; mode=display">J_t = \sum_{(x,y)\in D}-logP(y|x)</script><p>D为训练的语料。x 表示源句子，y表示翻译后的目标句子。</p><h3 id="Attention-based-Models"><a href="#Attention-based-Models" class="headerlink" title="Attention-based Models"></a>Attention-based Models</h3><p>这部分包括两种注意力机制：global 和 local。两种方式在解码阶段，将使用堆叠LSTM顶层的隐藏状态 $h_t$ 作为输入。区别在于获取上下文向量表示$c_t$方法不同。然后通过一个 简单的 <em>concatenate layer</em> 获得一个注意力隐藏状态$\hat h_t$:</p><script type="math/tex; mode=display">\hat h_t = tanh(W_c[c_t;h_t])</script><p>最后通过 <em>softmax layer</em> 得出预测概率分布:</p><script type="math/tex; mode=display">p(y_t|y<t,x)=softmax(W_s\hat h_t)</script><p><strong>Global Attention</strong></p><p><img src="/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712174342526.png" alt="image-20210712174342526"></p><p>主要思想是通过编码器的所有隐藏状态(hidden state)来获取上下文向量(context vector)表示 $c_t$。可变长度对齐向量$a_t$通过比较当前目标隐藏状态$h_t$和每个源隐藏状态$\overline h_s$得到：</p><script type="math/tex; mode=display">a_t(s) = align(h_t,\overline h_s)=\frac{exp(score(h_t,\overline h_s))}{\sum_{s'}exp(h_t,\overline h_{s^{'}})}</script><p>score被称为 <em>content-based</em> 函数：</p><script type="math/tex; mode=display">score(h_t,\overline h_s)=\begin{cases}h_t^{T}\overline h_s, dot\\h_t^{T}W_a\overline h_s, general\\W_a[h_t;\overline h_s], concat\end{cases}</script><p>与<strong>[1]</strong>的区别在于：</p><ul><li>只在编码器和解码器的顶部使用隐藏状态</li><li>计算路径更加简单：$h_t-&gt;a_t-&gt;c_t-&gt;\hat h_t$</li></ul><p><strong>Local Attention</strong></p><p>global 模式下，模型需要关注全局信息，其代价是非常大的。因此也就出现了 local attention。让注意力机制只去关注其中的一个子集部分。<em>其灵感来自于</em> <strong>[2]</strong>。</p><p><img src="/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712185709989.png" alt="image-20210712185709989"></p><p>对比两张模型图来看，其中的局部对齐权重$a_t$由一部分局部隐藏状态计算得到，其长度变成了固定的，并且还多了一个Aligned position $p_t$ ，然后上下文向量(context vector) $c_t$ 由窗口$[p_t-D,p_t+D]$内的隐藏状态集合的加权平均得到。<em>其中D根据经验所得</em>。</p><p>考虑两种变体：</p><ul><li><p>Monotonic alignment (local-m)</p><p>即简单设置 $p_t = t$ ，认为源序列于目标序列是单调对齐的，那么$a_t$ 其实就和公式（4）计算方法一样了。</p></li><li><p>Predictive alignment (local-p)</p><p>$p_t=S·sigmoid(v_p^{T}tanh(W_ph_t))$ ，$v_p$和$W_p$是预测$p_t$ 的模型参数。S为源句子长度。最后$p_t\in [0,S]$ 。同时为了使对齐点更靠近$p_t$，设置一个以$p_t$为中心 的高斯分布，即$a_t$ 为：$a_t(s)=align(h_t,\overline h_s)exp(-\frac{(s-p_t)^2}{2\sigma^2}),\sigma=\frac{D}{2}$，s为高斯分布区间内的一个整数。</p></li></ul><h3 id="Input-feeding-Approach"><a href="#Input-feeding-Approach" class="headerlink" title="Input-feeding Approach"></a>Input-feeding Approach</h3><p>这一部分，主要是为了捕获在翻译过程中哪些源单词已经被翻译过了。对齐决策应当综合考虑过去对齐的信息。该方法将注意力向量$\hat h_t$ 作为下一个时间步的输入。主要有两个作用：</p><ul><li>希望模型充分关注到先前的对齐信息</li><li>创建一个在水平和垂直方向上都很深的网络</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>整篇论文看下来，大概就是在别人的baseline中引入注意力机制（global and local），然后使用<em>Input-feeding</em> 方法将过去的对齐信息考虑进来（大概就是加入了一个先验知识吧）。【PS：震惊！这些创新的点的灵感都来自其让人的论文中的方法。】</p><p>最后手动滑稽：</p><blockquote><p>Attention is all you need!</p></blockquote><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.  2015. Neural machine translation by jointly learning to align and translate. InICLR.</p><p>[2] Kelvin Xu,  Jimmy Ba,  Ryan Kiros,  Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show,attend and tell: Neural image caption generation with visual attention. InICML.</p><p>[3] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2015. Recurrent neural network regularization. InICLR.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/D15-1166/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Effective Approaches to Attention-based Neural M
      
    
    </summary>
    
    
    
      <category term="paper reading" scheme="https://hahally.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>sequence-to-sequence-learning-with-neural-networks</title>
    <link href="https://hahally.github.io/articles/sequence-to-sequence-learning-with-neural-networks/"/>
    <id>https://hahally.github.io/articles/sequence-to-sequence-learning-with-neural-networks/</id>
    <published>2021-07-10T09:30:14.000Z</published>
    <updated>2021-07-10T09:55:31.766Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a></p><p>基于神经网络的seq2seq</p></blockquote><p>很好的解决了序列到序列之间的映射问题，在语音识别和机器翻译这种长度未知且具有顺序的问题能够得到很好的解决。该模型应用到英语到法语的翻译任务，数据集来自WMT’14，在整个测试集上的BLEU得到达到34.8。</p><p>模型结构：</p><p><img src="/articles/sequence-to-sequence-learning-with-neural-networks/image-20210710154526137.png" alt="image-20210710154526137"></p><p>这里使用了两个串联的 lstm 网络，前一个用于读取输入序列，产生一个大的固定维度的向量表示，然后再用一个lstm 网络从向量表示中提取输出序列。一个编码器(Encoder)，一个解码器(Decoder)。</p><p>值得注意的是，论文提到在实现时，有三点与 lstm 不一样。【Our actual models differ from the above description in three important ways.】</p><ul><li>使用两个不同的lstm</li><li>4层 lstm</li><li>将输入序列进行反转【a,b,c —&gt; c,b,a】【PS： 这就是传说中的反向操作吗？！莫名其妙的trick，滑稽.jpg】</li></ul><p>目标函数：</p><script type="math/tex; mode=display">\frac{1}{|S|}\sum_{(T,S)\in S}logP(T|S)</script><p>S为训练集，T表示正确的翻译。训练结束后，进行翻译时，在模型产生的多个翻译结果中找到最可能正确的翻译：</p><script type="math/tex; mode=display">\hat T = \mathop{argmax}_{T}P(T|S)</script><p>通过一个简单的 <em>left-to-right beam search</em> 解码器(decoder)搜索最可能的翻译。</p><blockquote><p>We search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some translation. At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the model’s log probability. As soon as the “<EOS>“ symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam search(Table 1).</EOS></p></blockquote><p><strong>一个小的总结</strong></p><p>总的来说， <em>Reversing the Source Sentences</em> 这个操作给模型带来了很大的提升。</p><blockquote><p>the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU scores of its decoded translations increased from 25.9 to 30.6.</p></blockquote><p>至于为什么，论文中也没有给出很好的解释（大概是说，反转后，前面源句子的前几个词与目标句子的前几个词距离更近，模型能更好收敛。【a,b,c |w,x,y—&gt; c,b,a|w,x,y】w,x,y为a,b,c对应的翻译，反转后，a,b,c与w,x,y的平均距离不变，a离w更近了。但是c离y更远却没有影响模型精度。可能这就是玄学吧？！）。【PS：难道是因为误打误撞的尝试然后发现效果惊人，然后就发论文了？不过这篇论文确实奠定了之后的seq2seq模型的基础。】</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;S
      
    
    </summary>
    
    
    
      <category term="paper reading" scheme="https://hahally.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>Lost-in-just-the-translation</title>
    <link href="https://hahally.github.io/articles/Lost-in-just-the-translation/"/>
    <id>https://hahally.github.io/articles/Lost-in-just-the-translation/</id>
    <published>2021-07-09T14:09:24.000Z</published>
    <updated>2021-07-09T14:10:35.104Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Lost in just the translation</p></blockquote><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>本文介绍了自然语言翻译文本信息隐藏系统的设计与实现，并给出了实验结果。与前人的工作不同，本文提出的协议<strong>只需要翻译文本就可以恢复隐藏信息</strong>。这是一个重大的改进，因为传输源文本既浪费资源又不安全。现在，系统的安全性得到了改善，这不仅是因为源文本不再对敌方有用，还因为现在可以使用更广泛的防御系统(如混合人机翻译)。</p><h3 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h3><p><img src="/articles/Lost-in-just-the-translation/image-20210709214343204.png" alt="image-20210709214343204"></p><ul><li><p>Producing translations</p><p>方法大致与论文【Translation-Based Steganography】中提到的一样</p></li><li><p>Tokenization</p><p>双方使用相同的 Tokenization 算法，以获得相同的句子序列</p></li><li><p>Choosing h</p><p>选择合适的 h ($h \ge 0$)，h表示将信息隐藏在每个句子中的长度的位数。</p></li><li><p>Selecting translations</p><p>对于所有翻译，编码器首先使用与接收方共享的密钥计算每个翻译的加密键值散列。其基本思想是在给定句子的所有译文中选择一个句子，然后对其进行适当的长度编码，并在隐藏的信息中选择合适的位置。然而，由于给定句子中的位编码数量是可变的，因此该算法在这方面有很大的自由度。</p></li><li><p>Optimized Handling of Hash Collisions</p><p>哈希冲突的处理优化</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>为了不传输源文本，从而，引入了哈希映射和 Tokenization以及参数 h。产生翻译文本过程中混合人机翻译结果，使得隐藏的信息更加难以检测。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Lost in just the translation&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h3&gt;&lt;p&gt;本文介绍了自然
      
    
    </summary>
    
    
    
      <category term="paper reading" scheme="https://hahally.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>基于翻译的隐写术</title>
    <link href="https://hahally.github.io/articles/%E5%9F%BA%E4%BA%8E%E7%BF%BB%E8%AF%91%E7%9A%84%E9%9A%90%E5%86%99%E6%9C%AF/"/>
    <id>https://hahally.github.io/articles/%E5%9F%BA%E4%BA%8E%E7%BF%BB%E8%AF%91%E7%9A%84%E9%9A%90%E5%86%99%E6%9C%AF/</id>
    <published>2021-07-09T14:04:26.000Z</published>
    <updated>2021-07-09T14:12:34.780Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://dl.acm.org/doi/10.1007/11558859_17" target="_blank" rel="noopener">Translation-Based Steganography</a></p><p>基于翻译的隐写术</p></blockquote><h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>这篇论文研究了用隐写术在自然语言文档自动翻译产生的噪音(“noise”)中隐藏信息的可能性。由于自然语言固有的冗余性为翻译的变化创造了足够的空间，因此机器翻译非常适合隐写。此外，因为在自动文本翻译中经常出现错误，信息隐藏机制插入的额外错误就很难检测出来，看起来就像是翻译过程中产生的正常噪音的一部分。正因如此，我们是很难确定翻译中的不准确是由隐写术的使用还是由翻译软件的缺陷造成的。</p><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>本文提出了一种用于自然语言文本中的隐蔽消息传输的新协议，为此我们有一个概念验证(proof-of-concept )实现。关键点就是将信息隐藏在自然语言翻译中经常出现的噪音中。在一对自然语言之间翻译[non-trivial]文本时，通常有许多可能的翻译结果。【大概意思应该是在不改变原文意思的情况下，翻译的结果是多种多样的】。选择这些翻译结果之一就可用于对信息进行编码。一个 <em>adversary</em> 要想检测出其中隐藏的信息，就必须明白包含隐藏信息的翻译是不可能由普通翻译生成的。由于翻译过程中本就夹杂一些噪声，这使得检测隐藏信息是十分困难的。例如，由于同义词的存在，在对原文进行翻译的过程中，使用同义词进行替换。随着翻译结果的增加，也增加了信息隐藏的可能性。</p><p>本文评估了使用自动机器翻译 (MT) 的自然语言翻译中隐蔽消息传输的潜在性。为了描述在机器翻译中的哪种变化是合理的，我们研究了各种 MT 系统产生的不同类型的错误。在机器翻译中观察到的一些变化对于人工翻译也显然是合理的。除了让 <em>adversary</em> 难以检测到隐藏信息的存在之外，基于翻译的隐写术也更容易使用。与之前的基于文本、图像和声音的隐写系统不一样，基于翻译的隐写，其 <em>cover</em> 是不需要保密的【the cover does not have to be secret.】。在基于翻译的隐写术中，源语言的原始文本可以是公开的，可以从公共资源中获取，并与译文一起，在<em>adversary</em>的视线范围内，在两方之间进行交换。在传统的图像隐写术中，经常出现的问题是，随后隐藏消息的源图像必须由发送者保密并且只使用一次（否则“diff”攻击将揭示隐藏消息的存在）。这增加了用户为每条消息创建新的秘密封面(secret cover)【周杰伦的专辑《不能说的秘密》？！滑稽脸.jpg】的负担。</p><blockquote><p> In translation-based steganography, the original text in the source language can be publically known, obtained from public sources, and, together with the translation, exchanged between the two parties in plain sight of the adversary. In traditional image steganography, the problem often occurs that the source image in which the message is subsequently hidden must be kept secret by the sender and used only once (as otherwise a “diff” attack would reveal the presence of a hidden message). This burdens the user with creating a new, secret cover for each message.</p></blockquote><p>基于翻译的隐写术没有这个缺点，因为对手无法对翻译应用差异分析来检测隐藏的消息。对手可能会生成原始消息的翻译，但无论使用隐写术，翻译可能会有所不同，使得差异分析无法检测隐藏的消息。</p><p>为了证明这一点，我们实现了一个隐写编码器和解码器。该系统通过以类似于在现有 MT 系统中观察到的变化和错误的方式更改机器翻译来隐藏消息。我们的网页上提供了原型的交互式版本。</p><p>在本文的其余结构如下。首先，第 2 节回顾了相关工作。在第 3 节中，描述了隐写交换的基本协议。在第 4 节中，我们给出了现有机器翻译系统中产生的错误的特征。第 5 节概述了实现和一些实验结果。在第 6 节中，我们讨论了基本协议的变体，以及各种攻击和可能的防御。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h3 id="Protocol"><a href="#Protocol" class="headerlink" title="Protocol"></a>Protocol</h3><p>本文的基本隐写协议工作如下。发件人首先需要获得源语言的封面(cover)。封面不必是保密的(secret)，可以从公共来源获得 ， 例如，新闻网站。然后发送者使用隐写编码器将源文本中的句子翻译成目标语言。隐写编码器本质上为每个句子创建多个翻译，并选择其中之一来对隐藏消息中的位进行编码。然后将翻译后的文本连同足以获得源文本的信息一起发送给接收者。这可以是源文本本身或对源的引用。然后接收者还使用相同的隐写编码器配置执行源文本的翻译。通过比较结果句子，接收者重建隐藏消息的比特流。图 1 说明了基本协议。</p><p><img src="/articles/%E5%9F%BA%E4%BA%8E%E7%BF%BB%E8%AF%91%E7%9A%84%E9%9A%90%E5%86%99%E6%9C%AF/image-20210709163311687.png" alt="image-20210709163311687"></p><h4 id="Producing-translations"><a href="#Producing-translations" class="headerlink" title="Producing translations"></a>Producing translations</h4><p>获取源文本后，发送方和接收方的第一步是使用相同的算法生成源文本的多个翻译。此步骤的目标是确定性地生成源文本的多个不同翻译。实现这一目标的最简单方法是在源文本中的每个句子上应用所有可用 MT 系统的（子集）。如果各方可以完全访问统计 MT 系统的代码，他们可以通过使用不同的语料库训练，从同一代码库生成多个 MT 系统。</p><p>除了使用多个翻译系统生成不同的句子外，还可以对结果翻译应用后处理以获得额外的变化。这种后处理包括模拟任何（MT）翻译中固有噪声的转换。例如，后处理器可以插入常见的翻译错误（如第 4 节所述）。</p><p>由于不同引擎之间的翻译质量不同，并且还取决于应用了哪些后处理器来处理结果，因此翻译系统使用启发式方法为每个翻译分配一个概率，描述其与其他翻译相比的相对质量。启发式可以基于生成器的经验和基于语言模型对句子质量进行排名的算法 。用于生成翻译及其排名的特定翻译引擎、训练语料库和后处理操作集是想要进行秘密通信的两方密钥共享的一部分。</p><h4 id="Selecting-a-translation"><a href="#Selecting-a-translation" class="headerlink" title="Selecting a translation"></a>Selecting a translation</h4><p>选择翻译以对隐藏消息进行编码时，编码器首先使用生成器算法分配的概率构建可用转换的霍夫曼树。然后算法选择与要编码的位序列对应的句子。 </p><p>使用霍夫曼树根据翻译质量估计选择句子可确保较少选择翻译质量较低的句子。此外，所选翻译的质量越低，传输的比特数就越高。</p><p>这减少了所需的封面文本总量，从而减少了对手可以分析的文本量。编码器可以使用相对翻译质量的下限来排除估计翻译质量低于某个阈值的句子，在这种情况下，该阈值成为发送者和接收者之间共享秘密的一部分。</p><h4 id="Keeping-the-source-text-secret"><a href="#Keeping-the-source-text-secret" class="headerlink" title="Keeping the source text secret"></a>Keeping the source text secret</h4><p>所提出的方案可以适用于需要对源文本保密的水印。这可以按如下方式实现。编码器计算每个翻译句子的（加密）哈希。然后它选择一个句子，使得翻译句子的散列的最后一位对应于要传输的隐藏消息中的下一位。 然后解码器只计算接收到的句子的散列码并连接相应的最低位获取隐藏信息。</p><p>该方案假设句子足够长，几乎总是有足够的变化来获得具有所需最低位的散列。每当没有一个句子产生可接受的哈希码时，就必须使用纠错码来纠正错误。使用这种变化会降低编码所能达到的比特率。更多细节可以在我们的技术报告中找到。</p><h3 id="Lost-in-Translation"><a href="#Lost-in-Translation" class="headerlink" title="Lost in Translation"></a>Lost in Translation</h3><p>现代 MT 系统会在翻译中产生许多常见错误。本节描述了其中一些错误的特征。虽然我们描述的错误不是可能错误的完整列表，但它们代表了我们在示例翻译中经常观察到的错误类型。翻译错误的扩展特征可以在我们的技术报告中找到（由于篇幅限制，此处省略）。这些错误中的大多数是由于当代 MT 系统对统计和句法文本分析的依赖造成的，导致缺乏语义和上下文意识。这会产生一系列错误类型，我们可以使用它们来合理地改变文本，从而产生进一步的标记可能性。</p><h4 id="Functional-Words"><a href="#Functional-Words" class="headerlink" title="Functional Words"></a>Functional Words</h4><p>一类经常发生但不破坏意义的错误是功能词翻译不正确，如冠词、代词和介词。因为这些功能词通常与句子中的另一个词或短语有很强的关联，复杂的结构似乎经常会导致这些词的翻译错误。此外，不同的语言对这些词的处理方式非常不同，因此在使用未考虑这些差异的引擎时会导致翻译错误。</p><p>例如，许多使用冠词的语言并不在所有名词前使用它们。这在从文章规则不同的语言翻译时会导致问题。例如，法语句子“La vie est paralysee.”在英语中翻译为“Life is paralyzed.”。然而，翻译引擎可以预见地将其翻译为“The life is paralyzed.”；“life in general”意义上的“life”并没有用出现在一篇英文文章中。这与许多不可数名词如“水”和 “钱”一样，而导致类似的错误。</p><p>通常，介词的正确选择完全取决于句子的上下文。例如，法语中的 $J’habite$ $\grave{a}$ 100 $m\grave{e}tres$ $de$ $lui$在英语中的意思是“我住在离他100米的地方”。然而，[20] 将其翻译为“我与他一起生活 100 米”，而 [71]将其翻译为“在他的 100 米处生活”。两者都使用“$\grave{a}$”（“with/in”）的不同翻译这完全不适合上下文。</p><h4 id="Blatant-Word-Choice-Errors"><a href="#Blatant-Word-Choice-Errors" class="headerlink" title="Blatant Word Choice Errors"></a>Blatant Word Choice Errors</h4><p>不太常见的是，在翻译中选择完全不相关的单词或短语。例如，<em>I’m staying home</em>和<em>I am staying home</em>都被[20]翻译成德语为<em>Ich bleibe Haupt</em>（<em>I’m staying head</em>）而不是<em>Ich bleibe zu Hause</em>。这些不同于语义错误，反映了实际引擎或其字典中的某种缺陷，明显影响了翻译质量。</p><h4 id="Additional-Errors"><a href="#Additional-Errors" class="headerlink" title="Additional Errors"></a>Additional Errors</h4><p>遇到了其他几种有趣的错误类型，由于篇幅原因，我们将只简要介绍这些错误类型。</p><ul><li>基本语法错误导致翻译如<em>It do not work</em></li><li>逐字翻译，尤其是惯用语的翻译，会产生诸如<em>The pencils are at me.</em>这样的结构</li><li>源词典中没有的单词只是不翻译</li><li>语言之间反身结构的不正确映射会导致反身冠词被错误地插入目标翻译中（例如，<em>Ich kamme mich</em>变成了<em>I comb myself</em>）。</li></ul><h4 id="Translations-between-Typologically-Dissimilar-Languages"><a href="#Translations-between-Typologically-Dissimilar-Languages" class="headerlink" title="Translations between Typologically Dissimilar Languages"></a>Translations between Typologically Dissimilar Languages</h4><p>类型学上相距遥远的语言是指形式结构彼此完全不同的语言。这些结构差异体现在许多领域（例如句法（短语和句子结构）、语义（含义结构）和形态（词结构））。毫不奇怪，由于这些差异，在类型上相距遥远的语言（中文和英文、英文和阿拉伯文等）之间的翻译经常很糟糕，以至于不连贯或不可读。我们在这项工作中没有考虑这些语言，因为翻译质量通常很差，结果翻译的交换可能是难以置信的。</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>本节描述了实现的一些方面，重点介绍了用于获得生成的翻译变化的不同技术。</p><h4 id="Translation-Engines"><a href="#Translation-Engines" class="headerlink" title="Translation Engines"></a>Translation Engines</h4><p>当前实现使用 Internet 上可用的不同翻译服务来获得初始翻译。当前的实现支持三种不同的服务，我们计划在未来添加更多服务。添加新服务只需要编写一个函数，将给定的句子从源语言翻译成目标语言。应使用可用 MT 服务的哪个子集由用户决定，但必须至少选择一个引擎。</p><p>选择多个不同翻译引擎的一个可能问题是它们可能具有不同的错误特征（例如，一个引擎可能无法翻译带有缩写的单词）。知道特定机器翻译系统存在此类问题的对手可能会发现所有句子中有一半存在与这些特征匹配的错误。由于普通用户不太可能在不同的翻译引擎之间交替，这将揭示隐藏消息的存在。</p><p>更好的选择是使用相同的机器翻译软件，但使用不同的语料库对其进行训练。特定语料库成为隐写编码器使用的密钥的一部分； Victor Raskin 和 Umut Topkara 之前在另一个上下文（[2] 的上下文）中讨论了这种使用语料库作为关键字的情况。因此，对手无法再检测到不同机器翻译算法导致的差异。这种方法的一个问题是获得好的语料库很昂贵。此外，划分单个语料库以生成多个较小的语料库将导致更糟糕的翻译，这可能再次导致可疑文本。也就是说，完全控制翻译引擎还可以允许翻译算法本身的微小变化。例如，GIZA++系统提供了多种计算翻译的算法[9]。这些算法的主要区别在于如何生成翻译“候选结果”。更改这些选项也有助于生成多个翻译。</p><p>从翻译引擎获得一个或多个翻译后，该工具会使用各种后处理算法生成其他变体。只需使用一个高质量的翻译引擎并依靠后处理生成替代翻译，就可以避免使用多个引擎的问题。</p><h4 id="Semantic-Substitution"><a href="#Semantic-Substitution" class="headerlink" title="Semantic Substitution"></a>Semantic Substitution</h4><p>语义替换是一种非常有效的 post-pass，并且已在以前的方法中用于隐藏信息 [2,5]。与以前工作的一个主要区别是，与原始文本中的语义替换相比，由语义替换引起的错误在翻译中更合理。</p><p>传统语义替换的一个典型问题是需要替换列表。替换列表是由语义上足够接近的词组成的元组列表，可以在任意句子中用一个词替换另一个词。对于传统的语义替换，这些列表是手工生成的。语义替换列表中的一对单词的示例将是舒适和方便的。不仅手工构建替换列表很乏味，而且列表中包含的内容也必须是保守的。例如，一般替换列表不能包含诸如明亮和光之类的词对，因为光可以用于不同的意义（意味着轻松、不精确甚至用作名词）。</p><p>翻译的语义替换没有这个问题。使用原始句子，可以自动生成语义替换，甚至可以包含上述某些情况（无法添加到一般单语替换列表中）。基本思想是在两种语言之间来回翻译以找到语义相似的单词。假设翻译是准确的，源语言中的单词可以帮助提供必要的上下文信息，以限制对当前上下文中语义接近的单词的替换。</p><p>假设源语言是德语（d），翻译的目标语言是英语（e）。原始句子包含一个德语单词 d1<br>并且翻译包含一个单词 e1，它是 d1的翻译。基本算法如下，如图2所示：</p><p><img src="/articles/%E5%9F%BA%E4%BA%8E%E7%BF%BB%E8%AF%91%E7%9A%84%E9%9A%90%E5%86%99%E6%9C%AF/image-20210709175052191.png" alt="image-20210709175052191"></p><ul><li>找出 d1 的所有其他翻译的集合，并称这个集合为$E_{d1}$。 $E_{d1}$是语义替换的候选集。$e_1 \in E_{d1}$。</li><li>找出 e1 的所有翻译；将此集合称为 $D_{e1}$。此集合称为集合<em>witnesses</em>。</li><li>对于每个单词$e \in  E_{d1}-\{e1\}$找到所有的翻译 $D_{e1}$并计算$D_e \cap D_{e1}$中元素的数量。如果该数字高于给定的阈值 t，则将 e 添加到 e1 的可能语义替代列表中。</li></ul><p>一个<em>witness</em>是源语言中的一个词，它也翻译成目标语言中的两个词，从而确认两个词的语义接近度。<em>witness</em>阈值 t 可用于将更多可能的替换与更高的不适当替换的可能性进行交换。</p><h4 id="Adding-plausible-mistakes"><a href="#Adding-plausible-mistakes" class="headerlink" title="Adding plausible mistakes"></a>Adding plausible mistakes</h4><p>另一种可能的 post-pass 将 MT 系统常见的错误添加到翻译中。我们的实现可以使用的转换基于第 4 节中对 MT 错误的研究。当前系统支持使用手工制作的语言特定替换来更改冠词和介词，这些替换尝试模仿观察到的可能错误。</p><h4 id="Results-from-the-Prototype"><a href="#Results-from-the-Prototype" class="headerlink" title="Results from the Prototype"></a>Results from the Prototype</h4><p>系统的不同配置产生不同质量的翻译，但即使质量下降也是不可预测的。有时我们的修改实际上（巧合）提高了翻译质量。</p><p>应该注意的是，为简单起见，原型当前使用的引擎是公开可用的免费网络引擎，并且这不是自定义生成引擎或付费商业软件的输出的示范。为了更好地说明原型系统，给出了以下稍微更广泛的示例： 24 位字符串“lit”是在来自 Deutsche Welle 网站的电影评论部分的翻译中编码的。使用我们的原型将文本从德语翻译成英语，没有语义替换，启用冠词和介词替换，也没有“不良阈值”。源引擎是 Babelfish、Google 和 LinguaTec。德语文本是一段关于摩洛哥电影《风马》的评论的第一部分，内容如下：</p><p>······省略······</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>本节讨论对隐写编码的各种攻击以及针对这些攻击的可能防御。讨论是非正式的，因为该系统基于 MT 的缺陷，这些缺陷很难正式分析（这也是 MT 是一个如此困难的话题的原因之一）。</p><h4 id="Future-Machine-Translation-Systems"><a href="#Future-Machine-Translation-Systems" class="headerlink" title="Future Machine Translation Systems"></a>Future Machine Translation Systems</h4><p>【所提出的隐写编码在未来可能面临的一个可能问题是机器翻译的重大进展。如果机器翻译变得更加准确，那么可能出现的似是而非的错误可能会变得更小。然而，当前机器翻译错误的一大类是由于机器翻译器没有考虑到上下文。】</p><p>为了显着改进现有的机器翻译系统，一个必要的功能是保存从一个句子到下一个句子的上下文信息。只有有了这些信息，才有可能消除某些错误。但是将这种上下文引入机器翻译系统也为在翻译中隐藏信息带来了新的机会。【一旦机器翻译软件开始保留上下文，使用隐写协议的两方就有可能使用这个上下文作为密钥。】通过为各自的翻译引擎植入 k 位上下文，他们可以使翻译中的偏差变得合理，迫使对手可能尝试$2^k$种可能的上下文输入，以便甚至确定使用该机制的可能性。这类似于基于密钥拆分语料库的想法，不同之处在于不会影响每句翻译的整体质量。</p><h4 id="Repeated-Sentence-Problem"><a href="#Repeated-Sentence-Problem" class="headerlink" title="Repeated Sentence Problem"></a>Repeated Sentence Problem</h4><p>在翻译中隐藏消息的任何方法的一个普遍问题是，如果源语言中的文本包含两次相同的句子，它可能会被翻译成两个不同的句子，具体取决于隐藏位的值。由于机器翻译系统（不保留上下文）总是会产生相同的句子，这将允许攻击者怀疑使用了隐写术。解决这个问题的方法是不要在源文本中使用重复的句子来隐藏数据，而始终输出用于该句子第一次出现的翻译。</p><p>这种攻击类似于图像隐写术中使用的攻击。如果图像经过数字化修改，图像某些不可信区域的颜色变化可能会揭示隐藏信息的存在。解决这个问题对于文本隐写术来说更容易，因为检测两个句子是否相同比检测图像中的一系列像素属于相同的数字构造形状并因此必须具有相同的颜色更容易。</p><h4 id="Statistical-Attacks"><a href="#Statistical-Attacks" class="headerlink" title="Statistical Attacks"></a>Statistical Attacks</h4><p>统计攻击在击败图像、音频和视频的隐写术方面非常成功（参见，例如，[8,14,19]）。对手可能有一个统计模型（例如语言模型），所有可用 MT 系统的翻译都遵守该模型。例如，Zipf 定律 [15] 指出，一个单词的频率与其在所有单词的按频率排序的列表中的排名成反比。Zipf 定律适用于英语，事实上，甚至在名词、动词、形容词等个别类别中也适用。</p><p>假设所有合理的翻译引擎通常都遵循这样的统计模型，隐写编码器必须小心不要导致与此类分布的明显偏差。一旦知道这样的统计规律，实际上很容易修改隐写编码器以消除明显偏离所需分布的翻译。例如，Golle 和 Farahat [10] 指出（在不同的加密上下文中）可以在不明显偏离 Zipf 定律的情况下广泛修改自然语言文本。换句话说，这是一个非常易于管理的困难，只要隐写系统是“Zipf-aware”的。</p><p>我们不能排除尚未发现的翻译语言模型的存在，这些模型可能会被我们现有的实现所违反。然而，我们希望发现和验证这样的模型对于对手来说是一项重要的任务。另一方面，给定这样的模型（正如我们上面指出的）修改隐写系统很容易，通过避免被标记的句子来消除偏差。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1007/11558859_17&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Translation-Based Steganography&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
    
      <category term="paper reading" scheme="https://hahally.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>基于形容词删除策略的语言隐写与密钥共享</title>
    <link href="https://hahally.github.io/articles/%E5%9F%BA%E4%BA%8E%E5%BD%A2%E5%AE%B9%E8%AF%8D%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E7%9A%84%E8%AF%AD%E8%A8%80%E9%9A%90%E5%86%99%E4%B8%8E%E5%AF%86%E9%92%A5%E5%85%B1%E4%BA%AB/"/>
    <id>https://hahally.github.io/articles/%E5%9F%BA%E4%BA%8E%E5%BD%A2%E5%AE%B9%E8%AF%8D%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E7%9A%84%E8%AF%AD%E8%A8%80%E9%9A%90%E5%86%99%E4%B8%8E%E5%AF%86%E9%92%A5%E5%85%B1%E4%BA%AB/</id>
    <published>2021-07-08T13:38:14.000Z</published>
    <updated>2021-07-08T13:47:44.862Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://aclanthology.org/C12-1031/" target="_blank" rel="noopener">Adjective Deletion for Linguistic Steganography and Secret Sharing</a></p></blockquote><p><strong>概念</strong></p><ul><li><p>Adjective Deletion 【形容词删除】</p></li><li><p>Linguistic Steganography 【语言隐写术】隐写术就是将秘密信息隐藏到看上去普通的信息中进行传送。</p><blockquote><p>Linguistic steganography is a form of covert communication in which information is embedded in a seemly innocent cover text so that the presence of the information is imperceptible to an outside observer (human or computer).</p><p>理想的 Linguistic Steganography满足两个基本要求：high imperceptibility（不易察觉） and high payload capacity（高信息承载容量）</p></blockquote></li><li><p>Secret Sharing 【密钥共享】一种分发、保存、恢复秘密密钥的方法。</p></li></ul><p><strong>文章所作工作</strong></p><ol><li><p>验证删除形容词的可行性的两种方法：「checking the acceptability of adjective deletion in noun phrases.」</p><ul><li>Google n-gram corpus 【谷歌语料库】「check 删除一个形容词后的 <strong>context</strong> 的流利程度」</li><li>SVM模型(使用n-gram counts和其他方法训练得到) 「classify 是否在 <strong>context</strong> 删除形容词」</li></ul></li><li><p>证明删除形容词技术可以集成到一个存在的语言系统(an existing linguistic stegosyste)</p></li><li><p>提出一种新的基于形容词删除技术(adjective deletion)的密钥共享(secret sharing)方法</p></li></ol><p><strong>(t,n)-threshold scheme</strong></p><p>论文中采用的 secret sharing方法是基于(2, 2)-threshold, 其中共享的必须是两个可比较的文本(two comparable texts)。通过形容词删除技术将【0s 和 1s 的加密位字符串(secret bitstring;)】嵌入到两个文本中，这两个文本可以组合起来，获得秘密位串。</p><blockquote><p>Hence the proposed method is a novel combination of secret sharing and linguistic steganography.</p></blockquote><p>一种密钥共享与语言隐写技术的新颖组合方法？！</p><p><strong>Adjective Deletion</strong></p><p>在不影响句子流利程度和语义的情况下，可以将一些形容词删除。在下面的例子中，删除 <em>own</em> 这个形容词后，句意并没有发生改变。</p><blockquote><p>he spent only his own money.</p><p>he spent only his money.</p></blockquote><p>一种极端情况 adjective-noun ：大致可以理解为正确的废话（正确但duck不必的形容）吧。</p><blockquote><p>unfair prejudice</p><p>horrible crime</p><p>fragile glass</p></blockquote><p><strong>隐写术种的语言转换(Linguistic Transformations for Steganography)</strong></p><p>如：词汇替换、短语意译、句子结构调整、语义转换等【PS：有种毕业论文降重的赶脚】</p><p>还有一种研究通过在翻译的文本中嵌入信息。在机器翻译算法中引入水印作为参数，对带有水印的译文进行概率识别。</p><p>【Watermarking the outputs of structured prediction with an application in statistical machine translation】</p><blockquote><p>Another recent work proposedby Venugopal et al. (2011) introduces a watermark as a parameter in the machine translation algorithm and probabilistically identifies the watermarked translation.</p></blockquote><p><strong>隐写系统评估</strong></p><p>可以从两个方面对系统进行评估：安全性(security level)和嵌入容量( embedding capacity)</p><ol><li><p>security level： automatic evaluation and human evaluation.</p><p>automatic evaluation 大概就是使用机器翻译评价指标 BLEU 和 NIST。计算隐藏文本与原始文本之间的距离。</p><p>human evaluation 就是认为指定的一套评估标准(seven-point scale)。</p></li><li><p>embedding capacity</p><p>将嵌入的信息按每个语言单位(每个句子或每个单词)比特进行量化。</p></li></ol><p>隐写系统的语言转换和编码方法，以及隐写文本的选择都会影响隐写系统的安全级别和有效负载能力。</p><p><strong>句子压缩</strong></p><p>句子压缩，文本简化和文本摘要通常涉及删除句子中不重要的词，以使文本更简洁。论文中指出，形容词删除可以用在句子压缩之前或之后。进一步简化句子。</p><blockquote><p>The proposed adjective deletion methods can be applied before and/or after a sentence compression system. Deleting unnecessary adjectives before can help the system focus on other content of a sentence. Deleting unnecessary adjectives after can generate an even more concise sentence.</p></blockquote><p><strong>Deletable Adjective Classification</strong></p><p>论文中，为了使一个形容词的删除是可以接受的，使用两个检查：语法性和自然性检查(grammaticality and naturalness checks)。</p><ol><li><p>N-gram Count 方法</p><p>计算删除形容词前后文本的 N-gram 统计得分，通过设置一个阈值，来判断删除后的文本是否可接受。</p></li><li><p>Features for the SVM</p><p>支持向量机的特征有：</p><ul><li>N-gram Counts</li><li>Lexical Association Measures【确定形容词和名词之间的关联程度。】</li><li>Noun and Adjective Entropy【名词和形容词熵】</li><li>Contextual α-Skew Divergence【上下文的倾斜散度？】</li></ul></li></ol><p><strong>Secret Sharing Scheme</strong></p><p>将一个密钥位串分成两个部分$share_0$和 $share_1$ 。若目标形容词在$share_0$ 中保留，则密钥值取0，若目标形容词在$share_1$中保留，则密钥值取1。</p><blockquote><p>Share0 holds secret bits as 0s and Share1 holds secret bits as 1s</p></blockquote><p>下面是一个密钥位串为 101 的例子：</p><p><img src="/articles/%E5%9F%BA%E4%BA%8E%E5%BD%A2%E5%AE%B9%E8%AF%8D%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E7%9A%84%E8%AF%AD%E8%A8%80%E9%9A%90%E5%86%99%E4%B8%8E%E5%AF%86%E9%92%A5%E5%85%B1%E4%BA%AB/image-20210708213208658.png" alt="image-20210708213208658"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/C12-1031/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Adjective Deletion for Linguistic Steganography 
      
    
    </summary>
    
    
    
      <category term="paper reading" scheme="https://hahally.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>蛋白质结构预测之lgb的baseline</title>
    <link href="https://hahally.github.io/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B%E4%B9%8Blgb%E7%9A%84baseline/"/>
    <id>https://hahally.github.io/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B%E4%B9%8Blgb%E7%9A%84baseline/</id>
    <published>2021-07-03T11:30:36.000Z</published>
    <updated>2021-07-03T12:56:48.376Z</updated>
    
    <content type="html"><![CDATA[<p>赛题：<a href="https://challenge.xfyun.cn/topic/info?type=protein" target="_blank" rel="noopener">蛋白质结构预测挑战赛</a></p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">################## utils.py #####################</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_fa</span><span class="params">(file, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> mode <span class="keyword">in</span> &#123;<span class="string">'train'</span>,<span class="string">'test'</span>&#125;</span><br><span class="line">    labels = []</span><br><span class="line">    seqs_info = []</span><br><span class="line">    cates_id = []</span><br><span class="line">    seq = <span class="string">''</span></span><br><span class="line">    <span class="keyword">with</span> open(file,mode=<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        line = f.readline().strip()</span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            <span class="keyword">if</span> line[<span class="number">0</span>]==<span class="string">'&gt;'</span>:</span><br><span class="line">                info = line[<span class="number">1</span>:].split(<span class="string">' '</span>)</span><br><span class="line">                cates_id.append(info[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">                    labels.append(<span class="string">''</span>.join(info[<span class="number">1</span>].split(<span class="string">'.'</span>)[:<span class="number">2</span>]))</span><br><span class="line">                <span class="keyword">if</span> seq:</span><br><span class="line">                    seqs_info.append(seq)</span><br><span class="line">                    seq = <span class="string">''</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                seq += line</span><br><span class="line">            line = f.readline().strip()</span><br><span class="line">        seqs_info.append(seq)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cates_id,seqs_info,labels</span><br><span class="line"></span><br><span class="line"><span class="comment">################## main.py #####################</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, fbeta_score, precision_score, recall_score, roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold <span class="keyword">as</span> KFold</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line">train_file = <span class="string">'./训练集/astral_train.fa'</span></span><br><span class="line">test_file = <span class="string">'./测试集/astral_test.fa'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_sample_id, train_seqs_info, train_labels = read_fa(train_file, mode=<span class="string">'train'</span>)</span><br><span class="line">test_sample_id, test_seqs_info, _ = read_fa(test_file, mode=<span class="string">'test'</span>)</span><br><span class="line">train_data = &#123;</span><br><span class="line">    <span class="string">'sample_id'</span>: train_sample_id,</span><br><span class="line">    <span class="string">'seq_info'</span>: train_seqs_info,</span><br><span class="line">    <span class="string">'label'</span>: train_labels</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_data = &#123;</span><br><span class="line">    <span class="string">'sample_id'</span>: test_sample_id,</span><br><span class="line">    <span class="string">'seq_info'</span>: test_seqs_info,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">label_map = &#123;l:idx <span class="keyword">for</span> idx,l <span class="keyword">in</span> enumerate(set(train_labels))&#125;</span><br><span class="line"></span><br><span class="line">rev_label_map = &#123;v:k <span class="keyword">for</span> k,v <span class="keyword">in</span> label_map.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(label_map)</span></span><br><span class="line"></span><br><span class="line">train = pd.DataFrame(data=train_data)</span><br><span class="line">test = pd.DataFrame(data=test_data)</span><br><span class="line"></span><br><span class="line">train[<span class="string">'label'</span>] = train[<span class="string">'label'</span>].map(label_map)</span><br><span class="line"></span><br><span class="line">alp = list(set(<span class="string">''</span>.join(train_seqs_info + test_seqs_info)))</span><br><span class="line"></span><br><span class="line">train[<span class="string">'seq_len'</span>] = train[<span class="string">'seq_info'</span>].apply(<span class="keyword">lambda</span> x:len(x))</span><br><span class="line">test[<span class="string">'seq_len'</span>] = test[<span class="string">'seq_info'</span>].apply(<span class="keyword">lambda</span> x:len(x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> alp:</span><br><span class="line">    train[<span class="string">'count_'</span>+s] = train[<span class="string">'seq_info'</span>].apply(<span class="keyword">lambda</span> x:x.count(s))</span><br><span class="line">    train[<span class="string">'freq_'</span>+s] = train[<span class="string">'seq_info'</span>].apply(<span class="keyword">lambda</span> x:x.count(s)/len(x))</span><br><span class="line">    </span><br><span class="line">    test[<span class="string">'count_'</span>+s] = test[<span class="string">'seq_info'</span>].apply(<span class="keyword">lambda</span> x:x.count(s))</span><br><span class="line">    test[<span class="string">'freq_'</span>+s] = test[<span class="string">'seq_info'</span>].apply(<span class="keyword">lambda</span> x:x.count(s)/len(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">feats = [i <span class="keyword">for</span> i <span class="keyword">in</span> train.columns <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'label'</span>,<span class="string">'sample_id'</span>,<span class="string">'seq_info'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(feats)</span></span><br><span class="line"></span><br><span class="line">x_train = train[feats]</span><br><span class="line">y_train = train[<span class="string">'label'</span>]</span><br><span class="line">x_test = test[feats]</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">params = &#123; </span><br><span class="line">    <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,  </span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'multiclass'</span>,  </span><br><span class="line">    <span class="string">'num_class'</span>: <span class="number">245</span>,  </span><br><span class="line">    <span class="string">'metric'</span>: <span class="string">'multi_error'</span>,  </span><br><span class="line">    <span class="string">'num_leaves'</span>: <span class="number">300</span>,  </span><br><span class="line">    <span class="string">'min_data_in_leaf'</span>: <span class="number">500</span>,  </span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.007</span>,  </span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">0.8</span>,  </span><br><span class="line">    <span class="string">'bagging_fraction'</span>: <span class="number">0.8</span>,  </span><br><span class="line">    <span class="string">'bagging_freq'</span>: <span class="number">5</span>,  </span><br><span class="line">    <span class="string">'lambda_l1'</span>: <span class="number">0.4</span>,  </span><br><span class="line">    <span class="string">'lambda_l2'</span>: <span class="number">0.5</span>,  </span><br><span class="line">    <span class="string">'min_gain_to_split'</span>: <span class="number">0.2</span>,  </span><br><span class="line">    <span class="string">'verbose'</span>: <span class="number">-1</span>,</span><br><span class="line">    <span class="string">'num_threads'</span>:<span class="number">2</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 五折交叉验证</span></span><br><span class="line">folds = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">2021</span>)</span><br><span class="line"></span><br><span class="line">oof = np.zeros([len(x_train),<span class="number">245</span>])</span><br><span class="line">predictions = np.zeros([len(x_test),<span class="number">245</span>])</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> fold_, (trn_idx, val_idx) <span class="keyword">in</span> enumerate(folds.split(x_train, y_train)):</span><br><span class="line">    print(<span class="string">"fold n°&#123;&#125;"</span>.format(fold_+<span class="number">1</span>))</span><br><span class="line">    trn_data = lgb.Dataset(x_train.iloc[trn_idx], y_train.iloc[trn_idx])</span><br><span class="line">    val_data = lgb.Dataset(x_train.iloc[val_idx], y_train.iloc[val_idx])</span><br><span class="line"> </span><br><span class="line">    num_round = <span class="number">1000</span></span><br><span class="line">    clf = lgb.train(params, </span><br><span class="line">                    trn_data, </span><br><span class="line">                    num_round, </span><br><span class="line">                    valid_sets = [trn_data, val_data], </span><br><span class="line">                    verbose_eval = <span class="number">100</span>, </span><br><span class="line">                    early_stopping_rounds = <span class="number">50</span>)</span><br><span class="line">    oof[val_idx] = clf.predict(x_train.iloc[val_idx][feats], num_iteration=clf.best_iteration)    </span><br><span class="line">    predictions += clf.predict(x_test, num_iteration=clf.best_iteration) / folds.n_splits</span><br><span class="line">    <span class="comment">#print(predictions)</span></span><br><span class="line"></span><br><span class="line">x_test[<span class="string">'sample_id'</span>] = test[<span class="string">'sample_id'</span>]</span><br><span class="line">x_test[<span class="string">'category_id'</span>] = [rev_label_map[list(x).index(max(x))] <span class="keyword">for</span> x <span class="keyword">in</span> predictions]</span><br><span class="line">x_test[<span class="string">'category_id'</span>] = x_test[<span class="string">'category_id'</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>]+<span class="string">'.'</span>+x[<span class="number">1</span>:])</span><br><span class="line">x_test[[<span class="string">'sample_id'</span>, <span class="string">'category_id'</span>]].to_csv(<span class="string">'base_sub.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">y_pre = oof.argmax(axis=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"F1 score: &#123;&#125;"</span>.format(f1_score(y_train, y_pre,average=<span class="string">'micro'</span>)))</span><br><span class="line">print(<span class="string">"Precision score: &#123;&#125;"</span>.format(precision_score(y_train, y_pre,average=<span class="string">'micro'</span>)))</span><br><span class="line">print(<span class="string">"Recall score: &#123;&#125;"</span>.format(recall_score(y_train, y_pre,average=<span class="string">'micro'</span>)))</span><br></pre></td></tr></table></figure><p>提交结果：目前【14/27(提交团队数)】</p><p><img src="/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B%E4%B9%8Blgb%E7%9A%84baseline/image-20210703193646587.png" alt="image-20210703193646587"></p><p>主要是提取了氨基酸组成(AAC)特征，即一些简单的统计特征。没有考虑氨基酸之间的相对位置信息，也没有必要调参，最后预测结果也很是拉跨。</p><p>下一步直接尝试<code>nlp</code> 相关模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;赛题：&lt;a href=&quot;https://challenge.xfyun.cn/topic/info?type=protein&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;蛋白质结构预测挑战赛&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码：&lt;/p&gt;
&lt;figure clas
      
    
    </summary>
    
    
    
      <category term="BDC" scheme="https://hahally.github.io/tags/BDC/"/>
    
  </entry>
  
  <entry>
    <title>GMAN</title>
    <link href="https://hahally.github.io/articles/GMAN/"/>
    <id>https://hahally.github.io/articles/GMAN/</id>
    <published>2021-03-02T14:27:22.000Z</published>
    <updated>2021-03-02T14:32:51.690Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/abs/1911.08415" target="_blank" rel="noopener">GMAN: A Graph Multi-Attention Network for Traffic Prediction</a></p></blockquote><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>由于交通系统的复杂性和众多影响因素的不断变化，长期交通预测具有很大的挑战性。本文针对时空因素，提出了一种基于图的多注意网络（GMAN）来预测路网图上不同位置时间步长头部的交通状况。GMAN采用编解码器架构，其中编码器和解码器均由多个时空注意模块组成，以模拟时空因素对交通状况的影响。 编码器对输入的流量特征进行编码，解码器预测输出序列。在编码器和解码器之间，应用变换注意层来转换编码的流量特征，以生成未来时间步长的序列表示作为解码器的输入。 变换注意机制对历史步骤和将来时间步骤之间的直接关系进行建模，这有助于减轻预测时间步骤之间的错误传播问题。 在两个实际交通预测任务（即交通量预测和交通速度预测）上的实验结果证明了GMAN的优越性。 特别是，在提前1小时的预测中，GMAN的MAE指标提高了4％，优于最新技术。<a href="https://github.com/zhengchuanpan/GMAN" target="_blank" rel="noopener">https://github.com/zhengchuanpan/GMAN</a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>交通预测的目的是根据历史观测（如通过传感器记录）预测道路网络中未来的交通状况（如交通量或速度）。它在许多实际应用中扮演着重要的角色。例如，准确的交通预测可以帮助运输机构更好地控制交通，以减少交通拥挤。</p><p>附近位置的交通状况的预测会相互影响。 为了捕获这种空间相关性，卷积神经网络（CNN）被广泛使用。 同时，一个地点的交通状况也与其历史观测值有关。循环神经网络（RNN）被广泛应用于这种对时间相关性问题进行建模。</p><p>由于交通条件受路网图的限制，近年来的研究将交通预测看作图形建模问题。利用图卷积网络（GCN），这些研究在短期（提前5∼15分钟）交通预测方面取得了不错的结果。然而，长期(提前几个小时)的交通预测在文献中仍然缺乏令人满意的进展，主要是由于以下挑战。</p><p>1) 复杂的时空相关性</p><ul><li>动态空间相关性。如图1所示，路网中传感器之间交通状况的相关性随着时间的推移而显著变化（例如，在高峰时间之前和期间）。如何动态选择相关传感器的数据来预测目标传感器的长期交通状况是一个具有挑战性的问题。</li><li>非线性时间相关。 同样在图1中，传感器处的交通状况可能急剧且突然地波动（例如，由于事故），从而影响不同时间步长之间的相关性。 当时间更远时，如何自适应地对非线性时间相关性建模仍然是一个挑战。</li></ul><p>2) 对误差传播的敏感性。从长期的角度来看，当对未来的预测更进一步时，每个时间步上的小误差可能会放大。这样的误差传播使得对遥远未来的预测非常具有挑战性。</p><p><img src="/articles/GMAN/image-20210223150430150.png" alt="image-20210223150430150"></p><p>为了应对上述挑战，我们提出了一种图形多注意网络（GMAN）来预测未来一段时间内道路网络图上的交通状况。此处，交通状况指的是对交通系统的观测，可以以数值形式报告 。 为了便于说明，我们将重点放在交通量和交通速度预测上，尽管我们的模型可以应用于其他数字交通数据的预测。</p><p>GMAN遵循编码器-解码器体系结构，其中编码器对输入的流量特征进行编码，而解码器预测输出序列。 在编码器和解码器之间添加了一个转换注意层，以转换编码的历史流量特征以生成将来的表示。编码器和解码器都由ST-Attention块的堆栈组成。 每个ST-Attention块由用于对动态空间相关性进行建模的空间注意机制，用于对非线性时间相关性进行建模的时间注意机制以及通过自适应融合时空表示的融合融合机制形成。变换注意力机制模型直接控制了历史和未来时间步长之间的关系，从而减轻了错误传播的影响。在两个真实数据集上的实验证实，GMAN具有最先进的性能。</p><p>这项工作的贡献概述如下:</p><ul><li>我们分别提出了空间和时间的注意机制来模拟动态的空间和非线性的时间相关。此外，我们还设计了一种自适应融合时空注意机制提取信息的门控融合方法。</li><li>提出了一种将历史交通特征转化为未来交通特征的注意力转化机制。该注意机制建立了历史时间步长与未来时间步长之间的直接关系模型，以缓解错误传播问题。</li><li>我们在两个实际流量数据集上对我们的图形多注意网络(GMAN)进行了评估，并且在1小时的预测中观察到了比最先进的基线方法提高4% 的改进和优越的容错能力。</li></ul><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="交通预测"><a href="#交通预测" class="headerlink" title="交通预测"></a>交通预测</h4><p>在过去的几十年中，交通预测得到了广泛的研究。与传统的时间序列方法（如自动回归综合移动影像匹配(ARIMA)）和机器学习模型（如支持向量回归(SVR)、 k 最近邻(KNN)）相比，深度学习方法（例如，长-短期记忆（LSTM））在捕获交通状况下的时间相关性方面表现出更优越的性能。为了建立空间相关性模型，研究人员应用卷积神经网络(CNN)来捕捉欧氏空间中的相关性。最近的研究制定了基于图的交通预测，并使用图卷积网络(GCN)建模道路网络中的非欧氏关联。这些基于图的模型通过一步一步的方法在预测前生成多个步骤，并可能受到不同预测步骤之间的误差传播的影响。</p><h4 id="图深度学习"><a href="#图深度学习" class="headerlink" title="图深度学习"></a>图深度学习</h4><p>将神经网络泛化为图结构化数据是一个新兴的话题。一系列研究概括了CNN，以在图谱或空间角度上对任意图形建模。另一类研究集中在图嵌入上，它学习保留图结构信息的顶点的低维表示将 WaveNet 集成到 GCN 中以进行时空建模。由于它学习静态邻接矩阵，因此该方法难以捕获动态空间相关性。</p><h4 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h4><p>由于注意力机制的高效性和建模依赖性的灵活性，其注意力机制已广泛应用于各个领域。注意机制的核心思想是根据输入的数据自适应地关注最相关的特征。最近，研究人员将注意力机制应用于图形结构化数据，来建模图形分类的空间相关性。我们将注意力机制扩展到图形时空数据预测。</p><h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>我们把道路网络表示为一个加权有向图 $ G=(V,E,A) $ 。这里，V 是 N = | V | 的顶点集合，代表道路网络上的点（例如交通传感器）。E 是一组边，代表顶点之间的连通性。$A \in R^{N \times N}$ 表示加权邻接矩阵。其中 $A_{v_i,v_j}$ 代表两个顶点之间的接近程度（由道路网络距离度量）。在时间步长为 t 的交通状况用图$G$ 上的图信号 $X_t \in R^{N \times C}$ 表示，其中 $C$ 是感兴趣的交通状况的数量（例如交通量，交通速度等。）</p><h4 id="Problem-Studied"><a href="#Problem-Studied" class="headerlink" title="Problem Studied"></a>Problem Studied</h4><p>给定在顶点 N 处的历史 P 时间步长 $X = (X_{t_1},X_{t_2},…,X_{t_P}) \in R^{P\times N \times C}$ ，我们的目标是预测所有顶点在下一个 Q 时间步长的交通情况，表示为 $\hat Y = (\hat X_{t_{P+1}},\hat X_{t_{P+2}},…,\hat X_{t_{P+Q}}) \in R^{Q\times N \times C}$ 。</p><h3 id="Graph-Multi-Attention-Network"><a href="#Graph-Multi-Attention-Network" class="headerlink" title="Graph Multi-Attention Network"></a>Graph Multi-Attention Network</h3><p>图2显示了我们提出的 GMAN 框架，包含一个 encoder-decoder 结构。编码器和解码器都包含 L 个残差连接的 STAtt Block。每一个 STAtt Block 都是由具有门控功能的时空注意机制组成的。在编码器与解码器之间，网络中加入了一个 <strong>transform</strong> 注意层将编码的交通特征转换为解码特征。我们还通过时空嵌入（STE）将图形结构和时间信息整合到多注意机制中。此外，为了方便残差连接，所有层输出维度都是 D。接下来将详细介绍这些模块。</p><p><img src="/articles/GMAN/image-20210301090744336.png" alt="image-20210301090744336"></p><h4 id="Spatio-Temporal-Embedding"><a href="#Spatio-Temporal-Embedding" class="headerlink" title="Spatio-Temporal Embedding"></a>Spatio-Temporal Embedding</h4><p>由于交通条件的演变受到了道路网络的限制，将道路网信息整合到预测模型中至关重要。为此，我们提出了一种将顶点编码为向量的局部嵌入方法，以保存图形结构信息。具体来说，我们利用 <strong>node2vec</strong> 方法来学习顶点表示。此外，为了协同训练预训练好的整个模型的向量，这些向量会被输入到一个两层完全连接的神经网络中。然后，我们将得到空间嵌入，表示为 $e^S_{v_i} \in R^D, v_i \in V$ 。</p><p>空间嵌入只能提供静态的表示，不能反映路网中交通传感器之间的动态相关性。因此，我们进一步提出了一种时间嵌入方法，将每个时间步编码成一个向量。具体来说，把一天分成 T 个时间步长。我们使用独热编码将每一个时间步长的 <em>day-of-week</em> 和 <em>time-of-day</em> 编码到 $R^7$ 和 $R^{T+7}$ 中，接下来，我们应用一个两层完全连接的神经网络将时间特征转化为向量。在我们的模型中，我们嵌入了历史 P 和未来 Q 时间步长的时间特征，表示为$e^T_{t_j} \in R^D$ ，其中$t_j = t_1,…,t_P,…,t_{P+Q}$ 。为了获得随时间变化的顶点表示，我们将上述空间嵌入和时间嵌入融合为时空嵌入（STE），如图2b所示。具体来说，对于时间步长 $t_j$ 时的顶点 $v_i$ ，其 STE 被定义为 $e_{v_i,t_j} = e^S_{v_i} + e^T_{t_j}$ 。因此，在 P+Q 时间步长中，N 个顶点的 STE 表示为 $E \in R^{(P+Q)\times N \times D}$ 。STE包含图形结构和时间信息，可用于空间、时间和 transform 注意机制。</p><h4 id="ST-Attention-Block"><a href="#ST-Attention-Block" class="headerlink" title="ST-Attention Block"></a>ST-Attention Block</h4><p>如图 2c 所示，ST-Attention 模块包含一个空间注意机制，一个时间注意机制和一个门控机制。我们将第 $l$ 个 block 的输入表示为 $H^{(l-1)}$ ，其中在步长$t_j$ 处顶点$v_i$ 的隐藏状态表示为 $h^{(l-1)}_{v_i,v_j}$ 。在第 $l$ 个 block 中，空间和时间注意机制的输出表示为 $H^{(l)}_S$ 和 $H^{(l)}_T$ ，其中在步长$t_j$ 处顶点$v_i$ 的隐藏状态表示为 $hs^{(l)}_{v_i,v_j},ht^{(l)}_{vi,tj}$ 。通过门控融合，得到了该模块的输出结果，表示为 $H^{(l)}$ 。</p><p>为了便于说明，我们将非线性变换表示为：</p><script type="math/tex; mode=display">f(x) = ReLU(xW+b),</script><h4 id="Spatial-Attention"><a href="#Spatial-Attention" class="headerlink" title="Spatial Attention"></a>Spatial Attention</h4><p>条道路的交通状况受到其他道路的不同影响。这种影响是动态的，随着时间的推移而变化。为了模拟这些特性，我们设计了一种空间注意机制来自适应地捕捉道路网络中传感器之间的相关性。其关键思想是在不同的时间步动态地将不同的权重分配给不同的顶点（例如，传感器），如图3所示。对于在时间步长 $t_j$ 处的顶点 $v_i$ ，我们计算所有顶点的加权和：</p><script type="math/tex; mode=display">hs^{(l)}_{v_i,v_j} = \sum_{v\in v} \alpha_{v_i,v} · h^{(l-1)}_{v,t_j} ,</script><p>表示 $V$ 所有顶点的集合， $\alpha_{v_i,v}$ 为注意机制得分，表示顶点 $v$ 到 $v_i$ 之间的重要程度，其分数之和等于 1：$\sum_{v \in V} \alpha_{v_i,v} = 1$</p><p><img src="/articles/GMAN/image-20210301104231532.png" alt="image-20210301104231532"></p><p>在一定的时间步长下，当前交通状况和路网结构都会影响传感器之间的相关性。例如，道路上的拥堵可能会严重影响其相邻道路的交通状况。基于这种直觉，我们同时考虑交通特征和图形结构来学习注意力得分。具体来说，我们将隐藏状态与时空嵌入连接起来，并采用标度点积方法计算顶点 $v_i$ 与 $V$ 之间的相关性。</p><script type="math/tex; mode=display">s_{v_i,v} = \frac{<h_{v_i,t_j}^{(l-1)}||e_{v_i,t_j},h^{(l-1)}_{v,t_j}||e_{v,t_j}>}{\sqrt{2D}},</script><p>其中 $||$ 表示连接操作，$〈•,•〉$ 表示内积操作，2D 是$h_{v_i,t_j}^{(l-1)}||e_{v_i,t_j}$ 的维度。然后，通过 Softmax 归一化为：</p><script type="math/tex; mode=display">\alpha_{v_i,v} = \frac{exp(s_{v_i,v})}{\sum_{v_r \in V}exp(s_{v_i,v_r})}</script><p>在获得注意得分 $\alpha_{v_i,v}$ 之后，可以通过等式2更新隐藏状态。</p><p>为了稳定学习过程，我们将空间注意力机制扩展为 <strong>multi-head</strong>。 具体来说，我们将K个并行注意机制与不同的可学习预测联系起来：</p><p><img src="/articles/GMAN/image-20210302084718874.png" alt="image-20210302084718874"></p><p><img src="/articles/GMAN/image-20210302084736425.png" alt="image-20210302084736425"></p><p>其中 $f^{(k)}_{s,1}(·),f^{(k)}_{s,2}(·),f^{(k)}_{s,3}(·)$ 表示在第 k 个 <strong>head attention</strong> 中三个不同的非线性预测，输出维度 $d = D/K$ 。</p><p>当顶点数量 N 很大时，由于我们需要计算 $N^2$ 个注意分数，因此时间和内存消耗都很大。 为了解决此限制，我们进一步提出了一个<em>group spatial attention</em>，其中包含了<em>intra-group spatial attention</em>和<em>inter-group spatial attention</em>，如图4所示。</p><p><img src="/articles/GMAN/image-20210302085935418.png" alt="image-20210302085935418"></p><p>我们随机将 N 个顶点划分为 G 个组，其中每个组包含 $M=N/G$ 个顶点。在每一组中，我们通过方程5、6、7 计算 <em>intra-group attention</em> 以模拟顶点之间的局部空间相关性，其中组间共享参数。然后，我们在每个组中应用最大池化方法，以获得每个组的单个表示。接下来，我们计算<em>inter-group spatial attention</em>来建模不同组之间的相关性，为每个组产生一个全局特征。局部特征被添加到相应的全局特征中，作为最终输出。</p><p>在<em>group spatial attention</em>上，我们需要计算每个时间步长的  $GM^2 + G^2 = NM+(N/M)^2$ 个 <em>attention scores</em> 。梯度为零时，我们知道 $M = \sqrt[3]{2N}$ 时 <em>attention score</em> 的值达到最小值 $2^{-1/3}N^{4/3}\ll N^2 $ 。</p><h4 id="Temporal-Attention"><a href="#Temporal-Attention" class="headerlink" title="Temporal Attention"></a>Temporal Attention</h4><p>一个地点的交通状况与它之前的观察结果是相关的，并且这种相关性随着时间步长的变化是非线性的。为了模拟这些特性，我们设计了一个 <em>Temporal Attention</em> 来自适应地模拟不同时间步骤之间的非线性关联，如图5所示。注意，时间相关性受交通条件和相应的时间背景的影响。例如，发生在早晨高峰时间的拥堵可能会影响几个小时的交通。因此，我们同时考虑流量特征和时间来度量不同时间步长之间的相关性。具体来说，我们将隐藏状态与时空嵌入相连接，并采用<em>multi-head</em>方法计算 <em>attention score</em> 。最后考虑顶点 $v_i$ ，时间步长 $t_j$ 与 $t$ 之间的相关性定义为：</p><p><img src="/articles/GMAN/image-20210302095459275.png" alt="image-20210302095459275"></p><p>其中 $u^{(k)}_{t_j,t}$ 表示时间步长 $t_j$ 与 $t$ 之间的相关性，$\beta^{(k)}_{t_j,t}$ 表示第 k 个 <em>head attention score</em> 表明时间步骤 $t$ 到$t_j$ 的重要性，$f^{(k)}_{t,1},f^{(k)}_{t,2}$ 表示两个不同的可学习的 <em>transforms</em> ，$N_{t_j}$ 表示时间步长 $t_j$ 之前的集合。仅考虑时间步中早于目标步的信息以启用因果关系(<strong>causality</strong>)。一旦获得 <em>attention score</em> 后，顶点 $v_i$ 在时间步长 $t_j$ 处 的 <em>hidden state</em> 个更新方式如下：</p><p><img src="/articles/GMAN/image-20210302204132568.png" alt="image-20210302204132568"></p><p>其中 $f^{(k)}_{t,3}$ 代表一个非线性投影。公式8、9和10中的可学习参数通过并行计算在所有顶点和时间步上共享。</p><h4 id="Gated-Fusion"><a href="#Gated-Fusion" class="headerlink" title="Gated Fusion"></a>Gated Fusion</h4><p>道路在某一特定时间步长下的交通状况与其前期值和其他道路的交通状况相关。如图2c所示，我们设计了门控融合来自适应地融合空间和时间表示。第 $l$ 个block ，时间和空间注意机制的输出表示为 $H^{(l)}_S,H^{(l)}_T$ ，两者在编码器与解码器中都有相同的形状 $R^{P\times N \times D},R^{Q\times N \times D}$ ，融合公式如下：</p><p><img src="/articles/GMAN/image-20210302205242473.png" alt="image-20210302205242473"></p><p><img src="/articles/GMAN/image-20210302205254514.png" alt="image-20210302205254514"></p><p>其中 $W_{z,1}\in R^{D\times D},W_{z,2}\in R^{D\times D},b_z \in R^D$ 是可学习参数，$\odot$ 表示按元素计算的乘积，$\sigma(·)$ 表示 sigmoid 激活函数，z 是门控。门控融合机制自适应地控制空间和时间依赖在每个顶点和时间步长的流。</p><h4 id="Transform-Attention"><a href="#Transform-Attention" class="headerlink" title="Transform Attention"></a>Transform Attention</h4><p>为了减轻长时间范围内不同预测时间步长之间的误差传播效应，在编码器和解码器之间增加了一个<em>Transform Attention</em>。它对每个未来时间步长和每个历史时间步长之间的直接关系进行建模，以转换已编码的交通特征，以生成未来表示，作为解码器的输入。如图6 所示，对顶点 $v_i$ ，通过 <em>spatio-temporal</em> 嵌入计算预测时间步长 $t_j(t_j = t_{P+1},…,t_{P+Q})$ 与历史时间步长 $t(t = t_1,..,t_P)$ 之间的相关性。</p><p><img src="/articles/GMAN/image-20210302221757111.png" alt="image-20210302221757111"></p><p>$\gamma^{(k)}_{t_j,t}$ 为 <em>attention score</em> ，通过在所有历史时间步骤中自适应地选择相关特征，将编码的流量特征转换到解码器。</p><p><img src="/articles/GMAN/image-20210302222011345.png" alt="image-20210302222011345"></p><p>方程13、14和15可以在所有顶点和时间步骤中并行计算，共享可学习的参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.08415&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GMAN: A Graph Multi-Attention Network for Traffic 
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Traffic-Network-Flow-Prediction</title>
    <link href="https://hahally.github.io/articles/Traffic-Network-Flow-Prediction/"/>
    <id>https://hahally.github.io/articles/Traffic-Network-Flow-Prediction/</id>
    <published>2021-02-22T12:24:33.000Z</published>
    <updated>2021-02-22T12:49:41.693Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://ieeexplore.ieee.org/document/9007678" target="_blank" rel="noopener">Traffic Network Flow Prediction Using ParallelTraining for Deep Convolutional NeuralNetworks on Spark Cloud</a></p></blockquote><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>道路网络中的交通流量是相互交互和相互依存的。 用分析方法描述交通网络流量的动态变化具有挑战性。 在本文中，将使用深度卷积神经网络（DCNN）模型解决交通网络流量预测问题。为DCNN模型的并行训练算法开发了理论基础。在Spark Cloud上实现了交通网络流量预测的主从并行计算解决方案。应用交通网络流量数据验证了dcnn预测模型和并行训练算法的有效性。实验结果表明，DCNN交通网络流量预测模型的预测性能优于基于BP神经网络、支持向量回归、径向基函数和决策树回归的典型预测模型。所提出的并行训练方法可以提高训练效率，并通过对数据子集的局部学习获得整个数据集的全局特征。</p><p><strong>index-item</strong> : 深度卷积神经网络(DCNNs)、并行训练、Spark云计算、交通大数据、交通网络流量预测</p><h3 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h3><p>交通流预测可以引导出行行为，从而缓解道路网络拥堵，提高出行效率，促进交通安全。准确高效的交通流预测是数据驱动智能交通系统的关键部分。</p><p>分布在道路网络上的交通流量是相互依赖，相互影响的。 交通网络流量中存在复杂的动态变化，其中道路的拥堵可能直接或间接影响网络中其他道路的交通流量。传统的路段交通流预测方法只考虑了路段的时间序列特征，忽略了路段之间的闭合关系和交互演变。交通网络流量预测模型既要反映每条道路的时间序列演变，又要反映网络流量的空间耦合关系，其中涉及到图像式的数据处理。</p><p>深度卷积神经网络(DCNN)模型与其他深度学习模型相比，在提取图像等高维数据特征方面具有优势。它已广泛应用于特征学习，语音识别和医学健康诊断。本文尝试利用 DCNN 模型提取交通网络流量的<em>时空相关动态学</em> 特征。</p><p>在处理交通网络流量的大数据进行预测时，在DCNN模型中训练大量参数非常耗时。 云计算在计算资源调度和大数据实时处理方面具有显着优势。特别是最近流行的Spark Cloud 采用了基于内存的计算和主从并行处理的高级架构，突破了某些大数据处理架构（如Hadoop）中的内存限制。此外，它还适用于复杂逻辑算法的实时科学计算。因此，我们尝试为DCNN模型开发一种并行训练方法，该方法适用于采用基于内存的计算和主从并行处理的云计算平台。本文的主要贡献如下:</p><ol><li>基于自适应梯度下降原理，为DCNN模型开发了并行训练算法的理论基础。它保证了所提出的并行训练算法能够像串行训练算法一样，通过对多个计算节点和各自的数据集进行局部学习，从而学习整个数据集的全局特征，具有良好的收敛性。</li><li>将 DCNN 模型应用于交通网络流量预测，以获取交通网络的时空数据特征。该模型首先在 Spark cloud 计算平台上实现，以解决交通网络流量大数据面临的计算复杂性问题。利用实际交通网络工作流数据验证了基于云的并行训练算法的渐近收敛性和加速优势。</li></ol><p>本文的其余部分安排如下。 第二节总结了相关的研究工作。 第三部分描述了DCNN预测模型的网络结构和训练样本。 在第四部分，我们为DCNN模型开发了并行训练算法的理论基础。 我们还开发了基于Spark Cloud 的实施解决方案。 在第五节中，验证了DCNN模型对交通网络流量预测的预测准确性和通用性。 此外，使用实际交通流数据演示了并行训练算法的收敛性和加速性能。 最后，第六节总结了本文。</p><h3 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h3><p>以往的交通流预测工作大致可分为两类: 模型驱动方法和数据驱动方法。模型驱动技术在过去几十年中被广泛应用于交通预测。例如，根据历史交通流的周期性相似性，应用自适应卡尔曼滤波方法[5]预测未来的交通流情况。然而，在这些方法中，交通流的动态波动被假定为线性。此外，它们还可以有效地进行单链路流量的时间序列预测。</p><p>数据驱动方法采用间接建模的方法来描述交通流的随机非线性特征。这些方法主要包括神经网络(NN)、支持向量回归(SVR)、径向基核函数(RBF)、决策树回归(DTR)、集成学习回归和极限学习机等。这些方法是数据挖掘的有效工具，可以从一系列数据中提取有用的信息。数据驱动方法用于确定直接模型的结构和参数。开发了一种连续的蚁群优化算法，以加速支持向量回归模型在城际高速公路交通流量预测中的参数选择。通过自适应粒子群优化确定了多层神经网络的最佳结构和参数，提高了流量变量的预测精度。然而，这些研究主要集中在道路时间序列特征的提取上。此外，用浅层结构模型提取的交通流特征也是有限的。</p><p>通过模拟人脑的多层感知结构，深度学习可以有效的从高维数据中提取特征信息。它一直被用来处理通信领域的网络流量控制和路由管理。该方法在道路交通流预测领域也得到了应用。通过对交通流数据的重构，将堆叠式编码的 <em>Levenberg-Marquardt</em> 模型和深度信息网络模型应用于挖掘短时交通流固有的时间特征。此外，结合多源数据融合开发了混合深度学习模型方法，以提高业务流预测的准确性和鲁棒性。卷积神经网络模型被用来提取交通网络速度的时间和空间特征。采用 DCNN 模型提取空间特征，采用长短期记忆模型提取交通网速时间序列特征。提出了一种针对 DCNN 模型的随机子空间学习方法，通过寻址不完全交通数据来提高交通流预测的鲁棒性。然而，利用深度学习模型进行交通网络流量预测仍然是一个难题。此外，DCNN 模型在面对大数据时的长时间训练是实时应用中一个尚未解决的问题。针对细胞神经网络模型，提出了一种混合并行训练方法，结合卷积层的数据并行性和完全连接层的模型并行性，并在 GPUs上实现。GPUs 不能灵活地扩展和高效地提供基于内存的计算资源。同时，也没有为并行训练算法的收敛性和稳定性提供理论基础。</p><p>云计算为减少训练时间提供了一种可行的解决方案。这是因为处理器、内存和存储器可以灵活地扩展和集群。近年来，apache Spark云计算平台在大数据处理方面取得了令人瞩目的成绩。采用基于内存的计算架构，由一个主计算节点和多个从计算节点组成。Spark cloud已经展示了在大数据科学计算领域应用的潜力，如贝叶斯网络分类、药物发现中的目标预测，以及使用深度学习的移动大数据分析。</p><p>本文采用与串行训练算法相同的自适应梯度下降机制，提出了一种并行训练算法。因此，所提出的算法具有理论基础，其中全局特征可以从局部学习中提取出来，即使数据集被分解并提供用于局部学习。采用园区云计算的主从结构实现了交通网络流量预测DCNN模型的并行训练算法。主节点和子节点之间只传输非常有限的数据，如学习参数、局部梯度平方、全局学习率和局部损失函数值。</p><h3 id="TRAFFIC-NETWORK-FLOW-PREDICTION-MODE"><a href="#TRAFFIC-NETWORK-FLOW-PREDICTION-MODE" class="headerlink" title="TRAFFIC NETWORK FLOW PREDICTION MODE"></a>TRAFFIC NETWORK FLOW PREDICTION MODE</h3><h4 id="A-DCNN-Prediction-Mode"><a href="#A-DCNN-Prediction-Mode" class="headerlink" title="A.   DCNN Prediction Mode"></a>A.   DCNN Prediction Mode</h4><p>DCNN模型可以捕获交通图像数据样本中丰富的时空特征。特征图中的卷积核连续滑动可以感知不同道路之间交通流的局部相关性。同时，在卷积特征图上滑动的池化窗口被用来进一步保留交通流的基本相关性并减小参数的维数。 最终，具有不同重量和偏置的完全连接的神经进一步重构了交通全局特征。</p><p>图1演示了交通网络流量的 DCNN 预测模型的结构。最左边的矩形代表一个可变的流量输入矩阵，紫色的矩形代表一个可变的内核矩阵，绿色的矩形代表一个特征映射矩阵，白色的矩形代表一个池窗口。9层网络结构包括输入层(LI)、卷积层1(Lc1)、池化层1(Lp1)、卷积层2(Lc2)、池化层2(Lp2)、三层全连接层(Lf1、 Lf2、 Lf3)和输出层(Lo)。DCNN 模型的训练目标是建立海量数据样本输入矩阵与输出向量之间的复杂非线性映射关系。在不破坏图像空间结构的前提下，输入图像可以直接输入网络。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210221234502743.png" alt="image-20210221234502743"></p><h4 id="B-Training-Sample-Construction"><a href="#B-Training-Sample-Construction" class="headerlink" title="B. Training Sample Construction"></a>B. Training Sample Construction</h4><p>交通网络结构是静态的，分布式协调流是动态的。类似于面部轮廓上的肌肉运动以形成不同的表情，流量分布和协调表示不同的流量形态。 交通网络流在空间维度上是相互依存和相互关联的。 同时，在时间维度上，每条道路的规则间隔都可能出现某些具有波动的相似交通模式。 因此，在大规模交通网络流量预测中应综合考虑交通网络流量的时空特征。</p><p>图2展示了时空训练图像样本的构建过程。 从多个传感器收集的原始数据被汇总到相应的时空序列中，以显示交通网络流量。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210221234809890.png" alt="image-20210221234809890"></p><p>一个二维时空输入矩阵构造为</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210221234959933.png" alt="image-20210221234959933"></p><p>其中，$x_{m,k}$ 表示在第k时刻路网第m条链路上的交通流量，d表示时维数据的截断长度。 输入矩阵中的行向量揭示了流在每个链路上的时间序列特征，输入矩阵中的列向量表示流在空间上的耦合特征。M 表示网络中最大的链路数。下文中，变量的下标表示该变量与第n个训练样本相关。训练样本的最大数量定义为N。 如果将输入矩阵中的每个元素都视为图像中的像素，则将大量矩阵类型的数据样本转换为一系列图像，馈入DCNN模型。 该模型提取行和列之间的时空相关性特征。</p><p>训练样本的输出表示为：</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222000325167.png" alt="image-20210222000325167"></p><p>其中 $y_{m,k+1}$ 是第 $k + 1$ 时刻在第$m$条链路上的输出业务流，$T$表示转置。 训练输入矩阵 $x_n$ 和相应的输出矢量$y_n$形成训练样本$D_n$。 </p><h3 id="PARALLEL-TRAINING-ON-SPARK-CLOUD"><a href="#PARALLEL-TRAINING-ON-SPARK-CLOUD" class="headerlink" title="PARALLEL TRAINING ON SPARK CLOUD"></a>PARALLEL TRAINING ON SPARK CLOUD</h3><h4 id="A-Problem-Statemen"><a href="#A-Problem-Statemen" class="headerlink" title="A. Problem Statemen"></a>A. Problem Statemen</h4><p>在交通大数据环境下，要优化的DCNN模型的大量参数需要进行计算密集型任务。 问题是在不降低精度的情况下提高模型参数的训练效率。一种解决方案是将总数据集分解为一些数据子集，并使用多个计算节点以并行方式针对特定于它们的数据子集训练模型参数（称为局部 学习）。 但是，必须协调仅提取数据子集的局部特征的局部学习，以获取整个数据集的全局特征。本节尝试为并行训练方法开发理论基础，并提出基于Spark cloud计算平台的并行训练方法的实现解决方案。 </p><h4 id="B-Objective-Function"><a href="#B-Objective-Function" class="headerlink" title="B. Objective Function"></a>B. Objective Function</h4><p>整个数据集D分为R个部分，每个部分定义为$D^r$（r = 1，2，…，R）。因此，数据集D可以表示为$D^{r}$的集合。其中变量中的 r 表示变量相对于第r个数据子集$D^r$，而$N^r$是第 r 个数据子集的大小。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222105827613.png" alt="image-20210222105827613"></p><p>基于(3)中提出的数据分解机制，给出了 DCNN 模型并行训练的目标函数</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222110329834.png" alt="image-20210222110329834"></p><p>DCNN 模型训练的目的是最小化目标函数(4) ，并利用 数据集 D 获得权值和偏差。这些权重和偏差称为全局学习参数。对于数据子集，最小均方误差的权值和偏差称为局部学习参数。</p><h4 id="C-Parallel-Training-Approach"><a href="#C-Parallel-Training-Approach" class="headerlink" title="C. Parallel Training Approach"></a>C. Parallel Training Approach</h4><p>并行训练方法是通过并行局部学习获得全局学习参数。这包括两个主要的训练阶段：并行特征前向学习和并行误差反向传播。</p><p>1) 并行特征正向学习：对于并行特征前向学习，基于相应的数据子集，以并行方式执行各个网络层中的所有激活函数。对于卷积层、池化层和全连通层，分别表示为  $a^{r,l}_{n,j,c}$ 、 $a^{r,l}_{n,j,p}$  、$a^{r,l}_{n,j,f}$</p><p>计算方法如下</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222111431596.png" alt="image-20210222111431596"></p><p>在 DCNN 模型中，c、 p 和 f 分别表示卷积层、池化层和全连接层；$l $表示第l层，$L$表示层数； $i $表示第 $l $层是卷积或池化层时的第 $i$个输入特征图，或第l层是完全连接层时的第 $i$ 个神经元；当第 $l$ 层是卷积或池化层时，$j$ 表示第$j$个输出特征图；当第$l$层是完全连接层时，$j$表示第$j$个神经元。$w^{r,l}_{j,i,c}$ 和 $b^{r,l}_{j,c}$ 分别是为卷积层的第 $l$ 层中的卷积核矩阵和偏差矩阵。 $N^{l-1}_p$ 表示($l-1$)层中输出特征映射的数量，该层是一个池层。$w^{r,l}_{j,i,f}$和$b^{r,l}_{j,f}$分别是为全连层的第 $l$ 层中的权值和偏置。$N^{l-1}_f$表示($l-1$)层中神经元的数量，该层是一个全连接层。 $σ( )$ 是激活函数，通常选择为整流线性单位，即 $σ(x) = max(0,x)$。* 表示卷积运算。 $H^l_p$ 和 $W^l_p$ 分别是第$l$层中池化窗口的高度和宽度。 $s^l_p$ 是最大池化操作的滑动步幅。</p><p>2) 并行误差 BP：基于传统的梯度下降原理[27]，在并行训练过程中确定了并行误差BP阶段的全局学习参数和局部学习参数之间的关系。在步骤 $t$，完全连接的层中的全局学习参数 $w^l_{j,i,f}$ 和 $b^l_{j,f}$ 更新为</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222165242874.png" alt="image-20210222165242874"></p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222165325532.png" alt="image-20210222165325532"></p><p>全连通层中的局部权重和偏差表示为</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222165411159.png" alt="image-20210222165411159"></p><p>类似地，在步骤t的卷积层中的全局学习参数 $w^l_{j,i,c}$ 和 $b^l_{j,c}$ 迭代计算为</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222165646807.png" alt="image-20210222165646807"></p><p>卷积层中的局部权重和偏差表示为</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222165723988.png" alt="image-20210222165723988"></p><p>式$(8)-(15)$表明，全局学习参数 $w^l_{j,i,c}$ ， $b^l_{j,c}$ ， $w^l_{j,i,f}$ 和 $b^l_{j,f}$ 分别是局部学习参数 $w^{r,l}_{j,i,c}$ ， $b^{r,l}_{j,c}$  和 $b^{r,l}_{j,f}$ 的平均值$(r = 1,2,…,R)$ 。</p><p>在$(8)-(15)$中，相对于数据子集$r$，迭代地计算出误差敏感性 $δ^{r,l}_{n,j,f}$ ， $δ^{r,l}_{n,j,p}$ 和 $δ^{r,l}_{n,j,c}$ ，如下所示：</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222170851525.png" alt="image-20210222170851525"></p><p>其中 $N^{l+1}_f$ 是第$(l + 1)$层中的神经元数量，该层是完全连接的层。 $N^{l+1}_c$ 是第$(l + 1)$层（卷积层）中输出特征图的数量。 $rot180(w^{r,l+1}_{\tau,j,c})$ 表示 $w^{r,l+1}_{\tau,j,c}$ 中所有元素翻转 $180°$， $up(δ^{r,l+1}_{n,j,p})$ 表示 $δ^{r,l+1}_{n,j,p}$ 的向上采样操作，以使其大小等于 $z^{r,l}_{n,j,c}$ ， $\bigodot$ 表示元素相乘。</p><p>令$θ(ξ) = {w^l_{j,i,c}, b^l_{j,c}, w^l_{j,i,f}, b^l_{j,f}}$表示从步骤 $ξ$的全局权重和偏差置换的列向量。</p><p>局部梯度表示为 $g^r(ξ) = \frac{∂J^r}{∂θ(ξ)}=\sum_{n=1}^{N^r}\frac{∂J^r_n}{∂θ(ξ)}.$ </p><p>局部梯度平方表示为 $G^r(ξ)=[g^r(ξ)]^Tg^r(ξ) .$</p><p>全局自适应学习率可由传统学习率确定，表示为</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222173540467.png" alt="image-20210222173540467"></p><p>其中ε是基本学习率，μ是极小的常数。公式$(19)$表示可以从前$(t-1)$个步骤的局部梯度平方$G^r(ξ)(ξ= 1,2,…,t-1)$的累积和中合成$η(t)$ 。</p><p>以上推导过程表明，DCNN模型的序列训练和并行训练算法都是相对于整个数据集从统一梯度下降原理出发的，也就是说，并行训练算法的收敛性类似于串行训练算法。 从理论上讲，这保证了整个数据集的全局特征都可以从局部学习中提取出来，而与各自的数据子集无关。</p><h4 id="D-Algorithm-Representation"><a href="#D-Algorithm-Representation" class="headerlink" title="D. Algorithm Representation"></a>D. Algorithm Representation</h4><p>算法1中详细说明了基于数据并行化的并行训练过程。主节点负责任务调度，资源分配，数据聚合以及从节点之间的数据分配。 同时，从节点主要承担特定的计算任务。 第1行和第2行初始化主节点和从节点。 第4行描述了主节点广播的全局参数。 第5-15行说明从属节点中的并行特征学习，第16–28行表示从属节点中的并行误差BP。 然后，从节点反馈局部学习参数，局部梯度平方和第29行的局部损失函数。主节点通过第30行聚合全局参数、学习速率和损失函数。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222174352908.png" alt="image-20210222174352908"></p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222174411353.png" alt="image-20210222174411353"></p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222174431003.png" alt="image-20210222174431003"></p><h4 id="E-Implementation-on-Spark-Cloud"><a href="#E-Implementation-on-Spark-Cloud" class="headerlink" title="E. Implementation on Spark Cloud"></a>E. Implementation on Spark Cloud</h4><p>DCNN模型的拟议并行训练方法已部署在Spark云上。 如图3所示，Spark云计算采用典型的主从结构，具有一个主节点和多个从节点。 红色箭头表示全局数据广播，而主节点和从节点之间的蓝色箭头表示本地数据收集。 资源管理器节点用于管理，调度和监视集群中节点的运行状态。数据并行性:将存储在Hadoop HDFS (distributed file system)中的流量网络流的大数据通过Spark应用程序编程接口进行分区。由此产生的交通数据分区被构造成弹性分布式数据集，并分布到相应的从节点上。并行训练：算法1中描述的并行训练过程适用于Spark云中的MapReduce编程系统。在迭代学习过程中，局部数据更新被视为Map阶段，而全局数据更新则在Reduce阶段实现。在Map阶段，所有从属节点以并行方式执行任务，尽管它们在不同的数据分区上。在Reduce阶段，主节点更新全局学习参数和学习率。然后，主节点将这些更新后的全局数据重新分配给每个从节点，作为后续迭代过程的初始值。该过程继续进行，直到达到最大迭代次数或训练精度条件。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222174921653.png" alt="image-20210222174921653"></p><p>在Spark云上并行训练交通网络流量预测模型的部署步骤如下:</p><p>1) 交通流训练数据集被提交到Spark cloud，并存储在所有slave nodes 中的HDFS中</p><p>2) 将DCNN预测模型的训练代码提交给Spark cloud</p><p>3) 计算节点信息是根据程序代码计算和配置的，程序代码包括执行器的数量、内存大小和每个执行器的CPU核数</p><p>4) 任务请求被提交给资源管理器(RM)，然后，RM 部署相应的计算节点，并在主节点和多个从节点之间建立连接</p><p>5) 主节点初始化网络结构和参数，并将模型副本分发给从节点</p><p>6) 主节点将全局学习参数和学习率分配给从节点</p><p>7) 从属节点以并行方式计算局部梯度、更新局部学习参数、梯度平方和损失函数，尽管在不同的数据分区上</p><p>8) 来自从节点的局部学习参数和梯度平方被累积以获取全局学习参数和主节点上的学习速率</p><p>9) 如果训练过程满足终止条件，则返回预测模型和预测结果。否则，转到步骤(6）</p><h3 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h3><h4 id="A-Dataset-Description"><a href="#A-Dataset-Description" class="headerlink" title="A. Dataset Description"></a>A. Dataset Description</h4><p>利用美国加州运输部交通绩效评估系统(PeMS)数据库中的路网交通流数据，通过并行训练对DCNN模型的预测性能进行评估。所需数据已从PeMS网站下载。道路网络中的日随机交通流呈现出相同的波动模式，反映了相对稳定的行驶需求和规则的交通流传播。这是一个很好的例子。可以观察到，一周中的同一天的交通流量呈现出一种与时间有关的周期性可重复性，并且受到道路的干扰。训练数据嵌入了高速公路上交通流量的时间序列波动特征及其与时空的耦合关系。 高速公路之间的交通流量。 DCNN模型用于捕获交通网络流的动态特征。</p><p>图4(a)示出加州高速公路交通网络结构，包括12条高速公路: $SR17-S,SR17-N,SR87-N,SR87-S,US101-N,US101-S,I280-N,I280-S,I680-N,I680-S,I880-N$以及$I880-S$ 。每30分钟收集一次交通数据，每5分钟收集一次来自39000多个探测器的数据，这些检测器分布在加州所有主要大城市的高速公路系统中。在本研究中，2016年的前9个月，从指定的12条高速公路收集的交通网络流量数据应用于实验。使用前8个月的数据作为训练数据集，剩余1个月的数据作为测试数据集。在Spark云计算平台上，将整个训练数据集分解为多个数据集，并分布到不同的计算节点上。通过多节点局部学习与对应数据子集的协调，学习整个数据集的全局数据特征。利用剩余的测试数据验证采用并行训练的DCNN模型是否具有提取稳定随机交通流模式的能力。将预测时间间隔定义为5、15、30、45和60min，将模型中的原始交通流数据聚合为相应的时间间隔。训练数据集和测试数据集分为输入数据和输出数据两部分。DCNN模型的输入和输出数据分别由(1)和式(2)构建。4(b)显示2016年5月9日的交通图像样本。红色区域代表拥堵，绿色区域代表交通顺畅。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222182324651.png" alt="image-20210222182324651"></p><h4 id="B-Evaluation-Indices-and-Parameter-Configuration"><a href="#B-Evaluation-Indices-and-Parameter-Configuration" class="headerlink" title="B. Evaluation Indices and Parameter Configuration"></a>B. Evaluation Indices and Parameter Configuration</h4><p>采用平均绝对误差（MAE）、平均相对误差（MRE）和均方根误差（RMSE）三个综合评价指标评价交通流预测模型的预测精度。计算方式如下：</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222182613529.png" alt="image-20210222182613529"></p><p>其中$\hat{y}^m_n$是第m个链路上的预测流量，$y^m_n$是第m个链路上的观测流量，$M$是测试数据样本的输出维度，而$N_t$是测试数据样本的数量。为了衡量路网中不同路段之间的预测精度，本文提出了一种性能指标，即预测精度指标的累积比例（CP）。以MRE索引为例进行说明。将所有链路的MREs按升序排序。最小值和最大值分别表示为$MRE^{min},MRE^{max}$。在链路上的MRE准确性的示例被定义为MREs。CP表示为</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222183518350.png" alt="image-20210222183518350"></p><p>其中$N_{MRE_S}$是$MRE_s$不大于$MRE_s$的链路数。 显然，当$MRE_s = MRE^{max}$时，$CP(MRE_s)= 1$。 CP与MRE的离散点曲线反映了随着MRE的增加，链路部分的累积分布。通过使用Spark云上不同数量的节点，加速Speedup用于评估并行训练算法的时间效率。 计算方式如下：</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222184214461.png" alt="image-20210222184214461"></p><p>其中$T_s$是具有指定数据集的单台机器上DCNN模型的训练时间，而$T_p$是使用同一数据集的并行训练的训练时间。</p><p>DCNN模型的网络结构设计和参数调整对于预测准确性至关重要。 在这项研究中，我们通过大量实验获得了针对交通网络流量预测的不同预测间隔的适当模型结构。 这些在表$I$中列出。DCNN模型的所有模拟都部署在阿里云Elastic MapReduce计算平台上。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222184809248.png" alt="image-20210222184809248"></p><h4 id="C-Prediction-Accuracy-and-Generality-Capability"><a href="#C-Prediction-Accuracy-and-Generality-Capability" class="headerlink" title="C. Prediction Accuracy and Generality Capability"></a>C. Prediction Accuracy and Generality Capability</h4><p>为了充分说明基于 DCNN 模型的交通流预测在预测精度和通用性方面的优势，我们选择了其他4种交通流预测模型作为比较对象: BP 神经网络、 RBF、 SVR 和 DTR。</p><p>如表 $II$所示，对于不同的预测区间，DCNN模型的 MAE、 MRE 和 RMSE 明显小于其他预测模型。在 MRE 指标方面，对于不同的流量网络工作流预测任务，DCNN 模型的预测准确率一般在90% 以上。具体来说，对于5min 的流量预测，DCNN模型的预测精度比 BP、 RBF、 SVR 和 DTR 模型分别提高了4.51% 、7.24% 、10.15% 和6.27% 。对于15分钟交通流量预测，DCNN模型的预测精度分别比 BP、 RBF、 SVR 和 DTR 模型分别高5.26% 、7.17% 、5.23% 和6.07% 。对于30分钟、45分钟和60分钟的交通流量预测，DCNN 模型的预测精度一般比其他模型的预测精度高3.29-9.85%。DCNN 模型的 MAE和 RMSE 平均分别比 BP、 RBF、 SVR 和 DTR 预测模型的 MAEs和 RMSE 低7.49-40.07% 和4.38-38.22% 。这些结果表明，利用 DCNN 模型从海量交通数据中分离出时空特征，可以显著提高预测精度。与 BP、 RBF、 SVR 和 DTR 模型相比，DCNN 模型具有独特的卷积过程，能够处理网络流图像数据，在不丢失空间结构信息的情况下提取时间序列特征。此外，多层网络结构可以平衡隐藏在流量大数据集中的海量信息的学习。并行训练方法能够快速、全局地学习交通网络流的基本特征。</p><p>泛化能力是衡量DCNN模型对不同交通场景适应性程度的重要指标。通过一系列实验，将DCNN模型与其它四种典型预测模型进行比较，评价了DCNN模型在不同预测区间的通用性。表$II$表明，DCNN模型的预测精度在90.03%到92.16%之间。这说明DCNN模型对不同预测区间的适应能力较强。然而，BP和SVR模型的MRE随着预测间隔的变化而迅速波动。这两个模型对预测间隔的变化很敏感，这意味着这两个模型的泛化能力很低。对于不同的预测区间，RBF和DTR模型的预测精度相对稳定。当时间间隔较大时，该模型的交通流预测精度略有下降。这是因为模型提取的有效特征会随着数据量的减少而减少。图5显示了不同预测间隔下CP与MRE的关系曲线。对于5、15、30、45和60分钟的预测间隔，超过90％，85％，60％，80％以及70％的高速公路链接上，DCNN模型的MRE预测精度超过90％。 在不同的预测间隔下，DCNN模型的CP与MRE的曲线都位于其他预测模型的CP与MRE的曲线的左上侧。 这表明不同高速公路个体之间DCNN模型的预测准确性有所提高。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222184742766.png" alt="image-20210222184742766"></p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222192313448.png" alt="image-20210222192313448"></p><h4 id="D-Convergence-and-Efficiency"><a href="#D-Convergence-and-Efficiency" class="headerlink" title="D. Convergence and Efficiency"></a>D. Convergence and Efficiency</h4><p>表$III$中显示了Spark云上单机和多节点计算环境之间的类似预测性能。 显然，使用一台机器的预测精度为90.18％，而在相同的训练时期，使用Spark云上不同数量的计算节点的精度在91.46-92.69％之间。图6（a）描述了单机训练和使用不同计算节点的并行训练的相似收敛过程。图6（b）中，从迭代步骤100开始开始，我们可以进一步观察到损失函数随着训练时间的增加而减少。 它们最终在Spark云上的单机训练和分布式并行训练中收敛到几乎相同的阈值。 在不同的计算节点之间，预测精度和收敛过程的细微差异是由训练开始时随机函数产生的初始权重和偏差引起的。 尽管如此，由于理论上有保证的训练算法，训练过程最终收敛到几乎相同的阈值。表$III$和图6说明并行训练与单机训练训练结果的收敛性一致性。这与第四节的理论分析是一致的。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222193820080.png" alt="image-20210222193820080"></p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222193559417.png" alt="image-20210222193559417"></p><p>并行计算方法取得了良好的计算效率，如图7所示。与单机计算环境相比，随着图7中左y轴的增加，Spark云上计算节点的数量增加，计算时间逐渐减少。大量的训练样本分布在多个从节点之间，可以减少计算量。 此外，Spark云计算将训练过程的中间结果捕获到内存中。 这进一步提高了迭代数据处理的计算效率。如图7所示，在右y轴的基础上，随着节点数量的增加，加速比在早期阶段近似线性地增加。 但是，当后续阶段节点数增加到一定规模时，节点间的数据传输增加了通信开销，云上的资源调度和进程管理增加了并行管理开销。 这相应地影响了加速比并使其线性度降低。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222194702280.png" alt="image-20210222194702280"></p><h4 id="E-Tempo–Spatial-Demonstration"><a href="#E-Tempo–Spatial-Demonstration" class="headerlink" title="E. Tempo–Spatial Demonstration"></a>E. Tempo–Spatial Demonstration</h4><p>采用基于DCNN的交通网络流量预测模型，获取高速公路交通流量的时间序列特征和空间耦合特征。图8展示了2016年9月6日至14日5分钟间隔交通网络流量的观测和预测结果。图8(a)描述每5分钟在高速公路上观测一次的交通流量，分别用1-12编号。图8(b)表示高速公路上每5分钟的交通流量预测结果。从空间维度上，图8中，预测结果表明，在时间-空间动态中，流量形态与实际流量极为相似。同时，图8表明交通流量的周期性变化趋势和随机波动已经被 DCNN 模型沿时间维度近似捕获。同时，图8表明交通流量的周期性变化趋势和随机波动已经被 DCNN 模型沿时间维度近似捕获。</p><p><img src="/articles/Traffic-Network-Flow-Prediction/image-20210222200609677.png" alt="image-20210222200609677"></p><p>总体而言，基于DCNN的Spark云并行训练算法的交通网络流量预测模型具有明显的优势。通过对相应数据子集中分布计算节点的局部学习，显示了良好的全局特征学习能力和学习收敛的理论基础。与DCNN模型的串行学习算法相比，在Spark云上并行训练算法的实施提高了实时参数学习效率。</p><h3 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h3><p>本文提出了一种面向交通大数据处理的基于 DCNN 模型的交通网络流量预测方法。该算法结合了参数化网络模型的并行训练算法和计算机网络模型的并行训练算法。考虑到数据分解不会削弱对交通网络流全局特征的捕获，而是有利于计算复杂度的处理，为保证并行训练算法提供了理论基础。实验结果表明，该方法在预测精度和通用性方面优于 BP、 RBF、 SVR 和 DTR 模型。该并行训练算法既能提取不同路段交通流的时间序列特征，又能提取路段间交通流的空间耦合特征。全局特征可以通过分布式数据集的局部学习以并行方式重构。提出的并行训练算法提高了交通网络流量预测的计算效率。Spark云计算平台提供了灵活的机制来扩展计算资源和能力。这使得该实现方案适用于全网流量预测。基于Spark云计算的交通网络流量预测可以帮助交通指挥中心做出及时的控制决策，引导出行者选择最优路径规避拥堵。这是留给将来研究的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/9007678&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Traffic Network Flow Prediction Using 
      
    
    </summary>
    
    
    
      <category term="paper translation" scheme="https://hahally.github.io/tags/paper-translation/"/>
    
  </entry>
  
  <entry>
    <title>九品炼丹师</title>
    <link href="https://hahally.github.io/articles/%E4%B9%9D%E5%93%81%E7%82%BC%E4%B8%B9%E5%B8%88/"/>
    <id>https://hahally.github.io/articles/%E4%B9%9D%E5%93%81%E7%82%BC%E4%B8%B9%E5%B8%88/</id>
    <published>2021-02-19T14:09:07.000Z</published>
    <updated>2021-02-19T15:03:06.773Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>前言</p></blockquote><p>第一次参加<code>cv</code> 赛事，由清华举办的一场<code>AI 挑战赛</code> , 旨在推广 <code>jittor</code>框架的吧。<a href="https://www.educoder.net/competitions/index/Jittor-2" target="_blank" rel="noopener">传送门</a></p><p>共有两个赛道，一个细分类，一个目标检测。由于有一个毕设与目标检测相关，于是毫不犹豫的报名参加了。算是入坑<code>DL</code> 了。前期在<code>tensorflow</code> 、<code>pytorch</code> 、<code>jittor</code> 三大框架之间反复横跳，最后还是抛弃了<code>tf</code>。主要是服务器上的<code>tf</code>用不了显卡的算力。环境问题懒得去倒腾了。<code>pytorch</code> 上手也很快，而且与<code>jittor</code> 相似。</p><p>选着狗细分类这个赛道试水，结果差点每淹死在水里面。查阅了许多细分类的论文，一个个提到说效果达到<code>SOTA</code> ，结果到自己手里就废了。</p><blockquote><p>在好的配方，也能被炼废。 </p></blockquote><p>拿到配方，丹炉架好，药材就绪，大力按下回车键后，看着进度条缓缓加载，epoch 1,2,3,…</p><p>这是一个漫长的过程，睡一觉第二天醒来，观察各项指标变化，没有预期那么好，却也差强人意。点击提交后，果然，依旧没有好的效果。2021.2.19，在尝试好几种配方，反复炼丹数十余日后，最终还是以失败告终。</p><blockquote><p>高端的食材往往只需要简单的烹饪。</p></blockquote><p>按照 <code>baseline</code>的方法，仅仅只是使用了一个简单的<code>resnet50</code>分类网络而已，最后的效果却要高于我各种花里胡哨的方法好几个百分点。开源的基线已是我望尘莫及的极限了。着实有些颓废。</p><blockquote><p>I know nothing but my ignorance.</p></blockquote><p>炼丹之路注定是布满荆棘的坎坷之路。才疏学浅，当厚积薄发才是。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;前言&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第一次参加&lt;code&gt;cv&lt;/code&gt; 赛事，由清华举办的一场&lt;code&gt;AI 挑战赛&lt;/code&gt; , 旨在推广 &lt;code&gt;jittor&lt;/code&gt;框架的吧。&lt;a href=&quot;https://
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>TsinghuaDogs-3</title>
    <link href="https://hahally.github.io/articles/TsinghuaDogs-3/"/>
    <id>https://hahally.github.io/articles/TsinghuaDogs-3/</id>
    <published>2021-02-09T08:43:23.000Z</published>
    <updated>2021-02-09T09:58:57.353Z</updated>
    
    <content type="html"><![CDATA[<h3 id="import-package"><a href="#import-package" class="headerlink" title="import package"></a>import package</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">print(<span class="string">'GPUs Available:'</span>,torch.cuda.is_available())</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><h3 id="build-model"><a href="#build-model" class="headerlink" title="build model"></a>build model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,n_classes)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(resnet18().conv1,</span><br><span class="line">                                      resnet18().bn1,</span><br><span class="line">                                      resnet18().relu,</span><br><span class="line">                                      resnet18().maxpool,</span><br><span class="line">                                      resnet18().layer1,</span><br><span class="line">                                      resnet18().layer2,</span><br><span class="line">                                      resnet18().layer3,</span><br><span class="line">                                      resnet18().layer4)</span><br><span class="line">        self.classifiers = nn.Sequential(nn.Linear(<span class="number">512</span>**<span class="number">2</span>, n_classes))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        feature_size = x.size(<span class="number">2</span>) * x.size(<span class="number">3</span>)</span><br><span class="line">        x = x.view(batch_size, <span class="number">512</span>, feature_size)</span><br><span class="line">        x = (torch.bmm(x, torch.transpose(x, <span class="number">1</span>, <span class="number">2</span>)) / feature_size).view(</span><br><span class="line">            batch_size, <span class="number">-1</span>)</span><br><span class="line">        x = torch.nn.functional.normalize(</span><br><span class="line">            torch.sign(x) * torch.sqrt(torch.abs(x) + <span class="number">1e-10</span>))</span><br><span class="line">        x = self.classifiers(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="load-data"><a href="#load-data" class="headerlink" title="load data"></a>load data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoadDogData</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data_dir,mode=<span class="string">'train'</span>,transforms=None)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.mode = mode.upper()</span><br><span class="line">        self.data_dir = data_dir</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span> self.mode <span class="keyword">in</span> [<span class="string">'TRAIN'</span>,<span class="string">'VALID'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(self.data_dir, self.mode + <span class="string">'_images.json'</span>), <span class="string">'r'</span>) <span class="keyword">as</span> j:</span><br><span class="line">            self.images = json.load(j)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(self.data_dir, self.mode + <span class="string">'_objects.json'</span>), <span class="string">'r'</span>) <span class="keyword">as</span> j:</span><br><span class="line">            self.objects = json.load(j)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">assert</span> len(self.images) == len(self.objects)</span><br><span class="line">        </span><br><span class="line">        self.total_len = len(self.images)</span><br><span class="line">        print(<span class="string">"[*] Loading &#123;&#125; &#123;&#125; images."</span>.format(self.mode,self.total_len))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print('./dataset/low-resolution/'+self.images[idx])</span></span><br><span class="line">        img = Image.open(<span class="string">'./dataset/low-resolution/'</span>+self.images[idx]).convert(<span class="string">'RGB'</span>)</span><br><span class="line">        objects = self.objects[idx]</span><br><span class="line">        labels = np.array(objects[<span class="string">'labels'</span>])[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transforms(img)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> img, labels</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.images)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># label map</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_label_map</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./dataset/JsonData/label_map.json'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> j:</span><br><span class="line">        labels = json.load(j)</span><br><span class="line">    <span class="keyword">return</span> labels</span><br></pre></td></tr></table></figure><h3 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">'train'</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">'valid'</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Visualize-a-few-images"><a href="#Visualize-a-few-images" class="headerlink" title="Visualize a few images"></a>Visualize a few images</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">dataset = LoadDogData(data_dir=<span class="string">'./dataset/JsonData/'</span>,</span><br><span class="line">                      mode=<span class="string">'Train'</span>,</span><br><span class="line">                      transforms=data_transforms[<span class="string">'train'</span>])</span><br><span class="line">dataloaders = torch.utils.data.DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(inp, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    inp = inp.numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line">inputs, class_names = next(iter(dataloaders))</span><br><span class="line">labels_map = get_label_map()</span><br><span class="line">rev = &#123;str(v):k <span class="keyword">for</span> k,v <span class="keyword">in</span> get_label_map().items()&#125;</span><br><span class="line"></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line">imshow(out, title=rev[str(int(class_names))])</span><br></pre></td></tr></table></figure><h3 id="train-model"><a href="#train-model" class="headerlink" title="train model"></a>train model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, dataloaders,num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.eval()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Iterate over data.</span></span><br><span class="line">            <span class="keyword">for</span> step ,(inputs, labels) <span class="keyword">in</span> enumerate(dataloaders[phase]):</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># zero the parameter gradients</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># forward</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">'train'</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">                    loss = criterion(outputs, labels.long())</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># statistics</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds == labels.data)</span><br><span class="line"><span class="comment">#                 print(type(running_loss),float(running_corrects))</span></span><br><span class="line">        </span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / len(dataloaders[phase])</span><br><span class="line">            epoch_acc = running_corrects / len(dataloaders[phase])</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># deep copy the model</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load best model weights</span></span><br><span class="line">    time_stamp = time.strftime(<span class="string">'%Y-%m-%d-%H-%M-%S'</span>,time.localtime(time.time()))</span><br><span class="line">    model.load_state_dict(best_model_wts,<span class="string">'./&#123;&#125;-best_model.pth'</span>.formator(time_stamp))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># --------------------- init parameter --------------------#</span></span><br><span class="line">    n_classes = <span class="number">131</span></span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line">    best_acc = <span class="number">0.</span></span><br><span class="line">    expochs = <span class="number">25</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ------------------------- model------------------------- #</span></span><br><span class="line">    net = Net(n_classes=n_classes)</span><br><span class="line">    net.to(device)</span><br><span class="line">    </span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line">    <span class="comment"># ------------------------ data load---------------------- #</span></span><br><span class="line">    dataset = LoadDogData(data_dir=<span class="string">'./dataset/JsonData/'</span>,</span><br><span class="line">                          mode=<span class="string">'train'</span>,</span><br><span class="line">                          transforms=data_transforms[<span class="string">'train'</span>])</span><br><span class="line">    train_loaders = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    dataset = LoadDogData(data_dir=<span class="string">'./dataset/JsonData/'</span>,</span><br><span class="line">                          mode=<span class="string">'valid'</span>,</span><br><span class="line">                          transforms=data_transforms[<span class="string">'valid'</span>])</span><br><span class="line"></span><br><span class="line">    valid_loaders = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dataloaders = &#123;</span><br><span class="line">        <span class="string">'train'</span>:train_loaders,</span><br><span class="line">        <span class="string">'val'</span>:valid_loaders</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    train_model(net, criterion, optimizer, exp_lr_scheduler,dataloaders,num_epochs=expochs)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;import-package&quot;&gt;&lt;a href=&quot;#import-package&quot; class=&quot;headerlink&quot; title=&quot;import package&quot;&gt;&lt;/a&gt;import package&lt;/h3&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
    
    
      <category term="CV" scheme="https://hahally.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>TsinghuaDogs-2</title>
    <link href="https://hahally.github.io/articles/TsinghuaDogs-2/"/>
    <id>https://hahally.github.io/articles/TsinghuaDogs-2/</id>
    <published>2021-02-08T10:10:40.000Z</published>
    <updated>2021-02-08T10:16:14.267Z</updated>
    
    <content type="html"><![CDATA[<h3 id="import-package"><a href="#import-package" class="headerlink" title="import package"></a>import package</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets, layers, Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.applications.vgg16 <span class="keyword">import</span> VGG16, preprocess_input, decode_predictions</span><br><span class="line"></span><br><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Tensorflow Version:"</span>, tf.__version__)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Num GPUs Available: "</span>,</span><br><span class="line">      len(tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>)))</span><br></pre></td></tr></table></figure><h3 id="load-dataset"><a href="#load-dataset" class="headerlink" title="load dataset"></a>load dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoadData</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data_dir,mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.mode = mode.upper()</span><br><span class="line">        self.data_dir = data_dir</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span> self.mode <span class="keyword">in</span> [<span class="string">'TRAIN'</span>,<span class="string">'VALID'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(self.data_dir, self.mode + <span class="string">'_images.json'</span>), <span class="string">'r'</span>) <span class="keyword">as</span> j:</span><br><span class="line">            self.images = json.load(j)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(self.data_dir, self.mode + <span class="string">'_objects.json'</span>), <span class="string">'r'</span>) <span class="keyword">as</span> j:</span><br><span class="line">            self.objects = json.load(j)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">assert</span> len(self.images) == len(self.objects)</span><br><span class="line"></span><br><span class="line">        self.total_len = len(self.images)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"[*] Loading &#123;&#125; &#123;&#125; images."</span>.format(self.mode,self.total_len))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#         image = cv2.imread('./dataset/low-resolution/'+self.images[idx])</span></span><br><span class="line"><span class="comment">#         image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB).astype("float32")</span></span><br><span class="line">        image = self.images[idx]</span><br><span class="line">        objects = self.objects[idx]</span><br><span class="line">        </span><br><span class="line">        labels = np.array(objects[<span class="string">'labels'</span>]) - <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> image, labels</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        labels = [ np.array(item[<span class="string">'labels'</span>]) - <span class="number">1.</span> <span class="keyword">for</span> item <span class="keyword">in</span> self.objects]</span><br><span class="line">        </span><br><span class="line">        labels = np.array(labels).reshape([<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        images = self.images</span><br><span class="line">        <span class="keyword">return</span> images,labels</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.images)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(data_dir,mode=<span class="string">'train'</span>,n_mode=<span class="number">100</span>,n_classes)</span>:</span></span><br><span class="line">    mode = mode.upper()</span><br><span class="line">    <span class="keyword">assert</span> mode <span class="keyword">in</span> [<span class="string">'TRAIN'</span>,<span class="string">'VALID'</span>]</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(data_dir, mode + <span class="string">'_images.json'</span>), <span class="string">'r'</span>) <span class="keyword">as</span> j:</span><br><span class="line">        images = json.load(j)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(data_dir, mode + <span class="string">'_objects.json'</span>), <span class="string">'r'</span>) <span class="keyword">as</span> j:</span><br><span class="line">        objects = json.load(j)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> len(images) == len(objects)</span><br><span class="line"></span><br><span class="line">    labels = []</span><br><span class="line">    imgs = []</span><br><span class="line">    count = &#123;str(i):<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n_classes)&#125;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> images:</span><br><span class="line">        label = int(item.split(<span class="string">'-'</span>)[<span class="number">1</span>][<span class="number">-3</span>:]) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> label&lt;n_classes:</span><br><span class="line">            <span class="keyword">if</span> count[str(label)]&gt;=n_mode:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            count[str(label)] += <span class="number">1</span></span><br><span class="line">            labels.append(label)</span><br><span class="line">            imgs.append(item)</span><br><span class="line">            </span><br><span class="line">    print(<span class="string">"[*] Loading &#123;&#125; &#123;&#125; images."</span>.format(mode,len(imgs)))</span><br><span class="line">    <span class="keyword">return</span> imgs,labels</span><br></pre></td></tr></table></figure><h3 id="images-precess"><a href="#images-precess" class="headerlink" title="images precess"></a>images precess</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 标准化</span></span><br><span class="line">    <span class="comment"># x: [224, 224, 3]</span></span><br><span class="line">    <span class="comment"># mean: [224, 224, 3], std: [3]</span></span><br><span class="line">    img_mean = tf.constant([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    img_std = tf.constant([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    </span><br><span class="line">    x = (x - img_mean)/img_std</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    con = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> x:</span><br><span class="line">        item = item.numpy().decode()</span><br><span class="line"></span><br><span class="line">        img = plt.imread(<span class="string">'./dataset/low-resolution/'</span>+item)</span><br><span class="line">        img = cv2.resize(img,(<span class="number">244</span>,<span class="number">244</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 图像增强</span></span><br><span class="line">        img = tf.image.random_flip_left_right(img) <span class="comment"># 左右镜像</span></span><br><span class="line">        img= tf.image.random_crop(img, [<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>]) <span class="comment"># 随机裁剪</span></span><br><span class="line">        img = tf.cast(img, dtype=tf.float32) / <span class="number">255.</span></span><br><span class="line">        img = normalize(img) <span class="comment"># 标准化</span></span><br><span class="line">        </span><br><span class="line">        con.append(tf.expand_dims(img,axis=<span class="number">0</span>))</span><br><span class="line">       </span><br><span class="line">    images = tf.concat(con,axis=<span class="number">0</span>)</span><br><span class="line">    labels = tf.convert_to_tensor(y) <span class="comment"># 转换成张量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> images,labels</span><br></pre></td></tr></table></figure><h3 id="build-model"><a href="#build-model" class="headerlink" title="build model"></a>build model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BCNN</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_classes)</span>:</span></span><br><span class="line">        super(BCNN,self).__init__()</span><br><span class="line"><span class="comment">#         self.resnet50 = ResNet50(weights='imagenet',</span></span><br><span class="line"><span class="comment">#                                  pooling=None,</span></span><br><span class="line"><span class="comment">#                                  include_top=False,</span></span><br><span class="line"><span class="comment">#                                  input_shape=(224, 224, 3))</span></span><br><span class="line">        self.vgg16 = VGG16(weights=<span class="string">'imagenet'</span>,</span><br><span class="line">                           pooling=<span class="literal">None</span>,</span><br><span class="line">                           include_top=<span class="literal">False</span>,</span><br><span class="line">                           input_shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        self.model_vgg = Model(inputs=self.vgg16.input,</span><br><span class="line">                               outputs=self.vgg16.get_layer(<span class="string">'block5_conv3'</span>).output)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         self.model_resnet = Model(inputs=ResNet50.input, </span></span><br><span class="line"><span class="comment">#                                   outputs=ResNet50.get_layer('block5_conv3').output)</span></span><br><span class="line">        </span><br><span class="line">        self.flatten = layers.Flatten()</span><br><span class="line">        self.fc = layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, images)</span>:</span></span><br><span class="line">        </span><br><span class="line">        feat = self.model_vgg(images)</span><br><span class="line">        </span><br><span class="line">        feat_size = feat.shape[<span class="number">1</span>]*feat.shape[<span class="number">2</span>]</span><br><span class="line"><span class="comment">#         feat2 = self.model_resnet(images)</span></span><br><span class="line">        <span class="comment"># feat.shape:(d,w,h,c)</span></span><br><span class="line">        x = tf.reshape(feat,(feat.shape[<span class="number">0</span>],feat_size,feat.shape[<span class="number">3</span>]))</span><br><span class="line">        </span><br><span class="line">        out = tf.matmul(x,tf.transpose(x,perm=[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]))/feat_size</span><br><span class="line">        out = tf.reshape(out,(out.shape[<span class="number">0</span>],<span class="number">-1</span>))</span><br><span class="line">        out = tf.sign(out)*tf.sqrt(tf.abs(out)+<span class="number">1e-10</span>)</span><br><span class="line">        </span><br><span class="line">        out = tf.nn.l2_normalize(out,axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        out = self.flatten(out)</span><br><span class="line">        </span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="run-code"><a href="#run-code" class="headerlink" title="run code"></a>run code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        logits = model(x, training=<span class="literal">True</span>)</span><br><span class="line">        loss_value = loss_fn(y, logits)</span><br><span class="line">    grads = tape.gradient(loss_value, model.trainable_weights)</span><br><span class="line">    optimizer.apply_gradients(zip(grads, model.trainable_weights))</span><br><span class="line">    train_acc_metric.update_state(y, logits)</span><br><span class="line">    <span class="keyword">return</span> loss_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_step</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    val_logits = model(x, training=<span class="literal">False</span>)</span><br><span class="line">    val_acc_metric.update_state(y, val_logits)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------config---------------------------------#</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">num_classes = <span class="number">130</span></span><br><span class="line">save_model_dir = <span class="string">'./modelfiles/'</span></span><br><span class="line"></span><br><span class="line">images, labels = get_data(<span class="string">'./dataset/JsonData/'</span>, mode=<span class="string">'train'</span>, n_mode=<span class="number">200</span>)</span><br><span class="line">trian_db = tf.data.Dataset.from_tensor_slices((images, labels))</span><br><span class="line">trian_db = trian_db.shuffle(<span class="number">50</span>).batch(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">images, labels = get_data(<span class="string">'./dataset/JsonData/'</span>, mode=<span class="string">'valid'</span>, n_mode=<span class="number">20</span>)</span><br><span class="line">valid_db = tf.data.Dataset.from_tensor_slices((images, labels))</span><br><span class="line">valid_db = valid_db.shuffle(<span class="number">50</span>).batch(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line">train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">model = BCNN(num_classes=num_classes)</span><br><span class="line">model.build(input_shape=(<span class="number">16</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------------------------- #</span></span><br><span class="line"></span><br><span class="line">best_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    print(<span class="string">"\nStart of epoch %d"</span> % (epoch, ))</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate over the batches of the dataset.</span></span><br><span class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(trian_db):</span><br><span class="line">        x_batch_train, y_batch_train = preprocess(x, y)</span><br><span class="line">        loss_value = train_step(x_batch_train, y_batch_train)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Log every 200 batches.</span></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch:'</span>, epoch, <span class="string">' Step:'</span>, step, <span class="string">'Training loss:'</span>,</span><br><span class="line">                  float(loss_value))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display metrics at the end of each epoch.</span></span><br><span class="line">    train_acc = train_acc_metric.result()</span><br><span class="line">    print(<span class="string">"Training acc over epoch: %.4f"</span> % (float(train_acc), ))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reset training metrics at the end of each epoch</span></span><br><span class="line">    train_acc_metric.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run a validation loop at the end of each epoch.</span></span><br><span class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(valid_db):</span><br><span class="line">        x_batch_val, y_batch_val = preprocess(x, y)</span><br><span class="line">        test_step(x_batch_val, y_batch_val)</span><br><span class="line"></span><br><span class="line">    val_acc = val_acc_metric.result()</span><br><span class="line">    val_acc_metric.reset_states()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Validation acc: %.4f"</span> % (float(val_acc), ))</span><br><span class="line">    print(<span class="string">"Time taken: %.2fs"</span> % (time.time() - start_time))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> best_acc &lt; val_acc:</span><br><span class="line">        best_acc = val_acc</span><br><span class="line">        print(<span class="string">'Valid in epoch'</span>, epoch, <span class="string">'Accuracy is'</span>, val_acc,</span><br><span class="line">              <span class="string">'Best accuracy is'</span>, best_acc)</span><br><span class="line"></span><br><span class="line">        time_stamp = time.strftime(<span class="string">'%Y-%m-%d-%H-%M-%S'</span>,</span><br><span class="line">                                   time.localtime(time.time()))</span><br><span class="line">        model.save_weights(</span><br><span class="line">            os.path.join(save_model_dirav, time_stamp + <span class="string">'.checkpoint'</span>))</span><br><span class="line">        print(<span class="string">'saved total model weights; timestamp:'</span>, time_stamp)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;import-package&quot;&gt;&lt;a href=&quot;#import-package&quot; class=&quot;headerlink&quot; title=&quot;import package&quot;&gt;&lt;/a&gt;import package&lt;/h3&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
    
    
      <category term="CV" scheme="https://hahally.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>A09</title>
    <link href="https://hahally.github.io/articles/A09/"/>
    <id>https://hahally.github.io/articles/A09/</id>
    <published>2021-02-03T15:59:00.000Z</published>
    <updated>2021-02-03T16:04:12.781Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h3><p>来源：第十二届服创大赛A类数据<a href="http://www.fwwb.org.cn/topic/show/6c22faa0-c905-4482-a0bc-3dfe11657565" target="_blank" rel="noopener">【A09】轨道交通智慧客流分析预测【八维通】</a></p><p>示例数据包含:</p><ul><li>行程数据<code>trips.csv</code></li><li>用户数据<code>users.csv</code></li><li>站点名称数据<code>station.csv</code></li><li>2020年全年节假日数据<code>workdays.csv</code></li></ul><hr><p>trips.csv：</p><div class="table-container"><table><thead><tr><th>说明</th><th>类型说明</th></tr></thead><tbody><tr><td>用户id</td><td>与user.csv 表中用户id进行关联，可以获取用户对应的行程</td></tr><tr><td>进站名</td><td></td></tr><tr><td>进站时间</td><td>YYYY-MM-DD hh:mm:ss</td></tr><tr><td>出站</td><td></td></tr><tr><td>出站时间</td><td>YYYY-MM-DD hh:mm:ss</td></tr><tr><td>渠道类型</td><td>本例子中不使用</td></tr><tr><td>票价</td><td>单位分</td></tr></tbody></table></div><hr><p>users.csv:</p><div class="table-container"><table><thead><tr><th>说明</th><th>类型说明</th></tr></thead><tbody><tr><td>说明</td><td>类型说明</td></tr><tr><td>用户id</td><td>和trip.csv里面的用户ID关联可获取用户对应的行程</td></tr><tr><td>省市</td><td></td></tr><tr><td>出生年份</td><td></td></tr><tr><td>性别</td><td>0 男，1 女</td></tr></tbody></table></div><hr><p>station.csv:</p><div class="table-container"><table><thead><tr><th>说明</th><th>类型说明</th></tr></thead><tbody><tr><td>站点id</td><td>无业务意义</td></tr><tr><td>站点名</td><td></td></tr><tr><td>线路名</td><td></td></tr><tr><td>站点所在区</td></tr></tbody></table></div><hr><p>workdays.csv:</p><div class="table-container"><table><thead><tr><th>说明</th><th>类型</th><th>类型说明</th></tr></thead><tbody><tr><td>日期</td><td>string</td><td>YYYYMMDD</td></tr><tr><td>节假日属性</td><td>枚举</td><td>1，2，或3</td></tr></tbody></table></div><p>1为工作日（包括周末调休变成的工作日）；2为正常周末；3为节假日，包括法定节假日，以及与法定节假日相连的周末。</p><hr><h3 id="导入相关库"><a href="#导入相关库" class="headerlink" title="导入相关库"></a>导入相关库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#ignore warnings</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure><h3 id="trips-csv"><a href="#trips-csv" class="headerlink" title="trips.csv"></a>trips.csv</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trips = pd.read_csv(<span class="string">'./trips.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本信息</span></span><br><span class="line">trips.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 791995 entries, 0 to 791994Data columns (total 7 columns):用户ID    791995 non-null object进站名称    791995 non-null object进站时间    791995 non-null object出站名称    791995 non-null object出站时间    791995 non-null object渠道编号    791995 non-null int64价格      791995 non-null int64dtypes: int64(2), object(5)memory usage: 42.3+ MB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 浏览数据集</span></span><br><span class="line">trips.sample(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>用户ID</th>      <th>进站名称</th>      <th>进站时间</th>      <th>出站名称</th>      <th>出站时间</th>      <th>渠道编号</th>      <th>价格</th>    </tr>  </thead>  <tbody>    <tr>      <th>361317</th>      <td>309d068b9686b17c2eea0f7396d6a3ce</td>      <td>Sta34</td>      <td>2020-06-14 11:59:47</td>      <td>Sta108</td>      <td>2020-06-14 12:27:01</td>      <td>2</td>      <td>400</td>    </tr>    <tr>      <th>369815</th>      <td>5f60d49862e08647d83cba6b88826a7c</td>      <td>Sta163</td>      <td>2020-04-11 09:45:12</td>      <td>Sta161</td>      <td>2020-04-11 10:21:50</td>      <td>2</td>      <td>400</td>    </tr>    <tr>      <th>24683</th>      <td>456baf61774e89b0f0ecd66561dc1ec3</td>      <td>Sta134</td>      <td>2020-03-09 19:17:07</td>      <td>Sta135</td>      <td>2020-03-09 19:24:20</td>      <td>3</td>      <td>200</td>    </tr>    <tr>      <th>265739</th>      <td>8259141547ffe7508d9ed0640064222e</td>      <td>Sta63</td>      <td>2020-06-19 09:39:06</td>      <td>Sta151</td>      <td>2020-06-19 10:09:06</td>      <td>3</td>      <td>400</td>    </tr>    <tr>      <th>360005</th>      <td>8e630d272f05413355bd4054ec57e1e4</td>      <td>Sta46</td>      <td>2020-03-22 14:30:38</td>      <td>Sta55</td>      <td>2020-03-22 15:05:00</td>      <td>3</td>      <td>400</td>    </tr>    <tr>      <th>302739</th>      <td>93cdc65de998cdfa253308f9e7a722fa</td>      <td>Sta49</td>      <td>2020-01-15 20:08:53</td>      <td>Sta89</td>      <td>2020-01-15 20:38:15</td>      <td>3</td>      <td>500</td>    </tr>    <tr>      <th>577747</th>      <td>7829bcf454c66b5bac54b4592f30d1ff</td>      <td>Sta137</td>      <td>2020-07-03 08:09:22</td>      <td>Sta37</td>      <td>2020-07-03 08:41:17</td>      <td>2</td>      <td>400</td>    </tr>    <tr>      <th>449944</th>      <td>e45e528ef8575a550847f5b33b732207</td>      <td>Sta67</td>      <td>2020-06-01 07:52:00</td>      <td>Sta100</td>      <td>2020-06-01 08:21:47</td>      <td>3</td>      <td>400</td>    </tr>    <tr>      <th>317479</th>      <td>47750740c3ff91bffaac1b92e7d8ac2f</td>      <td>Sta25</td>      <td>2020-06-15 09:44:47</td>      <td>Sta63</td>      <td>2020-06-15 10:03:20</td>      <td>2</td>      <td>300</td>    </tr>    <tr>      <th>430358</th>      <td>9fd1d4dae1cbed06dec272607911ed02</td>      <td>Sta144</td>      <td>2020-06-15 07:56:18</td>      <td>Sta20</td>      <td>2020-06-15 08:27:03</td>      <td>2</td>      <td>400</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除渠道编号</span></span><br><span class="line">trips = trips.drop(labels=<span class="string">'渠道编号'</span>,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trips.head()</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>用户ID</th>      <th>进站名称</th>      <th>进站时间</th>      <th>出站名称</th>      <th>出站时间</th>      <th>价格</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>d4ec5a712f2b24ce226970a8d315dfce</td>      <td>Sta18</td>      <td>2020-07-15 14:21:58</td>      <td>Sta9</td>      <td>2020-07-15 14:39:29</td>      <td>200</td>    </tr>    <tr>      <th>1</th>      <td>328266c5e7a8fccb976029683f723725</td>      <td>Sta74</td>      <td>2020-07-15 19:40:46</td>      <td>Sta133</td>      <td>2020-07-15 20:49:00</td>      <td>700</td>    </tr>    <tr>      <th>2</th>      <td>4ff0e057af749864432d223ae0d0fa49</td>      <td>Sta69</td>      <td>2020-07-15 20:02:22</td>      <td>Sta96</td>      <td>2020-07-15 20:17:04</td>      <td>300</td>    </tr>    <tr>      <th>3</th>      <td>59add5aed8f0d030858c01a78c9c79fe</td>      <td>Sta110</td>      <td>2020-07-15 16:09:38</td>      <td>Sta123</td>      <td>2020-07-15 16:39:44</td>      <td>400</td>    </tr>    <tr>      <th>4</th>      <td>c285b7d3d347f4a1f46c826307bd1b8a</td>      <td>Sta36</td>      <td>2020-07-15 15:20:39</td>      <td>Sta28</td>      <td>2020-07-15 15:40:20</td>      <td>400</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用户id</span></span><br><span class="line"></span><br><span class="line">id_count = trips.groupby(by=[<span class="string">'用户ID'</span>]).agg(&#123;<span class="string">'用户ID'</span>:[<span class="string">'count'</span>]&#125;).reset_index()</span><br><span class="line">id_count[<span class="string">'用户ID'</span>].describe()</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>count</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>83049.000000</td>    </tr>    <tr>      <th>mean</th>      <td>9.536478</td>    </tr>    <tr>      <th>std</th>      <td>20.346030</td>    </tr>    <tr>      <th>min</th>      <td>1.000000</td>    </tr>    <tr>      <th>25%</th>      <td>1.000000</td>    </tr>    <tr>      <th>50%</th>      <td>3.000000</td>    </tr>    <tr>      <th>75%</th>      <td>8.000000</td>    </tr>    <tr>      <th>max</th>      <td>322.000000</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 站点</span></span><br><span class="line">len(set(trips[<span class="string">'进站名称'</span>])),len(set(trips[<span class="string">'出站名称'</span>]))</span><br></pre></td></tr></table></figure><pre><code>(168, 168)</code></pre><h3 id="users-csv"><a href="#users-csv" class="headerlink" title="users.csv"></a>users.csv</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">users = pd.read_csv(<span class="string">'./users.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">users.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 124782 entries, 0 to 124781Data columns (total 4 columns):用户ID    124782 non-null object区域      124782 non-null int64出生年份    124782 non-null int64性别      124782 non-null int64dtypes: int64(3), object(1)memory usage: 3.8+ MB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">users.sample(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>用户ID</th>      <th>区域</th>      <th>出生年份</th>      <th>性别</th>    </tr>  </thead>  <tbody>    <tr>      <th>124450</th>      <td>659eb8d7aaff22d8898cbab2ce901eb9</td>      <td>5224</td>      <td>1988</td>      <td>1</td>    </tr>    <tr>      <th>49769</th>      <td>ab67516f36608867726bea171cd47af1</td>      <td>5113</td>      <td>1996</td>      <td>1</td>    </tr>    <tr>      <th>95642</th>      <td>02dafe7577cba98cd85edf8b7b206e3f</td>      <td>5107</td>      <td>1993</td>      <td>1</td>    </tr>    <tr>      <th>30679</th>      <td>691fba5ba30776c4d94ecb9f850e9628</td>      <td>5001</td>      <td>1985</td>      <td>1</td>    </tr>    <tr>      <th>80127</th>      <td>f29fd257eb729a263900184e4a4177b4</td>      <td>5002</td>      <td>1985</td>      <td>1</td>    </tr>    <tr>      <th>43695</th>      <td>616b3fb46a82b41269cd9833d69d9847</td>      <td>3212</td>      <td>1975</td>      <td>1</td>    </tr>    <tr>      <th>118850</th>      <td>914962a14711cee5b7d916711c22b4fd</td>      <td>1201</td>      <td>1967</td>      <td>0</td>    </tr>    <tr>      <th>35179</th>      <td>d215e6be7597de23b94b79c577aa7569</td>      <td>5002</td>      <td>1993</td>      <td>1</td>    </tr>    <tr>      <th>37903</th>      <td>7634c54e4e8c1bb968ead3da0a38fc7b</td>      <td>5002</td>      <td>1991</td>      <td>0</td>    </tr>    <tr>      <th>91696</th>      <td>2e3134b064a1a082f0f204dac0cb9899</td>      <td>5002</td>      <td>1988</td>      <td>1</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 男女比例</span></span><br><span class="line">users[<span class="string">'性别'</span>].value_counts()/users.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>1    0.5865350    0.413465Name: 性别, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 年龄</span></span><br><span class="line">users[<span class="string">'age'</span>] = <span class="number">2020</span> - users[<span class="string">'出生年份'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">users.head()</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>用户ID</th>      <th>区域</th>      <th>出生年份</th>      <th>性别</th>      <th>age</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>4bd084423e63c9bcf3dd66761506d8cf</td>      <td>4115</td>      <td>1996</td>      <td>0</td>      <td>24</td>    </tr>    <tr>      <th>1</th>      <td>6ce72d2bccd92862119d86713dd4e6fa</td>      <td>3623</td>      <td>1980</td>      <td>1</td>      <td>40</td>    </tr>    <tr>      <th>2</th>      <td>adc40e61672136ba20fc41ed3736afe1</td>      <td>5001</td>      <td>1996</td>      <td>1</td>      <td>24</td>    </tr>    <tr>      <th>3</th>      <td>16daef5eb951b4132fdbeb5ede7de172</td>      <td>3706</td>      <td>1988</td>      <td>0</td>      <td>32</td>    </tr>    <tr>      <th>4</th>      <td>5e7c6803e0c584f444b20a96104628cb</td>      <td>3301</td>      <td>1993</td>      <td>1</td>      <td>27</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">users.describe()</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>区域</th>      <th>出生年份</th>      <th>性别</th>      <th>age</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>124782.000000</td>      <td>124782.000000</td>      <td>124782.000000</td>      <td>124782.000000</td>    </tr>    <tr>      <th>mean</th>      <td>4711.264613</td>      <td>1989.136294</td>      <td>0.586535</td>      <td>30.863706</td>    </tr>    <tr>      <th>std</th>      <td>918.747877</td>      <td>9.986578</td>      <td>0.492457</td>      <td>9.986578</td>    </tr>    <tr>      <th>min</th>      <td>1101.000000</td>      <td>1930.000000</td>      <td>0.000000</td>      <td>10.000000</td>    </tr>    <tr>      <th>25%</th>      <td>5001.000000</td>      <td>1983.000000</td>      <td>0.000000</td>      <td>23.000000</td>    </tr>    <tr>      <th>50%</th>      <td>5002.000000</td>      <td>1992.000000</td>      <td>1.000000</td>      <td>28.000000</td>    </tr>    <tr>      <th>75%</th>      <td>5108.000000</td>      <td>1997.000000</td>      <td>1.000000</td>      <td>37.000000</td>    </tr>    <tr>      <th>max</th>      <td>6543.000000</td>      <td>2010.000000</td>      <td>1.000000</td>      <td>90.000000</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 省市区域</span></span><br><span class="line">len(set(users[<span class="string">'区域'</span>]))</span><br></pre></td></tr></table></figure><pre><code>450</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">users[<span class="string">'区域'</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>5002    284695001    149045102    121175003     54835116     3713        ...  3427        15121        14225        16208        16405        1Name: 区域, Length: 450, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># top 20</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">labels = users[<span class="string">'区域'</span>].value_counts().index[:<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line">y_pos = np.arange(len(labels))</span><br><span class="line"></span><br><span class="line">performance = users[<span class="string">'区域'</span>].value_counts().values[:<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line">error = np.random.rand(len(labels))</span><br><span class="line"></span><br><span class="line">ax.barh(y_pos, performance, xerr=error, align=<span class="string">'center'</span>)</span><br><span class="line">ax.set_yticks(y_pos)</span><br><span class="line">ax.set_yticklabels(labels)</span><br><span class="line">ax.invert_yaxis()  <span class="comment"># labels read top-to-bottom</span></span><br><span class="line">ax.set_xlabel(<span class="string">'Counts'</span>)</span><br><span class="line">ax.set_title(<span class="string">'catagory distribution'</span>);<span class="comment"># 加分号只输出图像</span></span><br></pre></td></tr></table></figure><p><img src="/articles/A09/output_21_0.png" alt="png"></p><h3 id="station-csv"><a href="#station-csv" class="headerlink" title="station.csv"></a>station.csv</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">station = pd.read_csv(<span class="string">'./station.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">station.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 163 entries, 0 to 162Data columns (total 4 columns):编号      163 non-null int64站点名称    163 non-null object线路      163 non-null object行政区域    163 non-null objectdtypes: int64(1), object(3)memory usage: 5.2+ KB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">station.sample(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>编号</th>      <th>站点名称</th>      <th>线路</th>      <th>行政区域</th>    </tr>  </thead>  <tbody>    <tr>      <th>138</th>      <td>1155</td>      <td>Sta145</td>      <td>10号线</td>      <td>Dist5</td>    </tr>    <tr>      <th>151</th>      <td>1172</td>      <td>Sta60</td>      <td>12号线</td>      <td>Dist2</td>    </tr>    <tr>      <th>16</th>      <td>1022</td>      <td>Sta149</td>      <td>1号线</td>      <td>Dist3</td>    </tr>    <tr>      <th>103</th>      <td>1118</td>      <td>Sta118</td>      <td>11号线</td>      <td>Dist5</td>    </tr>    <tr>      <th>118</th>      <td>1134</td>      <td>Sta152</td>      <td>11号线</td>      <td>Dist1</td>    </tr>    <tr>      <th>158</th>      <td>1185</td>      <td>Sta114</td>      <td>10号线</td>      <td>Dist5</td>    </tr>    <tr>      <th>9</th>      <td>1015</td>      <td>Sta80</td>      <td>1号线</td>      <td>Dist3</td>    </tr>    <tr>      <th>82</th>      <td>1094</td>      <td>Sta62</td>      <td>4号线</td>      <td>Dist8</td>    </tr>    <tr>      <th>96</th>      <td>1110</td>      <td>Sta146</td>      <td>11号线</td>      <td>Dist6</td>    </tr>    <tr>      <th>60</th>      <td>1069</td>      <td>Sta39</td>      <td>3号线</td>      <td>Dist5</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(set(station[<span class="string">'行政区域'</span>])),len(set(station[<span class="string">'站点名称'</span>])),len(set(station[<span class="string">'编号'</span>]))</span><br></pre></td></tr></table></figure><pre><code>(9, 163, 163)</code></pre><h3 id="workdays-csv"><a href="#workdays-csv" class="headerlink" title="workdays.csv"></a>workdays.csv</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wd = pd.read_csv(<span class="string">'./workdays2020.csv'</span>,header=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wd.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 366 entries, 0 to 365Data columns (total 2 columns):0    366 non-null int641    366 non-null objectdtypes: int64(1), object(1)memory usage: 5.8+ KB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wd.sample(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>    </tr>  </thead>  <tbody>    <tr>      <th>349</th>      <td>20201215</td>      <td>1</td>    </tr>    <tr>      <th>220</th>      <td>20200808</td>      <td>2</td>    </tr>    <tr>      <th>326</th>      <td>20201122</td>      <td>2</td>    </tr>    <tr>      <th>106</th>      <td>20200416</td>      <td>1</td>    </tr>    <tr>      <th>281</th>      <td>20201008</td>      <td>1</td>    </tr>  </tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;数据说明&quot;&gt;&lt;a href=&quot;#数据说明&quot; class=&quot;headerlink&quot; title=&quot;数据说明&quot;&gt;&lt;/a&gt;数据说明&lt;/h3&gt;&lt;p&gt;来源：第十二届服创大赛A类数据&lt;a href=&quot;http://www.fwwb.org.cn/topic/show/6c22
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Tianchi-base-v1</title>
    <link href="https://hahally.github.io/articles/Tianchi-base-v1/"/>
    <id>https://hahally.github.io/articles/Tianchi-base-v1/</id>
    <published>2021-02-02T14:47:06.000Z</published>
    <updated>2021-02-02T14:49:39.363Z</updated>
    
    <content type="html"><![CDATA[<h3 id="导入相关库"><a href="#导入相关库" class="headerlink" title="导入相关库"></a>导入相关库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#中文支持matplotlib</span></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]  <span class="comment"># 步骤一（替换sans-serif字体）</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span>    <span class="comment"># 步骤二（解决坐标轴负数的负号显示问题）</span></span><br></pre></td></tr></table></figure><h3 id="合并-anno-train-json"><a href="#合并-anno-train-json" class="headerlink" title="合并 anno_train.json"></a>合并 anno_train.json</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.listdir(<span class="string">'./dataset/annotations/'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#39;anno_train1.json&#39;, &#39;anno_train2.json&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合并两个标准文件</span></span><br><span class="line"></span><br><span class="line">d1 = pd.read_json(<span class="string">'./dataset/annotations/anno_train1.json'</span>)</span><br><span class="line">d2 = pd.read_json(<span class="string">'./dataset/annotations/anno_train2.json'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = pd.concat([d1,d2],ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d.to_csv(<span class="string">'./dataset/annotations/anno_train.csv'</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dt = pd.read_csv(<span class="string">'./dataset/annotations/anno_train.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dt.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 21323 entries, 0 to 21322Data columns (total 3 columns):bbox           21323 non-null objectdefect_name    21323 non-null objectname           21323 non-null objectdtypes: object(3)memory usage: 499.9+ KB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"><span class="comment"># Example data</span></span><br><span class="line">labels = dt[<span class="string">'defect_name'</span>].value_counts().index</span><br><span class="line"></span><br><span class="line">y_pos = np.arange(len(labels))</span><br><span class="line"></span><br><span class="line">performance = dt[<span class="string">'defect_name'</span>].value_counts().values</span><br><span class="line"></span><br><span class="line">error = np.random.rand(len(labels))</span><br><span class="line"></span><br><span class="line">ax.barh(y_pos, performance, xerr=error, align=<span class="string">'center'</span>)</span><br><span class="line">ax.set_yticks(y_pos)</span><br><span class="line">ax.set_yticklabels(labels)</span><br><span class="line">ax.invert_yaxis()  <span class="comment"># labels read top-to-bottom</span></span><br><span class="line">ax.set_xlabel(<span class="string">'Counts'</span>)</span><br><span class="line">ax.set_title(<span class="string">'catagory distribution'</span>);<span class="comment"># 加分号只输出图像</span></span><br></pre></td></tr></table></figure><p><img src="/articles/Tianchi-base-v1/output_11_0.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;导入相关库&quot;&gt;&lt;a href=&quot;#导入相关库&quot; class=&quot;headerlink&quot; title=&quot;导入相关库&quot;&gt;&lt;/a&gt;导入相关库&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter
      
    
    </summary>
    
    
    
      <category term="CV" scheme="https://hahally.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Tsinghua Dogs v1</title>
    <link href="https://hahally.github.io/articles/TsinghuaDogs-1/"/>
    <id>https://hahally.github.io/articles/TsinghuaDogs-1/</id>
    <published>2021-01-30T03:00:58.000Z</published>
    <updated>2021-02-06T08:30:45.830Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Tsinghua-Dogs数据集的结构"><a href="#Tsinghua-Dogs数据集的结构" class="headerlink" title="Tsinghua Dogs数据集的结构"></a>Tsinghua Dogs数据集的结构</h3><p>赛题地址:<a href="https://www.educoder.net/competitions/index/Jittor-2" target="_blank" rel="noopener">https://www.educoder.net/competitions/index/Jittor-2</a></p><p>来源：<a href="https://cg.cs.tsinghua.edu.cn/ThuDogs/" target="_blank" rel="noopener">Tsinghua Dogs数据集官网</a></p><ul><li>low-resolution：130类狗狗的图片集合</li><li>low-annotations：130类狗狗的标注结果，其中包括头部、整个身体的包围框</li><li>TrainAndValList：训练验证集切分</li></ul><p>每一类被分别保存在不同的文件夹里，该文件夹名称格式为：\$id-$class_id-\$class_name，其中 \$class_id 为类别标签，\$class_name为类别名称，如文件夹名为 220-n000069-Mexican_hairless 的文件对应的标签为 69，文件夹名为 480-n000125-Pekinese 的文件，对应的标签为 125。</p><h3 id="查看文件目录结构"><a href="#查看文件目录结构" class="headerlink" title="查看文件目录结构"></a>查看文件目录结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.listdir(<span class="string">'./dataset/'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#39;low-annotations&#39;, &#39;low-resolution&#39;, &#39;TEST_A&#39;, &#39;TrainAndValList&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">low_annotations = os.listdir(<span class="string">'./dataset/low-annotations'</span>)</span><br><span class="line">low_resolution = os.listdir(<span class="string">'./dataset/low-resolution'</span>)</span><br><span class="line"></span><br><span class="line">len(low_annotations),len(low_resolution),low_annotations[:<span class="number">5</span>],low_resolution[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><pre><code>(130, 131, [&#39;1043-n000001-Shiba_Dog&#39;,  &#39;1121-n000002-French_bulldog&#39;,  &#39;1160-n000003-Siberian_husky&#39;,  &#39;1324-n000004-malamute&#39;,  &#39;1936-n000005-Pomeranian&#39;], [&#39;1043-n000001-Shiba_Dog&#39;,  &#39;1121-n000002-French_bulldog&#39;,  &#39;1160-n000003-Siberian_husky&#39;,  &#39;1324-n000004-malamute&#39;,  &#39;1936-n000005-Pomeranian&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 无效文件</span></span><br><span class="line">low_resolution[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><pre><code>&#39;~$directoryfilecount.xlsx&#39;</code></pre><hr><h3 id="可视化Tsinghua-Dogs"><a href="#可视化Tsinghua-Dogs" class="headerlink" title="可视化Tsinghua Dogs"></a>可视化Tsinghua Dogs</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.dom.minidom</span><br><span class="line"><span class="keyword">from</span> xml.dom.minidom <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">DOMTree = parse(<span class="string">'./dataset/low-annotations/1043-n000001-Shiba_Dog/n100001.jpeg.xml'</span>)</span><br><span class="line">collection = DOMTree.documentElement</span><br><span class="line">img = cv2.imread(<span class="string">'./dataset/low-resolution/1043-n000001-Shiba_Dog/n100001.jpeg'</span>)</span><br><span class="line">headbndbox = collection.getElementsByTagName(<span class="string">"headbndbox"</span>)[<span class="number">0</span>]</span><br><span class="line">xmin = int(headbndbox.getElementsByTagName(<span class="string">"xmin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">ymin = int(headbndbox.getElementsByTagName(<span class="string">"ymin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">xmax = int(headbndbox.getElementsByTagName(<span class="string">"xmax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">ymax = int(headbndbox.getElementsByTagName(<span class="string">"ymax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">bodybndbox = collection.getElementsByTagName(<span class="string">"bodybndbox"</span>)[<span class="number">0</span>]</span><br><span class="line">xmin = int(bodybndbox.getElementsByTagName(<span class="string">"xmin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">ymin = int(bodybndbox.getElementsByTagName(<span class="string">"ymin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">xmax = int(bodybndbox.getElementsByTagName(<span class="string">"xmax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">ymax = int(bodybndbox.getElementsByTagName(<span class="string">"ymax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line">pl.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">pl.imshow(img[:,:,::<span class="number">-1</span>]);</span><br></pre></td></tr></table></figure><p><img src="/articles/TsinghuaDogs-1/output_8_0.png" alt="png"></p><h3 id="xml2csv"><a href="#xml2csv" class="headerlink" title="xml2csv"></a>xml2csv</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.dom.minidom</span><br><span class="line"><span class="keyword">from</span> xml.dom.minidom <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结构化</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xml2df</span><span class="params">(root_path,xml_filename)</span>:</span></span><br><span class="line">    </span><br><span class="line">    DOMTree = parse(root_path+xml_filename)</span><br><span class="line">    collection = DOMTree.documentElement</span><br><span class="line">    </span><br><span class="line">    folder = str(collection.getElementsByTagName(<span class="string">"folder"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    filename = str(collection.getElementsByTagName(<span class="string">"filename"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    dog_type = str(collection.getElementsByTagName(<span class="string">"name"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    </span><br><span class="line">    headbndbox = collection.getElementsByTagName(<span class="string">"headbndbox"</span>)[<span class="number">0</span>]</span><br><span class="line">    head_xmin = int(headbndbox.getElementsByTagName(<span class="string">"xmin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    head_ymin = int(headbndbox.getElementsByTagName(<span class="string">"ymin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    head_xmax = int(headbndbox.getElementsByTagName(<span class="string">"xmax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    head_ymax = int(headbndbox.getElementsByTagName(<span class="string">"ymax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    </span><br><span class="line">    bodybndbox = collection.getElementsByTagName(<span class="string">"bodybndbox"</span>)[<span class="number">0</span>]</span><br><span class="line">    body_xmin = int(bodybndbox.getElementsByTagName(<span class="string">"xmin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    body_ymin = int(bodybndbox.getElementsByTagName(<span class="string">"ymin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    body_xmax = int(bodybndbox.getElementsByTagName(<span class="string">"xmax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    body_ymax = int(bodybndbox.getElementsByTagName(<span class="string">"ymax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    </span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">'folder'</span>: [folder+<span class="string">'/'</span>+filename],</span><br><span class="line">        <span class="string">'dog_type'</span>: [dog_type],</span><br><span class="line">        <span class="string">'hbox'</span>: [[head_xmin,head_ymin,head_xmax,head_ymax]],</span><br><span class="line">        </span><br><span class="line">        <span class="string">'bbox'</span>: [[body_xmin,body_ymin,body_xmax,body_ymax]],</span><br><span class="line">        <span class="string">'label'</span>:[int(folder.split(<span class="string">'-'</span>)[<span class="number">1</span>][<span class="number">1</span>:].lstrip(<span class="string">'0'</span>))]</span><br><span class="line">    &#125;</span><br><span class="line">    df = pd.DataFrame(data=data)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">()</span>:</span></span><br><span class="line">    con = []</span><br><span class="line">    <span class="keyword">for</span> path <span class="keyword">in</span> tqdm(os.listdir(<span class="string">'./dataset/low-annotations/'</span>)):</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="string">'./dataset/low-annotations/'</span> + path):</span><br><span class="line">            root = <span class="string">'./dataset/low-annotations/'</span> + path + <span class="string">'/'</span></span><br><span class="line">            con.append(xml2df(root, file))</span><br><span class="line">    <span class="keyword">return</span> con</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">con = fun()</span><br></pre></td></tr></table></figure><pre><code>100%|████████████████████████████████████████████████████████████████| 130/130 [03:02&lt;00:00,  1.41s/it]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.concat(con,ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 70428 张图片</span></span><br><span class="line">df.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 70428 entries, 0 to 70427Data columns (total 5 columns):bbox        70428 non-null objectdog_type    70428 non-null objectfolder      70428 non-null objecthbox        70428 non-null objectlabel       70428 non-null int64dtypes: int64(1), object(4)memory usage: 2.7+ MB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head()</span><br></pre></td></tr></table></figure><div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>bbox</th>      <th>dog_type</th>      <th>folder</th>      <th>hbox</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>[241, 47, 720, 720]</td>      <td>Shiba_Dog</td>      <td>1043-n000001-Shiba_Dog/n100001.jpeg</td>      <td>[258, 43, 494, 322]</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>[126, 200, 462, 494]</td>      <td>Shiba_Dog</td>      <td>1043-n000001-Shiba_Dog/n100002.jpeg</td>      <td>[348, 206, 440, 300]</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>[81, 0, 587, 720]</td>      <td>Shiba_Dog</td>      <td>1043-n000001-Shiba_Dog/n100003.jpeg</td>      <td>[124, 5, 564, 458]</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>[81, 81, 342, 332]</td>      <td>Shiba_Dog</td>      <td>1043-n000001-Shiba_Dog/n100004.jpg</td>      <td>[244, 146, 333, 260]</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>[104, 97, 234, 391]</td>      <td>Shiba_Dog</td>      <td>1043-n000001-Shiba_Dog/n100005.jpg</td>      <td>[127, 118, 222, 247]</td>      <td>1</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_csv(<span class="string">'./ImageInfo.csv'</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a>数据划分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">'./dataset/low-resolution/'</span></span><br><span class="line">info_path = <span class="string">'./dataset/low-annotations/'</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'./dataset/TrainAndValList/train.lst'</span>,header=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证集</span></span><br><span class="line">val_data = pd.read_csv(<span class="string">'./dataset/TrainAndValList/validation.lst'</span>,header=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 完整数据</span></span><br><span class="line">all_data = pd.read_csv(<span class="string">'./ImageInfo.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">'folder'</span>] = train_data[<span class="number">0</span>].apply(<span class="keyword">lambda</span> x:x[<span class="number">3</span>:])</span><br><span class="line">val_data[<span class="string">'folder'</span>] = val_data[<span class="number">0</span>].apply(<span class="keyword">lambda</span> x:x[<span class="number">3</span>:])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = train_data.drop(columns=<span class="number">0</span>)</span><br><span class="line">val_data = val_data.drop(columns=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到最终划分的数据：train/validation</span></span><br><span class="line">train_data = train_data.merge(all_data,on=[<span class="string">'folder'</span>],how=<span class="string">'left'</span>)</span><br><span class="line">val_data = val_data.merge(all_data,on=[<span class="string">'folder'</span>],how=<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.head()</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>folder</th>      <th>bbox</th>      <th>dog_type</th>      <th>hbox</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1043-n000001-Shiba_Dog/n100001.jpeg</td>      <td>[241, 47, 720, 720]</td>      <td>Shiba_Dog</td>      <td>[258, 43, 494, 322]</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>1043-n000001-Shiba_Dog/n100002.jpeg</td>      <td>[126, 200, 462, 494]</td>      <td>Shiba_Dog</td>      <td>[348, 206, 440, 300]</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>1043-n000001-Shiba_Dog/n100003.jpeg</td>      <td>[81, 0, 587, 720]</td>      <td>Shiba_Dog</td>      <td>[124, 5, 564, 458]</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>1043-n000001-Shiba_Dog/n100004.jpg</td>      <td>[81, 81, 342, 332]</td>      <td>Shiba_Dog</td>      <td>[244, 146, 333, 260]</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>1043-n000001-Shiba_Dog/n100005.jpg</td>      <td>[104, 97, 234, 391]</td>      <td>Shiba_Dog</td>      <td>[127, 118, 222, 247]</td>      <td>1</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val_data.head()</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>folder</th>      <th>bbox</th>      <th>dog_type</th>      <th>hbox</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1043-n000001-Shiba_Dog/n100032.jpeg</td>      <td>[109, 44, 471, 533]</td>      <td>Shiba_Dog</td>      <td>[282, 60, 455, 279]</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>1043-n000001-Shiba_Dog/n100069.jpg</td>      <td>[52, 1, 315, 221]</td>      <td>Shiba_Dog</td>      <td>[206, 15, 309, 141]</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>1043-n000001-Shiba_Dog/n100109.jpeg</td>      <td>[41, 61, 363, 295]</td>      <td>Shiba_Dog</td>      <td>[119, 70, 220, 222]</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>1043-n000001-Shiba_Dog/n100112.jpg</td>      <td>[59, 0, 296, 472]</td>      <td>Shiba_Dog</td>      <td>[58, 0, 281, 227]</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>1043-n000001-Shiba_Dog/n100171.jpg</td>      <td>[30, 80, 318, 271]</td>      <td>Shiba_Dog</td>      <td>[188, 91, 257, 187]</td>      <td>1</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(train_data),len(val_data),len(all_data)</span><br></pre></td></tr></table></figure><pre><code>(65228, 5200, 70428)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存(save to csv file)</span></span><br><span class="line">train_data.to_csv(<span class="string">'./train.data.csv'</span>,index=<span class="literal">False</span>)</span><br><span class="line">val_data.to_csv(<span class="string">'./valid.data.csv'</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all_data = pd.read_csv(<span class="string">'./ImageInfo.csv'</span>)</span><br><span class="line">train_data = pd.read_csv(<span class="string">'./train.data.csv'</span>)</span><br><span class="line">val_data = pd.read_csv(<span class="string">'./valid.data.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># top 15 类型</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(d)</span>:</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">15</span>,<span class="number">10</span>))</span><br><span class="line">    plt.pie(d.values,  labels=d.index, autopct=<span class="string">'%1.2f%%'</span>, startangle=<span class="number">160</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> dt <span class="keyword">in</span> [all_data,train_data,val_data]:</span><br><span class="line">    d = dt[<span class="string">'dog_type'</span>].value_counts()[:<span class="number">15</span>]</span><br><span class="line">    show(d)</span><br></pre></td></tr></table></figure><p><img src="/articles/TsinghuaDogs-1/output_35_0.png" alt="png"></p><p><img src="/articles/TsinghuaDogs-1/output_35_1.png" alt="png"></p><p><img src="/articles/TsinghuaDogs-1/output_35_2.png" alt="png"></p><h3 id="check-data"><a href="#check-data" class="headerlink" title="check data"></a>check data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看数据维度</span></span><br><span class="line">all_data = pd.read_csv(<span class="string">'./ImageInfo.csv'</span>)</span><br><span class="line"></span><br><span class="line">root = <span class="string">'./dataset/low-resolution/'</span></span><br><span class="line">all_data[<span class="string">'flag'</span>] = all_data[<span class="string">'folder'</span>].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> <span class="number">3</span> == plt.imread(root + x).shape[<span class="number">2</span>] <span class="keyword">else</span> <span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有一张图像维度不是 3</span></span><br><span class="line">all_data[<span class="string">'flag'</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>1    704270        1Name: flag, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rgba 图像</span></span><br><span class="line">d = all_data[all_data[<span class="string">'flag'</span>]==<span class="number">0</span>][<span class="string">'folder'</span>].values[<span class="number">0</span>]</span><br><span class="line">plt.imread(<span class="string">'./dataset/low-resolution/'</span>+d).shape</span><br></pre></td></tr></table></figure><pre><code>(189, 213, 4)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># delete </span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'./train.data.csv'</span>)</span><br><span class="line">valid_data = pd.read_csv(<span class="string">'./valid.data.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[train_data[<span class="string">'folder'</span>]==d]</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>folder</th>      <th>bbox</th>      <th>dog_type</th>      <th>hbox</th>      <th>label</th>    </tr>  </thead>  <tbody>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">valid_data[valid_data[<span class="string">'folder'</span>]==d]</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>folder</th>      <th>bbox</th>      <th>dog_type</th>      <th>hbox</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>4375</th>      <td>274-n000110-Shetland_sheepdog/n136188.jpg</td>      <td>[3, 0, 203, 183]</td>      <td>Shetland_sheepdog</td>      <td>[10, 4, 85, 68]</td>      <td>110</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">valid_data = valid_data.drop(</span><br><span class="line">    valid_data[valid_data[<span class="string">'folder'</span>] == d].index).reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新保存</span></span><br><span class="line">valid_data.to_csv(<span class="string">'./valid.data.csv'</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="图像增强"><a href="#图像增强" class="headerlink" title="图像增强"></a>图像增强</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 亮度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_bright</span><span class="params">(img, delta=<span class="number">32</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Random change bright</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        im(numpy.array): one H,W,3 image</span></span><br><span class="line"><span class="string">        delta(int): bright ratio</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    im = np.copy(img)</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">        delta = random.uniform(-delta, delta)</span><br><span class="line">        im += delta</span><br><span class="line">        im = im.clip(min=<span class="number">0</span>, max=<span class="number">255</span>)</span><br><span class="line">    <span class="keyword">return</span> im</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对比度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_contrast</span><span class="params">(img, lower=<span class="number">0.5</span>, upper=<span class="number">1.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Random change contrast</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        im(numpy.array): one H,W,3 image</span></span><br><span class="line"><span class="string">        lower(float): change lower limit </span></span><br><span class="line"><span class="string">        upper(float): change upper limit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    im = np.copy(img)</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">        alpha = random.uniform(lower, upper)</span><br><span class="line">        im *= alpha</span><br><span class="line">        im = im.clip(min=<span class="number">0</span>, max=<span class="number">255</span>)</span><br><span class="line">    <span class="keyword">return</span> im</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 饱和度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_saturation</span><span class="params">(img, lower=<span class="number">0.5</span>, upper=<span class="number">1.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Random change saturation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        im(numpy.array): one H,W,3 image</span></span><br><span class="line"><span class="string">        lower(float): change lower limit </span></span><br><span class="line"><span class="string">        upper(float): change upper limit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    im = np.copy(img)</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">        im[:, :, <span class="number">1</span>] *= random.uniform(lower, upper)</span><br><span class="line">    <span class="keyword">return</span> im</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 色调</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_hue</span><span class="params">(img, delta=<span class="number">18.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Random change hue</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        im(numpy.array): one H,W,3 image</span></span><br><span class="line"><span class="string">        delta(int): hue ratio</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    im = np.copy(img)</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">        im = cv.cvtColor(im, cv.COLOR_RGB2HSV)</span><br><span class="line">        im[:, :, <span class="number">0</span>] += random.uniform(-delta, delta)</span><br><span class="line">        im[:, :, <span class="number">0</span>][im[:, :, <span class="number">0</span>] &gt; <span class="number">360.0</span>] -= <span class="number">360.0</span></span><br><span class="line">        im[:, :, <span class="number">0</span>][im[:, :, <span class="number">0</span>] &lt; <span class="number">0.0</span>] += <span class="number">360.0</span></span><br><span class="line">        im = cv.cvtColor(im, cv.COLOR_HSV2RGB)</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoad</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,folder=<span class="string">'./'</span>,dtype=<span class="string">'train'</span>,shuffle=False)</span>:</span></span><br><span class="line">        self.folder = folder</span><br><span class="line">        self.shuffle = shuffle</span><br><span class="line">        self.dtype = dtype</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span> self.dtype <span class="keyword">in</span> [<span class="string">'train'</span>,<span class="string">'test'</span>,<span class="string">'valid'</span>]</span><br><span class="line">        </span><br><span class="line">        self.data = pd.read_csv(os.path.join(self.folder+self.dtype+<span class="string">'.data.csv'</span>))</span><br><span class="line">            </span><br><span class="line">        self.mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">        self.std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">        <span class="keyword">if</span> self.shuffle:</span><br><span class="line">            self.data = self.data.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,i)</span>:</span></span><br><span class="line">        dt = self.data.iloc[i]</span><br><span class="line">        image = cv.imread(<span class="string">'./dataset/low-resolution/'</span>+dt[<span class="string">'folder'</span>])</span><br><span class="line">        image = cv.cvtColor(image,cv.COLOR_BGR2RGB).astype(<span class="string">"float32"</span>)</span><br><span class="line">        </span><br><span class="line">        bbox = np.array([dt[<span class="string">'bbox'</span>].strip(<span class="string">"[]"</span>).split(<span class="string">','</span>)]).astype(<span class="string">"float32"</span>)</span><br><span class="line">        hbox = np.array([dt[<span class="string">'hbox'</span>].strip(<span class="string">"[]"</span>).split(<span class="string">','</span>)]).astype(<span class="string">"float32"</span>)</span><br><span class="line">        label = np.array(dt[<span class="string">'label'</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.dtype == <span class="string">'train'</span>:</span><br><span class="line">            data_enhance = [random_bright, </span><br><span class="line">                            random_contrast, </span><br><span class="line">                            random_saturation, </span><br><span class="line">                            random_hue]</span><br><span class="line">            </span><br><span class="line">            random.shuffle(data_enhance)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> d <span class="keyword">in</span> data_enhance:</span><br><span class="line">                image = d(image)</span><br><span class="line">            </span><br><span class="line"><span class="comment">#             if random.random() &lt; 0.5:</span></span><br><span class="line"><span class="comment">#                 image, boxes = random_expand(image, boxes)            </span></span><br><span class="line"><span class="comment">#             image, boxes, labels = random_crop(image, boxes, labels)           </span></span><br><span class="line"><span class="comment">#             image, boxes = random_flip(image, boxes)</span></span><br><span class="line">            </span><br><span class="line">        height, width, _ = image.shape</span><br><span class="line">        image = cv.resize(image, (<span class="number">300</span>, <span class="number">300</span>))</span><br><span class="line">        image /= <span class="number">255.</span></span><br><span class="line">        image = (image - self.mean) / self.std</span><br><span class="line"><span class="comment">#         image = image.transpose((2,0,1)).astype("float32")</span></span><br><span class="line">        bbox[:,[<span class="number">0</span>,<span class="number">2</span>]] /= width</span><br><span class="line">        </span><br><span class="line">        bbox[:,[<span class="number">1</span>,<span class="number">3</span>]] /= height</span><br><span class="line"></span><br><span class="line">        hbox[:,[<span class="number">0</span>,<span class="number">2</span>]] /= width</span><br><span class="line">        hbox[:,[<span class="number">1</span>,<span class="number">3</span>]] /= height</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> image, bbox, hbox, label</span><br></pre></td></tr></table></figure><h3 id="Build-Model"><a href="#Build-Model" class="headerlink" title="Build Model"></a>Build Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets, layers, models</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VGG16</span></span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(</span><br><span class="line">    layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                  padding=<span class="string">"same"</span>,</span><br><span class="line">                  activation=<span class="string">'relu'</span>,</span><br><span class="line">                  input_shape=(<span class="number">300</span>, <span class="number">300</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">130</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = DataLoad(shuffle=<span class="literal">True</span>)</span><br><span class="line">valid = DataLoad(dtype=<span class="string">'valid'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cat_data</span><span class="params">(dt,n=<span class="number">100</span>)</span>:</span></span><br><span class="line">    con = []</span><br><span class="line">    lab = []</span><br><span class="line">    <span class="keyword">for</span> i,(image, bbox, hbox, label) <span class="keyword">in</span> enumerate(dt):</span><br><span class="line">        <span class="keyword">if</span> i == n:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        con.append(image)</span><br><span class="line">        lab.append(label)</span><br><span class="line">    lab = np.array(lab)    </span><br><span class="line">    <span class="keyword">return</span> np.array(con),lab</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train,y_train = cat_data(train,n=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">x_test,y_test = cat_data(valid,n=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train = tf.constant(y_train)</span><br><span class="line">y_test = tf.constant(y_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">    x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">    batch_size=<span class="number">1</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    validation_data=(x_test, y_test) </span><br><span class="line">)</span><br></pre></td></tr></table></figure><hr><h3 id="SSD-Jittor"><a href="#SSD-Jittor" class="headerlink" title="SSD-Jittor:"></a>SSD-Jittor:</h3><p>New a py file under the root dir: <code>process.py</code></p><p><code>new_parse_annotation()</code> :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_parse_annotation</span><span class="params">(annotation_path)</span>:</span></span><br><span class="line">    </span><br><span class="line">    DOMTree = parse(annotation_path)</span><br><span class="line">    collection = DOMTree.documentElement</span><br><span class="line">    </span><br><span class="line">    folder = str(collection.getElementsByTagName(<span class="string">"folder"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line"><span class="comment">#     filename = str(collection.getElementsByTagName("filename")[0].childNodes[0].data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     dog_type = str(collection.getElementsByTagName("name")[0].childNodes[0].data)   </span></span><br><span class="line"><span class="comment">#     headbndbox = collection.getElementsByTagName("headbndbox")[0]</span></span><br><span class="line"><span class="comment">#     head_xmin = int(headbndbox.getElementsByTagName("xmin")[0].childNodes[0].data)</span></span><br><span class="line"><span class="comment">#     head_ymin = int(headbndbox.getElementsByTagName("ymin")[0].childNodes[0].data)</span></span><br><span class="line"><span class="comment">#     head_xmax = int(headbndbox.getElementsByTagName("xmax")[0].childNodes[0].data)</span></span><br><span class="line"><span class="comment">#     head_ymax = int(headbndbox.getElementsByTagName("ymax")[0].childNodes[0].data)</span></span><br><span class="line">    difficult = int(collection.getElementsByTagName(<span class="string">"difficult"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    </span><br><span class="line">    bodybndbox = collection.getElementsByTagName(<span class="string">"bodybndbox"</span>)[<span class="number">0</span>]</span><br><span class="line">    body_xmin = int(bodybndbox.getElementsByTagName(<span class="string">"xmin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    body_ymin = int(bodybndbox.getElementsByTagName(<span class="string">"ymin"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    body_xmax = int(bodybndbox.getElementsByTagName(<span class="string">"xmax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    body_ymax = int(bodybndbox.getElementsByTagName(<span class="string">"ymax"</span>)[<span class="number">0</span>].childNodes[<span class="number">0</span>].data)</span><br><span class="line">    </span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">'boxes'</span>: [[body_xmin - <span class="number">1</span>,body_ymin - <span class="number">1</span>,body_xmax - <span class="number">1</span>,body_ymax - <span class="number">1</span>]],</span><br><span class="line">        <span class="string">'labels'</span>:[int(folder.split(<span class="string">'-'</span>)[<span class="number">1</span>][<span class="number">1</span>:].lstrip(<span class="string">'0'</span>))],</span><br><span class="line">        <span class="string">'difficulties'</span>:[difficult]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p><code>create_json_data()</code> :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_json_data</span><span class="params">(Lst_dir,anno_dir)</span>:</span></span><br><span class="line">    train_images = list()</span><br><span class="line">    train_objects = list()</span><br><span class="line"></span><br><span class="line">    lst_path = os.path.join(Lst_dir, <span class="string">'train.lst'</span>)</span><br><span class="line">    dt = pd.read_csv(lst_path,header=<span class="literal">None</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> tqdm(dt[<span class="number">0</span>]):</span><br><span class="line">        img = item.split(<span class="string">'/'</span>)[<span class="number">-2</span>] + <span class="string">'/'</span> + item.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        train_images.append(img)</span><br><span class="line">        anno_path = os.path.join(anno_dir,img) + <span class="string">'.xml'</span></span><br><span class="line">        train_objects.append(new_parse_annotation(anno_path))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">assert</span> len(train_objects) == len(train_images)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save to file</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(<span class="string">'./dataset/JsonData/'</span>, <span class="string">'TRAIN_images.json'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> j:</span><br><span class="line">        json.dump(train_images, j)</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(<span class="string">'./dataset/JsonData/'</span>, <span class="string">'TRAIN_objects.json'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> j:</span><br><span class="line">        json.dump(train_objects, j)</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(<span class="string">'./dataset/JsonData/'</span>, <span class="string">'label_map.json'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> j:</span><br><span class="line">        json.dump(label_map, j)  <span class="comment"># save label map too</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Validation data</span></span><br><span class="line">    test_images = list()</span><br><span class="line">    test_objects = list()</span><br><span class="line">    </span><br><span class="line">    lst_path = os.path.join(Lst_dir, <span class="string">'validation.lst'</span>)</span><br><span class="line">    dt = pd.read_csv(lst_path,header=<span class="literal">None</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> tqdm(dt[<span class="number">0</span>]):</span><br><span class="line">        img = item.split(<span class="string">'/'</span>)[<span class="number">-2</span>] + <span class="string">'/'</span> + item.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        test_images.append(img)</span><br><span class="line">        </span><br><span class="line">        anno_path = os.path.join(anno_dir,img) + <span class="string">'.xml'</span></span><br><span class="line">        test_objects.append(new_parse_annotation(anno_path))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">assert</span> len(test_objects) == len(test_images)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save to file</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(<span class="string">'./dataset/JsonData/'</span>, <span class="string">'VALID_images.json'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> j:</span><br><span class="line">        json.dump(test_images, j)</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(<span class="string">'./dataset/JsonData/'</span>, <span class="string">'VALID_objects.json'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> j:</span><br><span class="line">        json.dump(test_objects, j)</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">'Train data:&#123;&#125;,Valid data:&#123;&#125;'</span>.format(len(train_objects),len(test_objects)))</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Tsinghua-Dogs数据集的结构&quot;&gt;&lt;a href=&quot;#Tsinghua-Dogs数据集的结构&quot; class=&quot;headerlink&quot; title=&quot;Tsinghua Dogs数据集的结构&quot;&gt;&lt;/a&gt;Tsinghua Dogs数据集的结构&lt;/h3&gt;&lt;p&gt;赛题
      
    
    </summary>
    
    
    
      <category term="CV" scheme="https://hahally.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Titanic</title>
    <link href="https://hahally.github.io/articles/Titanic/"/>
    <id>https://hahally.github.io/articles/Titanic/</id>
    <published>2021-01-28T06:46:17.000Z</published>
    <updated>2021-01-29T06:03:54.701Z</updated>
    
    <content type="html"><![CDATA[<h3 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h3><p><code>Titanic Survival Predictions</code> 可以算是机器学习中的 <code>Hello World</code>了。像kaggle和天池这样的竞赛平台，都将此作为入门的经典题目。</p><ul><li><p>数据：Titanic Survival Predictions</p><ul><li>train.csv：训练集</li><li>test.csv：测试集</li></ul></li><li><p>任务：主要对 <code>train.csv</code> 数据进行处理分析。</p><ul><li>了解和掌握 <code>pandas</code> 透视表的一些基本操作(读取，查看，聚合…)</li><li>了解和掌握一些基本绘图方法</li><li>分析影响游客存活的因素</li><li>分析对缺失值的处理方法</li></ul></li><li><p>数据属性说明(可以参考这里的<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">Data Dictionary</a>)</p><ul><li>PassengerId：id</li><li>Survived：是否存活(0/1)</li><li>Pclass: 舱位类型(1,2,3)</li><li>Name：姓名</li><li>Sex：性别</li><li>Age：年龄</li><li>SibSp：of siblings / spouses aboard the Titanic</li><li>Parch：of parents / children aboard the Titanic</li><li>Ticket：Ticket number</li><li>Fare：票价</li><li>Cabin：房间编号</li><li>Embarked：登船港口</li></ul><ul><li>相关文档：<ul><li><a href="https://pandas.pydata.org/pandas-docs/stable/" target="_blank" rel="noopener">https://pandas.pydata.org/pandas-docs/stable/</a></li><li><a href="https://www.pypandas.cn/" target="_blank" rel="noopener">https://www.pypandas.cn/</a></li><li><a href="https://www.matplotlib.org.cn/" target="_blank" rel="noopener">https://www.matplotlib.org.cn/</a></li><li><a href="https://matplotlib.org/" target="_blank" rel="noopener">https://matplotlib.org/</a></li><li><a href="https://www.numpy.org.cn/" target="_blank" rel="noopener">https://www.numpy.org.cn/</a></li><li><a href="https://numpy.org/doc/stable/reference/" target="_blank" rel="noopener">https://numpy.org/doc/stable/reference/</a></li><li><a href="http://seaborn.pydata.org/" target="_blank" rel="noopener">http://seaborn.pydata.org/</a></li></ul></li></ul></li></ul><h3 id="导入相关的库"><a href="#导入相关的库" class="headerlink" title="导入相关的库"></a>导入相关的库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#ignore warnings</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure><h3 id="数据预览"><a href="#数据预览" class="headerlink" title="数据预览"></a>数据预览</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 读取数据</span></span><br><span class="line">dt_train = pd.read_csv(<span class="string">'../dataset/train.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># head() 函数默认查看前 5 条数据</span></span><br><span class="line">dt_train.head()</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Name</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Ticket</th>      <th>Fare</th>      <th>Cabin</th>      <th>Embarked</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0</td>      <td>3</td>      <td>Braund, Mr. Owen Harris</td>      <td>male</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>A/5 21171</td>      <td>7.2500</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>      <td>1</td>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>      <td>female</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>PC 17599</td>      <td>71.2833</td>      <td>C85</td>      <td>C</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1</td>      <td>3</td>      <td>Heikkinen, Miss. Laina</td>      <td>female</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>STON/O2. 3101282</td>      <td>7.9250</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>1</td>      <td>1</td>      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>      <td>female</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>113803</td>      <td>53.1000</td>      <td>C123</td>      <td>S</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>0</td>      <td>3</td>      <td>Allen, Mr. William Henry</td>      <td>male</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>373450</td>      <td>8.0500</td>      <td>NaN</td>      <td>S</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sample() 随机采样</span></span><br><span class="line">train.sample(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Fare</th>    </tr>  </thead>  <tbody>    <tr>      <th>127</th>      <td>128</td>      <td>1</td>      <td>3</td>      <td>24.00</td>      <td>0</td>      <td>0</td>      <td>7.1417</td>    </tr>    <tr>      <th>669</th>      <td>670</td>      <td>1</td>      <td>1</td>      <td>NaN</td>      <td>1</td>      <td>0</td>      <td>52.0000</td>    </tr>    <tr>      <th>229</th>      <td>230</td>      <td>0</td>      <td>3</td>      <td>NaN</td>      <td>3</td>      <td>1</td>      <td>25.4667</td>    </tr>    <tr>      <th>644</th>      <td>645</td>      <td>1</td>      <td>3</td>      <td>0.75</td>      <td>2</td>      <td>1</td>      <td>19.2583</td>    </tr>    <tr>      <th>337</th>      <td>338</td>      <td>1</td>      <td>1</td>      <td>41.00</td>      <td>0</td>      <td>0</td>      <td>134.5000</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># info() 查看数据基本信息</span></span><br><span class="line">dt_train.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId    891 non-null int64Survived       891 non-null int64Pclass         891 non-null int64Name           891 non-null objectSex            891 non-null objectAge            714 non-null float64SibSp          891 non-null int64Parch          891 non-null int64Ticket         891 non-null objectFare           891 non-null float64Cabin          204 non-null objectEmbarked       889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.7+ KB</code></pre><p>可以看到总共有 12 columns(12个字段,通过 <code>dt_train.columns</code> 可以得到)。<br>info中也可以大致知道各个字段缺失情况：<code>Age</code>、<code>Fare</code>、<code>Cabin</code>、<code>Embarked</code>都是存在缺失值的。<br>除此外，各个字段的数据类型也一目了然。</p><ul><li>Survived: int</li><li>Pclass: int</li><li>Name: string</li><li>Sex: string</li><li>Age: float</li><li>SibSp: int</li><li>Parch: int</li><li>Ticket: string</li><li>Fare: float</li><li>Cabin: string</li><li>Embarked: string</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取字段名称</span></span><br><span class="line">dt_train.columns</span><br></pre></td></tr></table></figure><pre><code>Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Name&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;,       &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;],      dtype=&#39;object&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看数值类型字段的大致数据分布情况</span></span><br><span class="line">dt_train.describe()</span><br></pre></td></tr></table></figure><div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Fare</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>891.000000</td>      <td>891.000000</td>      <td>891.000000</td>      <td>714.000000</td>      <td>891.000000</td>      <td>891.000000</td>      <td>891.000000</td>    </tr>    <tr>      <th>mean</th>      <td>446.000000</td>      <td>0.383838</td>      <td>2.308642</td>      <td>29.699118</td>      <td>0.523008</td>      <td>0.381594</td>      <td>32.204208</td>    </tr>    <tr>      <th>std</th>      <td>257.353842</td>      <td>0.486592</td>      <td>0.836071</td>      <td>14.526497</td>      <td>1.102743</td>      <td>0.806057</td>      <td>49.693429</td>    </tr>    <tr>      <th>min</th>      <td>1.000000</td>      <td>0.000000</td>      <td>1.000000</td>      <td>0.420000</td>      <td>0.000000</td>      <td>0.000000</td>      <td>0.000000</td>    </tr>    <tr>      <th>25%</th>      <td>223.500000</td>      <td>0.000000</td>      <td>2.000000</td>      <td>20.125000</td>      <td>0.000000</td>      <td>0.000000</td>      <td>7.910400</td>    </tr>    <tr>      <th>50%</th>      <td>446.000000</td>      <td>0.000000</td>      <td>3.000000</td>      <td>28.000000</td>      <td>0.000000</td>      <td>0.000000</td>      <td>14.454200</td>    </tr>    <tr>      <th>75%</th>      <td>668.500000</td>      <td>1.000000</td>      <td>3.000000</td>      <td>38.000000</td>      <td>1.000000</td>      <td>0.000000</td>      <td>31.000000</td>    </tr>    <tr>      <th>max</th>      <td>891.000000</td>      <td>1.000000</td>      <td>3.000000</td>      <td>80.000000</td>      <td>8.000000</td>      <td>6.000000</td>      <td>512.329200</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看缺失值分布: 主要年龄特征缺失严重</span></span><br><span class="line">dt_train.isnull().sum()</span><br></pre></td></tr></table></figure><pre><code>PassengerId      0Survived         0Pclass           0Name             0Sex              0Age            177SibSp            0Parch            0Ticket           0Fare             0Cabin          687Embarked         2dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 缺失百分比</span></span><br><span class="line">pd.isnull(dt_train).sum()*<span class="number">100</span>/dt_train.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>PassengerId     0.000000Survived        0.000000Pclass          0.000000Name            0.000000Sex             0.000000Age            19.865320SibSp           0.000000Parch           0.000000Ticket          0.000000Fare            0.000000Cabin          77.104377Embarked        0.224467dtype: float64</code></pre><p>PS：从数据出发，思考哪些因素或那类特征的人更容易从这场灾难中存活下来</p><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制条形图观察 性别 特征</span></span><br><span class="line">sns.barplot(x=<span class="string">"Sex"</span>, y=<span class="string">"Survived"</span>, data=dt_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#print percentages of females vs. males that survive</span></span><br><span class="line">print(</span><br><span class="line">    <span class="string">"Percentage of females who survived:"</span>,</span><br><span class="line">    dt_train.Survived[dt_train.Sex == <span class="string">'female'</span>].value_counts(normalize=<span class="literal">True</span>)[<span class="number">1</span>] *</span><br><span class="line">    <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">print(</span><br><span class="line">    <span class="string">"Percentage of males who survived:"</span>,</span><br><span class="line">    dt_train.Survived[dt_train.Sex == <span class="string">'male'</span>].value_counts(normalize=<span class="literal">True</span>)[<span class="number">1</span>] * <span class="number">100</span>)</span><br></pre></td></tr></table></figure><pre><code>Percentage of females who survived: 74.20382165605095Percentage of males who survived: 18.890814558058924</code></pre><p><img src="/articles/Titanic/output_15_1.png" alt="png"></p><p>从上面可以看到，女性比男性其实具有更高的生存机会。其他特征，可自行类比做图分析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直方图</span></span><br><span class="line"></span><br><span class="line">cols = [<span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>, <span class="string">'Pclass'</span>]</span><br><span class="line">dt_train[cols].hist(figsize = (<span class="number">15</span>, <span class="number">6</span>), color = <span class="string">'steelblue'</span>, edgecolor = <span class="string">'firebrick'</span>, linewidth = <span class="number">1.5</span>, layout = (<span class="number">2</span>, <span class="number">3</span>));</span><br></pre></td></tr></table></figure><p><img src="/articles/Titanic/output_17_0.png" alt="png"></p><p>从年龄来看，大部分是40岁以下的年轻人；Fare中大部分在一百美元内；Parch与SibSp可以知道独自乘船的人较多；Pclass说明大部分人订的是3等舱。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 年龄分布情况</span></span><br><span class="line">men = dt_train[dt_train[<span class="string">'Sex'</span>]  == <span class="string">'male'</span>]</span><br><span class="line">women = dt_train[dt_train[<span class="string">'Sex'</span>]  == <span class="string">'female'</span>]</span><br><span class="line"></span><br><span class="line">fig, (ax1, ax2, ax3) = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize = (<span class="number">13</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">sns.distplot(dt_train[dt_train[<span class="string">'Survived'</span>] == <span class="number">1</span>][<span class="string">'Age'</span>].dropna(), bins = <span class="number">20</span>, label = <span class="string">'Survived'</span>, ax = ax1, kde = <span class="literal">False</span>)</span><br><span class="line">sns.distplot(dt_train[dt_train[<span class="string">'Survived'</span>] == <span class="number">0</span>][<span class="string">'Age'</span>].dropna(), bins = <span class="number">20</span>, label = <span class="string">'Deceased'</span>, ax = ax1, kde = <span class="literal">False</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">ax1.set_title(<span class="string">'Age Distribution - All Passengers'</span>)</span><br><span class="line"></span><br><span class="line">sns.distplot(women[women[<span class="string">'Survived'</span>] == <span class="number">1</span>][<span class="string">'Age'</span>].dropna(), bins = <span class="number">20</span>, label = <span class="string">'Survived'</span>, ax = ax2, kde = <span class="literal">False</span>)</span><br><span class="line">sns.distplot(women[women[<span class="string">'Survived'</span>] == <span class="number">0</span>][<span class="string">'Age'</span>].dropna(), bins = <span class="number">20</span>, label = <span class="string">'Deceased'</span>, ax = ax2, kde = <span class="literal">False</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">ax2.set_title(<span class="string">'Age Distribution - Women'</span>)</span><br><span class="line"></span><br><span class="line">sns.distplot(men[men[<span class="string">'Survived'</span>] == <span class="number">1</span>][<span class="string">'Age'</span>].dropna(), bins = <span class="number">20</span>, label = <span class="string">'Survived'</span>, ax = ax3, kde = <span class="literal">False</span>)</span><br><span class="line">sns.distplot(men[men[<span class="string">'Survived'</span>] == <span class="number">0</span>][<span class="string">'Age'</span>].dropna(), bins = <span class="number">20</span>, label = <span class="string">'Deceased'</span>, ax = ax3, kde = <span class="literal">False</span>)</span><br><span class="line">ax3.legend()</span><br><span class="line">ax3.set_title(<span class="string">'Age Distribution - Men'</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/articles/Titanic/output_19_0.png" alt="png"></p><p>这三幅图似乎表明：</p><ul><li>二三四十岁的人存活率要更高</li><li>十岁以下的年龄段出现的峰值说明婴幼儿也更容易存活下来</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相关性矩阵图</span></span><br><span class="line">names = [<span class="string">'Survived'</span>, <span class="string">'Pclass'</span>, <span class="string">'Sex'</span>, <span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>]</span><br><span class="line"></span><br><span class="line">correlations = dt_train.corr()</span><br><span class="line">correction = abs(correlations)          <span class="comment"># 取绝对值，只看相关程度 ，不关心正相关还是负相关</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot correlation matrix</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(figsize=(<span class="number">20</span>, <span class="number">20</span>))  <span class="comment">#图的大小为20*20</span></span><br><span class="line"></span><br><span class="line">ax = sns.heatmap(correction,</span><br><span class="line">                 cmap=plt.cm.Greys,</span><br><span class="line">                 linewidths=<span class="number">0.05</span>,</span><br><span class="line">                 vmax=<span class="number">1</span>,</span><br><span class="line">                 vmin=<span class="number">0</span>,</span><br><span class="line">                 annot=<span class="literal">True</span>,</span><br><span class="line">                 annot_kws=&#123;</span><br><span class="line">                     <span class="string">'size'</span>: <span class="number">6</span>,</span><br><span class="line">                     <span class="string">'weight'</span>: <span class="string">'bold'</span></span><br><span class="line">                 &#125;)</span><br><span class="line"><span class="comment">#热力图参数设置（相关系数矩阵，颜色，每个值间隔等）</span></span><br><span class="line">plt.xticks(np.arange(len(names)) + <span class="number">0.5</span>, names)  <span class="comment">#横坐标标注点</span></span><br><span class="line">plt.yticks(np.arange(len(names)) + <span class="number">0.5</span>, names)  <span class="comment">#纵坐标标注点</span></span><br><span class="line">ax.set_title(<span class="string">'Characteristic correlation'</span>)      <span class="comment">#标题设置</span></span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&#39;Characteristic correlation&#39;)</code></pre><p><img src="/articles/Titanic/output_17_1.png" alt="png"></p><h4 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># value_counts():返回各个属性的数量</span></span><br><span class="line"><span class="comment"># 这里可以看到 男性 557人，女性 314人</span></span><br><span class="line"><span class="comment"># 参数 normalize=True 时，返回的是各自的比例</span></span><br><span class="line">dt_train[<span class="string">'Sex'</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>(male      577 female    314 Name: Sex, dtype: int64, &#39; &#39;, male      0.647587 female    0.352413 Name: Sex, dtype: float64)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dt_train[<span class="string">'Sex'</span>].value_counts(normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>male      0.647587female    0.352413Name: Sex, dtype: float64</code></pre><h3 id="关于缺失值"><a href="#关于缺失值" class="headerlink" title="关于缺失值"></a>关于缺失值</h3><p>对于数据中的缺失值如何处理是需要我们结合问题具体分析的。有时会用均值或中位数等统计值来填充，有的也可以用模型进行预测，对于缺失不是很严重的我们甚至可以直接丢弃。</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;大纲&quot;&gt;&lt;a href=&quot;#大纲&quot; class=&quot;headerlink&quot; title=&quot;大纲&quot;&gt;&lt;/a&gt;大纲&lt;/h3&gt;&lt;p&gt;&lt;code&gt;Titanic Survival Predictions&lt;/code&gt; 可以算是机器学习中的 &lt;code&gt;Hello World
      
    
    </summary>
    
    
    
      <category term="data analysis" scheme="https://hahally.github.io/tags/data-analysis/"/>
    
  </entry>
  
  <entry>
    <title>oneSimpleDL</title>
    <link href="https://hahally.github.io/articles/oneSimpleDL/"/>
    <id>https://hahally.github.io/articles/oneSimpleDL/</id>
    <published>2021-01-27T03:34:25.000Z</published>
    <updated>2021-01-27T03:37:09.922Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets, layers, models</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">f = np.load(<span class="string">'mnist.npz'</span>)</span><br><span class="line">f.files</span><br></pre></td></tr></table></figure><pre><code>[&#39;x_test&#39;, &#39;x_train&#39;, &#39;y_train&#39;, &#39;y_test&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取数据</span></span><br><span class="line">data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> f.files:</span><br><span class="line"></span><br><span class="line">    data[i] = f[i]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'x_test'</span>].shape,data[<span class="string">'x_train'</span>].shape,data[<span class="string">'y_train'</span>].shape,data[<span class="string">'y_test'</span>].shape,</span><br></pre></td></tr></table></figure><pre><code>((10000, 28, 28), (60000, 28, 28), (60000,), (10000,))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">plt.imshow(data[<span class="string">'x_train'</span>][<span class="number">0</span>]),data[<span class="string">'y_train'</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>(&lt;matplotlib.image.AxesImage at 0x1dcc0eb2860&gt;, 5)</code></pre><p><img src="/articles/oneSimpleDL/output_5_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">x_train = data[<span class="string">'x_train'</span>]/<span class="number">255.0</span></span><br><span class="line">y_train = data[<span class="string">'y_train'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">x_test = data[<span class="string">'x_test'</span>]/<span class="number">255.0</span></span><br><span class="line">y_test = data[<span class="string">'y_test'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加一个维度变成 [b,h,w,c]</span></span><br><span class="line">x_train = tf.expand_dims(x_train,axis=<span class="number">3</span>)</span><br><span class="line">x_test = tf.expand_dims(x_test,axis=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卷积网络 LeNet-5</span></span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积层</span></span><br><span class="line">model.add(layers.Conv2D(<span class="number">6</span>,(<span class="number">3</span>, <span class="number">3</span>), input_shape=(<span class="number">28</span>, <span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line"><span class="comment"># BN层</span></span><br><span class="line">model.add(layers.BatchNormalization())</span><br><span class="line"><span class="comment"># 池化层</span></span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>),strides=<span class="number">2</span>))</span><br><span class="line">model.add(layers.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积层</span></span><br><span class="line">model.add(layers.Conv2D(<span class="number">16</span>,(<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="comment"># BN层</span></span><br><span class="line">model.add(layers.BatchNormalization())</span><br><span class="line"><span class="comment"># 池化层</span></span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>),strides=<span class="number">2</span>))</span><br><span class="line">model.add(layers.ReLU())</span><br><span class="line"></span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">120</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">84</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential_2&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_2 (Conv2D)            (None, 26, 26, 6)         60        _________________________________________________________________batch_normalization_2 (Batch (None, 26, 26, 6)         24        _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 13, 13, 6)         0         _________________________________________________________________re_lu_2 (ReLU)               (None, 13, 13, 6)         0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 11, 11, 16)        880       _________________________________________________________________batch_normalization_3 (Batch (None, 11, 11, 16)        64        _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 5, 5, 16)          0         _________________________________________________________________re_lu_3 (ReLU)               (None, 5, 5, 16)          0         _________________________________________________________________flatten_1 (Flatten)          (None, 400)               0         _________________________________________________________________dense_3 (Dense)              (None, 120)               48120     _________________________________________________________________dense_4 (Dense)              (None, 84)                10164     _________________________________________________________________dense_5 (Dense)              (None, 10)                850       =================================================================Total params: 60,162Trainable params: 60,118Non-trainable params: 44_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置 from_logits=True 相当于在最后输出加了一个 softmax</span></span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train,</span><br><span class="line">                    y_train,</span><br><span class="line">                    epochs=<span class="number">10</span>,</span><br><span class="line">                    batch_size=<span class="number">200</span>,</span><br><span class="line">                    validation_data=(x_test, y_test))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.history[<span class="string">'accuracy'</span>], label=<span class="string">'accuracy'</span>)</span><br><span class="line">plt.plot(history.history[<span class="string">'val_accuracy'</span>], label = <span class="string">'val_accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.ylim([<span class="number">0.5</span>, <span class="number">1</span>])</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>10000/10000 - 2s - loss: 0.0346 - accuracy: 0.9896</code></pre><p><img src="/articles/oneSimpleDL/output_11_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_loss, test_acc</span><br></pre></td></tr></table></figure><pre><code>(0.034577180710506215, 0.9896)</code></pre><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpickle</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'rb'</span>) <span class="keyword">as</span> fo:</span><br><span class="line">        dict = pickle.load(fo, encoding=<span class="string">'bytes'</span>)</span><br><span class="line">    <span class="keyword">return</span> dict</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">'./cifar-10-python/cifar-10-batches-py/'</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.listdir(path)</span><br></pre></td></tr></table></figure><pre><code>[&#39;batches.meta&#39;, &#39;data_batch_1&#39;, &#39;data_batch_2&#39;, &#39;data_batch_3&#39;, &#39;data_batch_4&#39;, &#39;data_batch_5&#39;, &#39;readme.html&#39;, &#39;test_batch&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unpickle(<span class="string">'./cifar-10-python/cifar-10-batches-py/batches.meta'</span>)</span><br></pre></td></tr></table></figure><pre><code>{b&#39;label_names&#39;: [b&#39;airplane&#39;,  b&#39;automobile&#39;,  b&#39;bird&#39;,  b&#39;cat&#39;,  b&#39;deer&#39;,  b&#39;dog&#39;,  b&#39;frog&#39;,  b&#39;horse&#39;,  b&#39;ship&#39;,  b&#39;truck&#39;], b&#39;num_cases_per_batch&#39;: 10000, b&#39;num_vis&#39;: 3072}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x_train = []</span><br><span class="line">y_train = []</span><br><span class="line"></span><br><span class="line">x_test = []</span><br><span class="line">y_test = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(path):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'data'</span> <span class="keyword">in</span> i:</span><br><span class="line">        data_dict = unpickle(path + i)</span><br><span class="line">        x_train.append(data_dict[<span class="string">b'data'</span>].reshape(data_dict[<span class="string">b'data'</span>].shape[<span class="number">0</span>],</span><br><span class="line">                                                  <span class="number">3</span>, <span class="number">32</span>,</span><br><span class="line">                                                  <span class="number">32</span>).transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">        y_train = y_train + data_dict[<span class="string">b'labels'</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'test'</span> <span class="keyword">in</span> i:</span><br><span class="line">        test_dict = unpickle(path + i)</span><br><span class="line">        x_test.append(test_dict[<span class="string">b'data'</span>].reshape(test_dict[<span class="string">b'data'</span>].shape[<span class="number">0</span>],</span><br><span class="line">                                                 <span class="number">3</span>, <span class="number">32</span>,</span><br><span class="line">                                                 <span class="number">32</span>).transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">        y_test = y_test + test_dict[<span class="string">b'labels'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train = tf.constant(y_train)</span><br><span class="line">y_test = tf.constant(y_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train.shape,y_test.shape</span><br></pre></td></tr></table></figure><pre><code>(TensorShape([50000]), TensorShape([10000]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = tf.concat(x_train,axis=<span class="number">0</span>)</span><br><span class="line">x_test = tf.concat(x_test,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train.shape,x_test.shape</span><br></pre></td></tr></table></figure><pre><code>(TensorShape([50000, 32, 32, 3]), TensorShape([10000, 32, 32, 3]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VGG16</span></span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(</span><br><span class="line">    layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                  padding=<span class="string">"same"</span>,</span><br><span class="line">                  activation=<span class="string">'relu'</span>,</span><br><span class="line">                  input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">"same"</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># model.add(layers.Flatten())</span></span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d (Conv2D)              (None, 32, 32, 64)        1792      _________________________________________________________________conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     _________________________________________________________________conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    _________________________________________________________________conv2d_5 (Conv2D)            (None, 8, 8, 256)         590080    _________________________________________________________________conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         _________________________________________________________________conv2d_7 (Conv2D)            (None, 4, 4, 512)         1180160   _________________________________________________________________conv2d_8 (Conv2D)            (None, 4, 4, 512)         2359808   _________________________________________________________________conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         _________________________________________________________________conv2d_10 (Conv2D)           (None, 2, 2, 512)         2359808   _________________________________________________________________conv2d_11 (Conv2D)           (None, 2, 2, 512)         2359808   _________________________________________________________________conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 1, 1, 512)         0         _________________________________________________________________dense (Dense)                (None, 1, 1, 256)         131328    _________________________________________________________________dense_1 (Dense)              (None, 1, 1, 64)          16448     _________________________________________________________________dense_2 (Dense)              (None, 1, 1, 10)          650       =================================================================Total params: 14,863,114Trainable params: 14,863,114Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,</span><br><span class="line">    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">    x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">    batch_size=<span class="number">1</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    validation_data=(x_test, y_test) </span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
    
    
      <category term="DeepLearning" scheme="https://hahally.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Task5</title>
    <link href="https://hahally.github.io/articles/Task5/"/>
    <id>https://hahally.github.io/articles/Task5/</id>
    <published>2021-01-25T15:51:35.000Z</published>
    <updated>2021-01-25T15:54:55.697Z</updated>
    
    <content type="html"><![CDATA[<h2 id="任务说明"><a href="#任务说明" class="headerlink" title="任务说明"></a>任务说明</h2><ul><li>学习主题：作者关联（数据建模任务），对论文作者关系进行建模，统计最常出现的作者关系；</li><li>学习内容：构建作者关系图，挖掘作者关系</li><li>学习成果：论文作者知识图谱、图关系挖掘</li></ul><h2 id="数据处理步骤"><a href="#数据处理步骤" class="headerlink" title="数据处理步骤"></a>数据处理步骤</h2><p>将作者列表进行处理，并完成统计。具体步骤如下：</p><ul><li>将论文第一作者与其他作者（论文非第一作者）构建图；</li><li>使用图算法统计图中作者与其他作者的联系；</li></ul><h2 id="社交网络分析"><a href="#社交网络分析" class="headerlink" title="社交网络分析"></a>社交网络分析</h2><p>图是复杂网络研究中的一个重要概念。Graph是用<strong>点</strong>和<strong>线</strong>来刻画离散事物集合中的每对事物间以某种方式相联系的数学模型。Graph在现实世界中随处可见，如交通运输图、旅游图、流程图等。利用图可以描述现实生活中的许多事物，如用点可以表示交叉口，点之间的连线表示路径，这样就可以轻而易举的描绘出一个交通运输网络。</p><h3 id="图类型"><a href="#图类型" class="headerlink" title="图类型"></a>图类型</h3><ul><li><p>无向图，忽略了两节点间边的方向。</p></li><li><p>指有向图，考虑了边的有向性。</p></li><li><p>多重无向图，即两个结点之间的边数多于一条，又允许顶点通过同一条边和自己关联。</p></li></ul><h3 id="图统计指标"><a href="#图统计指标" class="headerlink" title="图统计指标"></a>图统计指标</h3><ul><li><p>度：是指和该节点相关联的边的条数，又称关联度。对于有向图，节点的入度 是指进入该节点的边的条数；节点的出度是指从该节点出发的边的条数；</p></li><li><p>迪杰斯特拉路径：.从一个源点到其它各点的最短路径，可使用迪杰斯特拉算法来求最短路径；</p></li><li><p>连通图：在一个无向图 G 中，若从顶点i到顶点j有路径相连，则称i和j是连通的。如果 G 是有向图，那么连接i和j的路径中所有的边都必须同向。如果图中任意两点都是连通的，那么图被称作连通图。如果此图是有向图，则称为强连通图。</p></li></ul><p>对于其他图算法，可以在networkx和igraph两个库中找到。</p><h2 id="具体代码以及讲解"><a href="#具体代码以及讲解" class="headerlink" title="具体代码以及讲解"></a>具体代码以及讲解</h2><p>首先读取我们想要的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的package</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment">#用于画图</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="comment">#用于爬取arxiv的数据</span></span><br><span class="line"><span class="keyword">import</span> re <span class="comment">#用于正则表达式，匹配字符串的模式</span></span><br><span class="line"><span class="keyword">import</span> requests <span class="comment">#用于网络连接，发送网络请求，使用域名获取对应信息</span></span><br><span class="line"><span class="keyword">import</span> json <span class="comment">#读取数据，我们的数据为json格式的</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#数据处理，数据分析</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#画图工具</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readArxivFile</span><span class="params">(path, columns=[<span class="string">'id'</span>, <span class="string">'submitter'</span>, <span class="string">'authors'</span>, <span class="string">'title'</span>, <span class="string">'comments'</span>, <span class="string">'journal-ref'</span>, <span class="string">'doi'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="string">'report-no'</span>, <span class="string">'categories'</span>, <span class="string">'license'</span>, <span class="string">'abstract'</span>, <span class="string">'versions'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="string">'update_date'</span>, <span class="string">'authors_parsed'</span>], count=None)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义读取文件的函数</span></span><br><span class="line"><span class="string">        path: 文件路径</span></span><br><span class="line"><span class="string">        columns: 需要选择的列</span></span><br><span class="line"><span class="string">        count: 读取行数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    data  = []</span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'r'</span>) <span class="keyword">as</span> f: </span><br><span class="line">        <span class="keyword">for</span> idx, line <span class="keyword">in</span> enumerate(f): </span><br><span class="line">            <span class="keyword">if</span> idx == count:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line">            d = json.loads(line)</span><br><span class="line">            d = &#123;col : d[col] <span class="keyword">for</span> col <span class="keyword">in</span> columns&#125;</span><br><span class="line">            data.append(d)</span><br><span class="line"></span><br><span class="line">    data = pd.DataFrame(data)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">data = readArxivFile(<span class="string">'arxiv-metadata-oai-snapshot.json'</span>, </span><br><span class="line">                     [<span class="string">'id'</span>, <span class="string">'authors_parsed'</span>],</span><br><span class="line">                    <span class="number">200000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure><p><div></div></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>authors_parsed</th>      <th>id</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>      <td>0704.0001</td>    </tr>    <tr>      <th>1</th>      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>      <td>0704.0002</td>    </tr>    <tr>      <th>2</th>      <td>[[Pan, Hongjun, ]]</td>      <td>0704.0003</td>    </tr>    <tr>      <th>3</th>      <td>[[Callan, David, ]]</td>      <td>0704.0004</td>    </tr>    <tr>      <th>4</th>      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>      <td>0704.0005</td>    </tr>  </tbody></table><p>创建作者链接的无向图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx </span><br><span class="line"><span class="comment"># 创建无向图</span></span><br><span class="line">G = nx.Graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只用五篇论文进行构建</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> data.iloc[:<span class="number">5</span>].itertuples():</span><br><span class="line">    authors = row[<span class="number">1</span>]</span><br><span class="line">    authors = [<span class="string">' '</span>.join(x[:<span class="number">-1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> authors]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第一个作者 与 其他作者链接</span></span><br><span class="line">    <span class="keyword">for</span> author <span class="keyword">in</span> authors[<span class="number">1</span>:]:</span><br><span class="line">        G.add_edge(authors[<span class="number">0</span>],author) <span class="comment">#　添加节点２，３并链接２３节点</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将作者关系图进行绘制：</span></span><br><span class="line">nx.draw(G, with_labels=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="/articles/Task5/output_6_0.png" alt="png"></p><p>如果我们500片论文构建图，则可以得到更加完整作者关系，并选择最大联通子图进行绘制，折线图为子图节点度值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> data.iloc[:<span class="number">500</span>].itertuples():</span><br><span class="line">    authors = row[<span class="number">1</span>]</span><br><span class="line">    authors = [<span class="string">' '</span>.join(x[:<span class="number">-1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> authors]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第一个作者 与 其他作者链接</span></span><br><span class="line">    <span class="keyword">for</span> author <span class="keyword">in</span> authors[<span class="number">1</span>:]:</span><br><span class="line">        G.add_edge(authors[<span class="number">0</span>],author) <span class="comment">#　添加节点２，３并链接２３节点</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">degree_sequence = sorted([d <span class="keyword">for</span> n, d <span class="keyword">in</span> G.degree()], reverse=<span class="literal">True</span>)</span><br><span class="line">dmax = max(degree_sequence)</span><br><span class="line"></span><br><span class="line">plt.loglog(degree_sequence, <span class="string">"b-"</span>, marker=<span class="string">"o"</span>)</span><br><span class="line">plt.title(<span class="string">"Degree rank plot"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"degree"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"rank"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># draw graph in inset</span></span><br><span class="line">plt.axes([<span class="number">0.45</span>, <span class="number">0.45</span>, <span class="number">0.45</span>, <span class="number">0.45</span>])</span><br><span class="line">Gcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=<span class="literal">True</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">pos = nx.spring_layout(Gcc)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">nx.draw_networkx_nodes(Gcc, pos, node_size=<span class="number">20</span>)</span><br><span class="line">nx.draw_networkx_edges(Gcc, pos, alpha=<span class="number">0.4</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/articles/Task5/output_9_0.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;任务说明&quot;&gt;&lt;a href=&quot;#任务说明&quot; class=&quot;headerlink&quot; title=&quot;任务说明&quot;&gt;&lt;/a&gt;任务说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;学习主题：作者关联（数据建模任务），对论文作者关系进行建模，统计最常出现的作者关系；&lt;/li&gt;
&lt;li&gt;学习内容
      
    
    </summary>
    
    
    
      <category term="data analysis" scheme="https://hahally.github.io/tags/data-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Task4</title>
    <link href="https://hahally.github.io/articles/Task4/"/>
    <id>https://hahally.github.io/articles/Task4/</id>
    <published>2021-01-22T13:58:19.000Z</published>
    <updated>2021-01-22T14:01:10.538Z</updated>
    
    <content type="html"><![CDATA[<h2 id="任务说明"><a href="#任务说明" class="headerlink" title="任务说明"></a>任务说明</h2><ul><li>学习主题：论文分类（数据建模任务），利用已有数据建模，对新论文进行类别分类；</li><li>学习内容：使用论文标题完成类别分类；</li><li>学习成果：学会文本分类的基本方法、<code>TF-IDF</code>等；</li></ul><h2 id="数据处理步骤"><a href="#数据处理步骤" class="headerlink" title="数据处理步骤"></a>数据处理步骤</h2><p>在原始arxiv论文中论文都有对应的类别，而论文类别是作者填写的。在本次任务中我们可以借助论文的标题和摘要完成：</p><ul><li>对论文标题和摘要进行处理；</li><li>对论文类别进行处理；</li><li>构建文本分类模型；</li></ul><h2 id="文本分类思路"><a href="#文本分类思路" class="headerlink" title="文本分类思路"></a>文本分类思路</h2><ul><li>思路1：TF-IDF+机器学习分类器</li></ul><p>直接使用TF-IDF对文本提取特征，使用分类器进行分类，分类器的选择上可以使用SVM、LR、XGboost等</p><ul><li>思路2：FastText</li></ul><p>FastText是入门款的词向量，利用Facebook提供的FastText工具，可以快速构建分类器</p><ul><li>思路3：WordVec+深度学习分类器</li></ul><p>WordVec是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可以选择TextCNN、TextRnn或者BiLSTM。</p><ul><li>思路4：Bert词向量</li></ul><p>Bert是高配款的词向量，具有强大的建模学习能力。</p><h2 id="具体代码实现以及讲解"><a href="#具体代码实现以及讲解" class="headerlink" title="具体代码实现以及讲解"></a>具体代码实现以及讲解</h2><p>为了方便大家入门文本分类，我们选择思路1和思路2给大家讲解。首先完成字段读取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的package</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment">#用于画图</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="comment">#用于爬取arxiv的数据</span></span><br><span class="line"><span class="keyword">import</span> re <span class="comment">#用于正则表达式，匹配字符串的模式</span></span><br><span class="line"><span class="keyword">import</span> requests <span class="comment">#用于网络连接，发送网络请求，使用域名获取对应信息</span></span><br><span class="line"><span class="keyword">import</span> json <span class="comment">#读取数据，我们的数据为json格式的</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#数据处理，数据分析</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#画图工具</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readArxivFile</span><span class="params">(path, columns=[<span class="string">'id'</span>, <span class="string">'submitter'</span>, <span class="string">'authors'</span>, <span class="string">'title'</span>, <span class="string">'comments'</span>, <span class="string">'journal-ref'</span>, <span class="string">'doi'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="string">'report-no'</span>, <span class="string">'categories'</span>, <span class="string">'license'</span>, <span class="string">'abstract'</span>, <span class="string">'versions'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="string">'update_date'</span>, <span class="string">'authors_parsed'</span>], count=None)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义读取文件的函数</span></span><br><span class="line"><span class="string">        path: 文件路径</span></span><br><span class="line"><span class="string">        columns: 需要选择的列</span></span><br><span class="line"><span class="string">        count: 读取行数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    data  = []</span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'r'</span>) <span class="keyword">as</span> f: </span><br><span class="line">        <span class="keyword">for</span> idx, line <span class="keyword">in</span> enumerate(f): </span><br><span class="line">            <span class="keyword">if</span> idx == count:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line">            d = json.loads(line)</span><br><span class="line">            d = &#123;col : d[col] <span class="keyword">for</span> col <span class="keyword">in</span> columns&#125;</span><br><span class="line">            data.append(d)</span><br><span class="line"></span><br><span class="line">    data = pd.DataFrame(data)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">data = readArxivFile(<span class="string">'arxiv-metadata-oai-snapshot.json'</span>, </span><br><span class="line">                     [<span class="string">'id'</span>, <span class="string">'title'</span>, <span class="string">'categories'</span>, <span class="string">'abstract'</span>],</span><br><span class="line">                    <span class="number">200000</span>)</span><br></pre></td></tr></table></figure><p>为了方便数据的处理，我们可以将标题和摘要拼接一起完成分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'text'</span>] = data[<span class="string">'title'</span>] + data[<span class="string">'abstract'</span>]</span><br><span class="line"></span><br><span class="line">data[<span class="string">'text'</span>] = data[<span class="string">'text'</span>].apply(<span class="keyword">lambda</span> x: x.replace(<span class="string">'\n'</span>,<span class="string">' '</span>))</span><br><span class="line">data[<span class="string">'text'</span>] = data[<span class="string">'text'</span>].apply(<span class="keyword">lambda</span> x: x.lower())</span><br><span class="line">data = data.drop([<span class="string">'abstract'</span>, <span class="string">'title'</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>由于原始论文有可能有多个类别，所以也需要处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多个类别，包含子分类</span></span><br><span class="line">data[<span class="string">'categories'</span>] = data[<span class="string">'categories'</span>].apply(<span class="keyword">lambda</span> x : x.split(<span class="string">' '</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单个类别，不包含子分类</span></span><br><span class="line">data[<span class="string">'categories_big'</span>] = data[<span class="string">'categories'</span>].apply(<span class="keyword">lambda</span> x : [xx.split(<span class="string">'.'</span>)[<span class="number">0</span>] <span class="keyword">for</span> xx <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure><p>然后将类别进行编码，这里类别是多个，所以需要多编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MultiLabelBinarizer</span><br><span class="line">mlb = MultiLabelBinarizer()</span><br><span class="line">data_label = mlb.fit_transform(data[<span class="string">'categories_big'</span>].iloc[:])</span><br></pre></td></tr></table></figure><h3 id="思路1"><a href="#思路1" class="headerlink" title="思路1"></a>思路1</h3><p>思路1使用TFIDF提取特征，限制最多4000个单词：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">vectorizer = TfidfVectorizer(max_features=<span class="number">4000</span>)</span><br><span class="line">data_tfidf = vectorizer.fit_transform(data[<span class="string">'text'</span>].iloc[:])</span><br></pre></td></tr></table></figure><p>由于这里是多标签分类，可以使用sklearn的多标签分类进行封装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和验证集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data_tfidf, data_label,</span><br><span class="line">                                                 test_size = <span class="number">0.2</span>,random_state = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建多标签分类模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.multioutput <span class="keyword">import</span> MultiOutputClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">clf = MultiOutputClassifier(MultinomialNB()).fit(x_train, y_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(y_test, clf.predict(x_test)))</span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.95      0.85      0.89      7925           1       0.85      0.79      0.82      7339           2       0.77      0.72      0.74      2944           3       0.00      0.00      0.00         4           4       0.72      0.48      0.58      2123           5       0.51      0.66      0.58       987           6       0.86      0.38      0.52       544           7       0.71      0.69      0.70      3649           8       0.76      0.61      0.68      3388           9       0.85      0.88      0.87     10745          10       0.46      0.13      0.20      1757          11       0.79      0.04      0.07       729          12       0.45      0.35      0.39       507          13       0.54      0.36      0.43      1083          14       0.69      0.14      0.24      3441          15       0.84      0.20      0.33       655          16       0.93      0.16      0.27       268          17       0.87      0.43      0.58      2484          18       0.82      0.38      0.52       692   micro avg       0.81      0.65      0.72     51264   macro avg       0.70      0.43      0.50     51264weighted avg       0.80      0.65      0.69     51264 samples avg       0.72      0.72      0.70     51264</code></pre><p>​    </p><pre><code>d:\program files\python\lib\site-packages\sklearn\metrics\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.  _warn_prf(average, modifier, msg_start, len(result))d:\program files\python\lib\site-packages\sklearn\metrics\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.  _warn_prf(average, modifier, msg_start, len(result))d:\program files\python\lib\site-packages\sklearn\metrics\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.  _warn_prf(average, modifier, msg_start, len(result))d:\program files\python\lib\site-packages\sklearn\metrics\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.  _warn_prf(average, modifier, msg_start, len(result))</code></pre><h3 id="思路2"><a href="#思路2" class="headerlink" title="思路2"></a>思路2</h3><p>思路2使用深度学习模型，单词进行词嵌入然后训练。将数据集处理进行编码，并进行截断：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data[<span class="string">'text'</span>].iloc[:<span class="number">100000</span>],</span><br><span class="line">                                                    data_label[:<span class="number">100000</span>],</span><br><span class="line">                                                    test_size=<span class="number">0.95</span>,</span><br><span class="line">                                                    random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># parameter</span></span><br><span class="line">max_features= <span class="number">500</span></span><br><span class="line">max_len= <span class="number">150</span></span><br><span class="line">embed_size=<span class="number">100</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"></span><br><span class="line">tokens = Tokenizer(num_words = max_features)</span><br><span class="line">tokens.fit_on_texts(list(data[<span class="string">'text'</span>].iloc[:<span class="number">100000</span>]))</span><br><span class="line"></span><br><span class="line">y_train = data_label[:<span class="number">100000</span>]</span><br><span class="line">x_sub_train = tokens.texts_to_sequences(data[<span class="string">'text'</span>].iloc[:<span class="number">100000</span>])</span><br><span class="line">x_sub_train = sequence.pad_sequences(x_sub_train, maxlen=max_len)</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><p>定义模型并完成训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LSTM model</span></span><br><span class="line"><span class="comment"># Keras Layers:</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D<span class="comment"># Keras Callback Functions:</span></span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Callback</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping,ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> initializers, regularizers, constraints, optimizers, layers, callbacks</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">sequence_input = Input(shape=(max_len, ))</span><br><span class="line">x = Embedding(max_features, embed_size, trainable=<span class="literal">True</span>)(sequence_input)</span><br><span class="line">x = SpatialDropout1D(<span class="number">0.2</span>)(x)</span><br><span class="line">x = Bidirectional(GRU(<span class="number">128</span>, return_sequences=<span class="literal">True</span>,dropout=<span class="number">0.1</span>,recurrent_dropout=<span class="number">0.1</span>))(x)</span><br><span class="line">x = Conv1D(<span class="number">64</span>, kernel_size = <span class="number">3</span>, padding = <span class="string">"valid"</span>, kernel_initializer = <span class="string">"glorot_uniform"</span>)(x)</span><br><span class="line">avg_pool = GlobalAveragePooling1D()(x)</span><br><span class="line">max_pool = GlobalMaxPooling1D()(x)</span><br><span class="line">x = concatenate([avg_pool, max_pool]) </span><br><span class="line">preds = Dense(<span class="number">19</span>, activation=<span class="string">"sigmoid"</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(sequence_input, preds)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=Adam(lr=<span class="number">1e-3</span>),metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_sub_train, y_train, </span><br><span class="line">          batch_size=batch_size, </span><br><span class="line">          validation_split=<span class="number">0.2</span>,</span><br><span class="line">          epochs=epochs)</span><br></pre></td></tr></table></figure><pre><code>C:\Users\ACER\AppData\Roaming\Python\Python35\site-packages\tensorflow_core\python\framework\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.  &quot;Converting sparse IndexedSlices to a dense Tensor of unknown shape. &quot;C:\Users\ACER\AppData\Roaming\Python\Python35\site-packages\tensorflow_core\python\framework\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.  &quot;Converting sparse IndexedSlices to a dense Tensor of unknown shape. &quot;Train on 80000 samples, validate on 20000 samplesEpoch 1/580000/80000 [==============================] - 844s 11ms/step - loss: 0.1697 - accuracy: 0.9456 - val_loss: 0.1262 - val_accuracy: 0.9571Epoch 2/580000/80000 [==============================] - 858s 11ms/step - loss: 0.1221 - accuracy: 0.9582 - val_loss: 0.1144 - val_accuracy: 0.9599Epoch 3/580000/80000 [==============================] - 833s 10ms/step - loss: 0.1131 - accuracy: 0.9599 - val_loss: 0.1097 - val_accuracy: 0.9610Epoch 4/580000/80000 [==============================] - 807s 10ms/step - loss: 0.1087 - accuracy: 0.9610 - val_loss: 0.1055 - val_accuracy: 0.9624Epoch 5/580000/80000 [==============================] - 791s 10ms/step - loss: 0.1170 - accuracy: 0.9613 - val_loss: 0.1047 - val_accuracy: 0.9623&lt;keras.callbacks.callbacks.History at 0x2a50566ef98&gt;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;任务说明&quot;&gt;&lt;a href=&quot;#任务说明&quot; class=&quot;headerlink&quot; title=&quot;任务说明&quot;&gt;&lt;/a&gt;任务说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;学习主题：论文分类（数据建模任务），利用已有数据建模，对新论文进行类别分类；&lt;/li&gt;
&lt;li&gt;学习内容：使用论
      
    
    </summary>
    
    
    
      <category term="data analysis" scheme="https://hahally.github.io/tags/data-analysis/"/>
    
  </entry>
  
</feed>

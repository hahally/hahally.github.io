<!DOCTYPE html>


  <html class="light page-post">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>GMAN | Hahally&#39;s BLOG</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Hahally,Django后端,Python,深度学习,机器学习,Hexo主题" />
  

  <meta name="description" content="GMAN: A Graph Multi-Attention Network for Traffic Prediction  Abstract由于交通系统的复杂性和众多影响因素的不断变化，长期交通预测具有很大的挑战性。本文针对时空因素，提出了一种基于图的多注意网络（GMAN）来预测路网图上不同位置时间步长头部的交通状况。GMAN采用编解码器架构，其中编码器和解码器均由多个时空注意模块组成，以模拟时">
<meta property="og:type" content="article">
<meta property="og:title" content="GMAN">
<meta property="og:url" content="https://hahally.github.io/articles/GMAN/index.html">
<meta property="og:site_name" content="Hahally&#39;s BLOG">
<meta property="og:description" content="GMAN: A Graph Multi-Attention Network for Traffic Prediction  Abstract由于交通系统的复杂性和众多影响因素的不断变化，长期交通预测具有很大的挑战性。本文针对时空因素，提出了一种基于图的多注意网络（GMAN）来预测路网图上不同位置时间步长头部的交通状况。GMAN采用编解码器架构，其中编码器和解码器均由多个时空注意模块组成，以模拟时">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210223150430150.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210301090744336.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210301104231532.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302084718874.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302084736425.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302085935418.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302095459275.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302204132568.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302205242473.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302205254514.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302221757111.png">
<meta property="og:image" content="https://hahally.github.io/articles/GMAN/image-20210302222011345.png">
<meta property="article:published_time" content="2021-03-02T14:27:22.000Z">
<meta property="article:modified_time" content="2021-03-02T14:32:51.690Z">
<meta property="article:author" content="hahally">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hahally.github.io/articles/GMAN/image-20210223150430150.png">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    
<link rel="stylesheet" href="/css/personal-style.css">

  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Hahally's BLOG" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-Work"><span class="toc-text">Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#交通预测"><span class="toc-text">交通预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#图深度学习"><span class="toc-text">图深度学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#注意力机制"><span class="toc-text">注意力机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Preliminaries"><span class="toc-text">Preliminaries</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Problem-Studied"><span class="toc-text">Problem Studied</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Graph-Multi-Attention-Network"><span class="toc-text">Graph Multi-Attention Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Spatio-Temporal-Embedding"><span class="toc-text">Spatio-Temporal Embedding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ST-Attention-Block"><span class="toc-text">ST-Attention Block</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spatial-Attention"><span class="toc-text">Spatial Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Temporal-Attention"><span class="toc-text">Temporal Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gated-Fusion"><span class="toc-text">Gated Fusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transform-Attention"><span class="toc-text">Transform Attention</span></a></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-GMAN" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">GMAN</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2021.03.02</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>hahally</span>
        </span>
      

      


      
        <span>
          <i class="icon-comment"></i>
          <a href="https://hahally.github.io/articles/GMAN/#disqus_thread"></a>
        </span>
      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <blockquote>
<p><a href="https://arxiv.org/abs/1911.08415" target="_blank" rel="noopener">GMAN: A Graph Multi-Attention Network for Traffic Prediction</a></p>
</blockquote>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>由于交通系统的复杂性和众多影响因素的不断变化，长期交通预测具有很大的挑战性。本文针对时空因素，提出了一种基于图的多注意网络（GMAN）来预测路网图上不同位置时间步长头部的交通状况。GMAN采用编解码器架构，其中编码器和解码器均由多个时空注意模块组成，以模拟时空因素对交通状况的影响。 编码器对输入的流量特征进行编码，解码器预测输出序列。在编码器和解码器之间，应用变换注意层来转换编码的流量特征，以生成未来时间步长的序列表示作为解码器的输入。 变换注意机制对历史步骤和将来时间步骤之间的直接关系进行建模，这有助于减轻预测时间步骤之间的错误传播问题。 在两个实际交通预测任务（即交通量预测和交通速度预测）上的实验结果证明了GMAN的优越性。 特别是，在提前1小时的预测中，GMAN的MAE指标提高了4％，优于最新技术。<a href="https://github.com/zhengchuanpan/GMAN" target="_blank" rel="noopener">https://github.com/zhengchuanpan/GMAN</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>交通预测的目的是根据历史观测（如通过传感器记录）预测道路网络中未来的交通状况（如交通量或速度）。它在许多实际应用中扮演着重要的角色。例如，准确的交通预测可以帮助运输机构更好地控制交通，以减少交通拥挤。</p>
<p>附近位置的交通状况的预测会相互影响。 为了捕获这种空间相关性，卷积神经网络（CNN）被广泛使用。 同时，一个地点的交通状况也与其历史观测值有关。循环神经网络（RNN）被广泛应用于这种对时间相关性问题进行建模。</p>
<p>由于交通条件受路网图的限制，近年来的研究将交通预测看作图形建模问题。利用图卷积网络（GCN），这些研究在短期（提前5∼15分钟）交通预测方面取得了不错的结果。然而，长期(提前几个小时)的交通预测在文献中仍然缺乏令人满意的进展，主要是由于以下挑战。</p>
<p>1) 复杂的时空相关性</p>
<ul>
<li>动态空间相关性。如图1所示，路网中传感器之间交通状况的相关性随着时间的推移而显著变化（例如，在高峰时间之前和期间）。如何动态选择相关传感器的数据来预测目标传感器的长期交通状况是一个具有挑战性的问题。</li>
<li>非线性时间相关。 同样在图1中，传感器处的交通状况可能急剧且突然地波动（例如，由于事故），从而影响不同时间步长之间的相关性。 当时间更远时，如何自适应地对非线性时间相关性建模仍然是一个挑战。</li>
</ul>
<p>2) 对误差传播的敏感性。从长期的角度来看，当对未来的预测更进一步时，每个时间步上的小误差可能会放大。这样的误差传播使得对遥远未来的预测非常具有挑战性。</p>
<p><img src="/articles/GMAN/image-20210223150430150.png" alt="image-20210223150430150"></p>
<p>为了应对上述挑战，我们提出了一种图形多注意网络（GMAN）来预测未来一段时间内道路网络图上的交通状况。此处，交通状况指的是对交通系统的观测，可以以数值形式报告 。 为了便于说明，我们将重点放在交通量和交通速度预测上，尽管我们的模型可以应用于其他数字交通数据的预测。</p>
<p>GMAN遵循编码器-解码器体系结构，其中编码器对输入的流量特征进行编码，而解码器预测输出序列。 在编码器和解码器之间添加了一个转换注意层，以转换编码的历史流量特征以生成将来的表示。编码器和解码器都由ST-Attention块的堆栈组成。 每个ST-Attention块由用于对动态空间相关性进行建模的空间注意机制，用于对非线性时间相关性进行建模的时间注意机制以及通过自适应融合时空表示的融合融合机制形成。变换注意力机制模型直接控制了历史和未来时间步长之间的关系，从而减轻了错误传播的影响。在两个真实数据集上的实验证实，GMAN具有最先进的性能。</p>
<p>这项工作的贡献概述如下:</p>
<ul>
<li>我们分别提出了空间和时间的注意机制来模拟动态的空间和非线性的时间相关。此外，我们还设计了一种自适应融合时空注意机制提取信息的门控融合方法。</li>
<li>提出了一种将历史交通特征转化为未来交通特征的注意力转化机制。该注意机制建立了历史时间步长与未来时间步长之间的直接关系模型，以缓解错误传播问题。</li>
<li>我们在两个实际流量数据集上对我们的图形多注意网络(GMAN)进行了评估，并且在1小时的预测中观察到了比最先进的基线方法提高4% 的改进和优越的容错能力。</li>
</ul>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="交通预测"><a href="#交通预测" class="headerlink" title="交通预测"></a>交通预测</h4><p>在过去的几十年中，交通预测得到了广泛的研究。与传统的时间序列方法（如自动回归综合移动影像匹配(ARIMA)）和机器学习模型（如支持向量回归(SVR)、 k 最近邻(KNN)）相比，深度学习方法（例如，长-短期记忆（LSTM））在捕获交通状况下的时间相关性方面表现出更优越的性能。为了建立空间相关性模型，研究人员应用卷积神经网络(CNN)来捕捉欧氏空间中的相关性。最近的研究制定了基于图的交通预测，并使用图卷积网络(GCN)建模道路网络中的非欧氏关联。这些基于图的模型通过一步一步的方法在预测前生成多个步骤，并可能受到不同预测步骤之间的误差传播的影响。</p>
<h4 id="图深度学习"><a href="#图深度学习" class="headerlink" title="图深度学习"></a>图深度学习</h4><p>将神经网络泛化为图结构化数据是一个新兴的话题。一系列研究概括了CNN，以在图谱或空间角度上对任意图形建模。另一类研究集中在图嵌入上，它学习保留图结构信息的顶点的低维表示将 WaveNet 集成到 GCN 中以进行时空建模。由于它学习静态邻接矩阵，因此该方法难以捕获动态空间相关性。</p>
<h4 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h4><p>由于注意力机制的高效性和建模依赖性的灵活性，其注意力机制已广泛应用于各个领域。注意机制的核心思想是根据输入的数据自适应地关注最相关的特征。最近，研究人员将注意力机制应用于图形结构化数据，来建模图形分类的空间相关性。我们将注意力机制扩展到图形时空数据预测。</p>
<h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>我们把道路网络表示为一个加权有向图 $ G=(V,E,A) $ 。这里，V 是 N = | V | 的顶点集合，代表道路网络上的点（例如交通传感器）。E 是一组边，代表顶点之间的连通性。$A \in R^{N \times N}$ 表示加权邻接矩阵。其中 $A_{v_i,v_j}$ 代表两个顶点之间的接近程度（由道路网络距离度量）。在时间步长为 t 的交通状况用图$G$ 上的图信号 $X_t \in R^{N \times C}$ 表示，其中 $C$ 是感兴趣的交通状况的数量（例如交通量，交通速度等。）</p>
<h4 id="Problem-Studied"><a href="#Problem-Studied" class="headerlink" title="Problem Studied"></a>Problem Studied</h4><p>给定在顶点 N 处的历史 P 时间步长 $X = (X_{t_1},X_{t_2},…,X_{t_P}) \in R^{P\times N \times C}$ ，我们的目标是预测所有顶点在下一个 Q 时间步长的交通情况，表示为 $\hat Y = (\hat X_{t_{P+1}},\hat X_{t_{P+2}},…,\hat X_{t_{P+Q}}) \in R^{Q\times N \times C}$ 。</p>
<h3 id="Graph-Multi-Attention-Network"><a href="#Graph-Multi-Attention-Network" class="headerlink" title="Graph Multi-Attention Network"></a>Graph Multi-Attention Network</h3><p>图2显示了我们提出的 GMAN 框架，包含一个 encoder-decoder 结构。编码器和解码器都包含 L 个残差连接的 STAtt Block。每一个 STAtt Block 都是由具有门控功能的时空注意机制组成的。在编码器与解码器之间，网络中加入了一个 <strong>transform</strong> 注意层将编码的交通特征转换为解码特征。我们还通过时空嵌入（STE）将图形结构和时间信息整合到多注意机制中。此外，为了方便残差连接，所有层输出维度都是 D。接下来将详细介绍这些模块。</p>
<p><img src="/articles/GMAN/image-20210301090744336.png" alt="image-20210301090744336"></p>
<h4 id="Spatio-Temporal-Embedding"><a href="#Spatio-Temporal-Embedding" class="headerlink" title="Spatio-Temporal Embedding"></a>Spatio-Temporal Embedding</h4><p>由于交通条件的演变受到了道路网络的限制，将道路网信息整合到预测模型中至关重要。为此，我们提出了一种将顶点编码为向量的局部嵌入方法，以保存图形结构信息。具体来说，我们利用 <strong>node2vec</strong> 方法来学习顶点表示。此外，为了协同训练预训练好的整个模型的向量，这些向量会被输入到一个两层完全连接的神经网络中。然后，我们将得到空间嵌入，表示为 $e^S_{v_i} \in R^D, v_i \in V$ 。</p>
<p>空间嵌入只能提供静态的表示，不能反映路网中交通传感器之间的动态相关性。因此，我们进一步提出了一种时间嵌入方法，将每个时间步编码成一个向量。具体来说，把一天分成 T 个时间步长。我们使用独热编码将每一个时间步长的 <em>day-of-week</em> 和 <em>time-of-day</em> 编码到 $R^7$ 和 $R^{T+7}$ 中，接下来，我们应用一个两层完全连接的神经网络将时间特征转化为向量。在我们的模型中，我们嵌入了历史 P 和未来 Q 时间步长的时间特征，表示为$e^T_{t_j} \in R^D$ ，其中$t_j = t_1,…,t_P,…,t_{P+Q}$ 。为了获得随时间变化的顶点表示，我们将上述空间嵌入和时间嵌入融合为时空嵌入（STE），如图2b所示。具体来说，对于时间步长 $t_j$ 时的顶点 $v_i$ ，其 STE 被定义为 $e_{v_i,t_j} = e^S_{v_i} + e^T_{t_j}$ 。因此，在 P+Q 时间步长中，N 个顶点的 STE 表示为 $E \in R^{(P+Q)\times N \times D}$ 。STE包含图形结构和时间信息，可用于空间、时间和 transform 注意机制。</p>
<h4 id="ST-Attention-Block"><a href="#ST-Attention-Block" class="headerlink" title="ST-Attention Block"></a>ST-Attention Block</h4><p>如图 2c 所示，ST-Attention 模块包含一个空间注意机制，一个时间注意机制和一个门控机制。我们将第 $l$ 个 block 的输入表示为 $H^{(l-1)}$ ，其中在步长$t_j$ 处顶点$v_i$ 的隐藏状态表示为 $h^{(l-1)}<em>{v_i,v_j}$ 。在第 $l$ 个 block 中，空间和时间注意机制的输出表示为 $H^{(l)}_S$ 和 $H^{(l)}_T$ ，其中在步长$t_j$ 处顶点$v_i$ 的隐藏状态表示为 $hs^{(l)}</em>{v_i,v_j},ht^{(l)}_{vi,tj}$ 。通过门控融合，得到了该模块的输出结果，表示为 $H^{(l)}$ 。</p>
<p>为了便于说明，我们将非线性变换表示为：<br>$$<br>f(x) = ReLU(xW+b),<br>$$</p>
<h4 id="Spatial-Attention"><a href="#Spatial-Attention" class="headerlink" title="Spatial Attention"></a>Spatial Attention</h4><p>条道路的交通状况受到其他道路的不同影响。这种影响是动态的，随着时间的推移而变化。为了模拟这些特性，我们设计了一种空间注意机制来自适应地捕捉道路网络中传感器之间的相关性。其关键思想是在不同的时间步动态地将不同的权重分配给不同的顶点（例如，传感器），如图3所示。对于在时间步长 $t_j$ 处的顶点 $v_i$ ，我们计算所有顶点的加权和：<br>$$<br>hs^{(l)}<em>{v_i,v_j} = \sum</em>{v\in v} \alpha_{v_i,v} · h^{(l-1)}<em>{v,t_j} ,<br>$$<br>表示 $V$ 所有顶点的集合， $\alpha</em>{v_i,v}$ 为注意机制得分，表示顶点 $v$ 到 $v_i$ 之间的重要程度，其分数之和等于 1：$\sum_{v \in V} \alpha_{v_i,v} = 1$</p>
<p><img src="/articles/GMAN/image-20210301104231532.png" alt="image-20210301104231532"></p>
<p>在一定的时间步长下，当前交通状况和路网结构都会影响传感器之间的相关性。例如，道路上的拥堵可能会严重影响其相邻道路的交通状况。基于这种直觉，我们同时考虑交通特征和图形结构来学习注意力得分。具体来说，我们将隐藏状态与时空嵌入连接起来，并采用标度点积方法计算顶点 $v_i$ 与 $V$ 之间的相关性。<br>$$<br>s_{v_i,v} = \frac{&lt;h_{v_i,t_j}^{(l-1)}||e_{v_i,t_j},h^{(l-1)}<em>{v,t_j}||e</em>{v,t_j}&gt;}{\sqrt{2D}},<br>$$<br>其中 $||$ 表示连接操作，$〈•,•〉$ 表示内积操作，2D 是$h_{v_i,t_j}^{(l-1)}||e_{v_i,t_j}$ 的维度。然后，通过 Softmax 归一化为：<br>$$<br>\alpha_{v_i,v} = \frac{exp(s_{v_i,v})}{\sum_{v_r \in V}exp(s_{v_i,v_r})}<br>$$<br>在获得注意得分 $\alpha_{v_i,v}$ 之后，可以通过等式2更新隐藏状态。</p>
<p>为了稳定学习过程，我们将空间注意力机制扩展为 <strong>multi-head</strong>。 具体来说，我们将K个并行注意机制与不同的可学习预测联系起来：</p>
<p><img src="/articles/GMAN/image-20210302084718874.png" alt="image-20210302084718874"></p>
<p><img src="/articles/GMAN/image-20210302084736425.png" alt="image-20210302084736425"></p>
<p>其中 $f^{(k)}<em>{s,1}(·),f^{(k)}</em>{s,2}(·),f^{(k)}_{s,3}(·)$ 表示在第 k 个 <strong>head attention</strong> 中三个不同的非线性预测，输出维度 $d = D/K$ 。</p>
<p>当顶点数量 N 很大时，由于我们需要计算 $N^2$ 个注意分数，因此时间和内存消耗都很大。 为了解决此限制，我们进一步提出了一个<em>group spatial attention</em>，其中包含了<em>intra-group spatial attention</em>和<em>inter-group spatial attention</em>，如图4所示。</p>
<p><img src="/articles/GMAN/image-20210302085935418.png" alt="image-20210302085935418"></p>
<p>我们随机将 N 个顶点划分为 G 个组，其中每个组包含 $M=N/G$ 个顶点。在每一组中，我们通过方程5、6、7 计算 <em>intra-group attention</em> 以模拟顶点之间的局部空间相关性，其中组间共享参数。然后，我们在每个组中应用最大池化方法，以获得每个组的单个表示。接下来，我们计算<em>inter-group spatial attention</em>来建模不同组之间的相关性，为每个组产生一个全局特征。局部特征被添加到相应的全局特征中，作为最终输出。</p>
<p>在<em>group spatial attention*上，我们需要计算每个时间步长的  $GM^2 + G^2 = NM+(N/M)^2$ 个 *attention scores</em> 。梯度为零时，我们知道 $M = \sqrt[3]{2N}$ 时 <em>attention score</em> 的值达到最小值 $2^{-1/3}N^{4/3}\ll N^2 $ 。</p>
<h4 id="Temporal-Attention"><a href="#Temporal-Attention" class="headerlink" title="Temporal Attention"></a>Temporal Attention</h4><p>一个地点的交通状况与它之前的观察结果是相关的，并且这种相关性随着时间步长的变化是非线性的。为了模拟这些特性，我们设计了一个 <em>Temporal Attention</em> 来自适应地模拟不同时间步骤之间的非线性关联，如图5所示。注意，时间相关性受交通条件和相应的时间背景的影响。例如，发生在早晨高峰时间的拥堵可能会影响几个小时的交通。因此，我们同时考虑流量特征和时间来度量不同时间步长之间的相关性。具体来说，我们将隐藏状态与时空嵌入相连接，并采用<em>multi-head*方法计算 *attention score</em> 。最后考虑顶点 $v_i$ ，时间步长 $t_j$ 与 $t$ 之间的相关性定义为：</p>
<p><img src="/articles/GMAN/image-20210302095459275.png" alt="image-20210302095459275"></p>
<p>其中 $u^{(k)}<em>{t_j,t}$ 表示时间步长 $t_j$ 与 $t$ 之间的相关性，$\beta^{(k)}</em>{t_j,t}$ 表示第 k 个 <em>head attention score</em> 表明时间步骤 $t$ 到$t_j$ 的重要性，$f^{(k)}<em>{t,1},f^{(k)}</em>{t,2}$ 表示两个不同的可学习的 <em>transforms</em> ，$N_{t_j}$ 表示时间步长 $t_j$ 之前的集合。仅考虑时间步中早于目标步的信息以启用因果关系(<strong>causality</strong>)。一旦获得 <em>attention score</em> 后，顶点 $v_i$ 在时间步长 $t_j$ 处 的 <em>hidden state</em> 个更新方式如下：</p>
<p><img src="/articles/GMAN/image-20210302204132568.png" alt="image-20210302204132568"></p>
<p>其中 $f^{(k)}_{t,3}$ 代表一个非线性投影。公式8、9和10中的可学习参数通过并行计算在所有顶点和时间步上共享。</p>
<h4 id="Gated-Fusion"><a href="#Gated-Fusion" class="headerlink" title="Gated Fusion"></a>Gated Fusion</h4><p>道路在某一特定时间步长下的交通状况与其前期值和其他道路的交通状况相关。如图2c所示，我们设计了门控融合来自适应地融合空间和时间表示。第 $l$ 个block ，时间和空间注意机制的输出表示为 $H^{(l)}_S,H^{(l)}_T$ ，两者在编码器与解码器中都有相同的形状 $R^{P\times N \times D},R^{Q\times N \times D}$ ，融合公式如下：</p>
<p><img src="/articles/GMAN/image-20210302205242473.png" alt="image-20210302205242473"></p>
<p><img src="/articles/GMAN/image-20210302205254514.png" alt="image-20210302205254514"></p>
<p>其中 $W_{z,1}\in R^{D\times D},W_{z,2}\in R^{D\times D},b_z \in R^D$ 是可学习参数，$\odot$ 表示按元素计算的乘积，$\sigma(·)$ 表示 sigmoid 激活函数，z 是门控。门控融合机制自适应地控制空间和时间依赖在每个顶点和时间步长的流。</p>
<h4 id="Transform-Attention"><a href="#Transform-Attention" class="headerlink" title="Transform Attention"></a>Transform Attention</h4><p>为了减轻长时间范围内不同预测时间步长之间的误差传播效应，在编码器和解码器之间增加了一个<em>Transform Attention*。它对每个未来时间步长和每个历史时间步长之间的直接关系进行建模，以转换已编码的交通特征，以生成未来表示，作为解码器的输入。如图6 所示，对顶点 $v_i$ ，通过 *spatio-temporal</em> 嵌入计算预测时间步长 $t_j(t_j = t_{P+1},…,t_{P+Q})$ 与历史时间步长 $t(t = t_1,..,t_P)$ 之间的相关性。</p>
<p><img src="/articles/GMAN/image-20210302221757111.png" alt="image-20210302221757111"></p>
<p>$\gamma^{(k)}_{t_j,t}$ 为 <em>attention score</em> ，通过在所有历史时间步骤中自适应地选择相关特征，将编码的流量特征转换到解码器。</p>
<p><img src="/articles/GMAN/image-20210302222011345.png" alt="image-20210302222011345"></p>
<p>方程13、14和15可以在所有顶点和时间步骤中并行计算，共享可学习的参数。</p>

    
  </div>

     <div id="gitalk-container"></div>
     

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: 'ee979f0e6d62adec6722',
  clientSecret: 'af19f8bdcad202e18bd00c4b65105281776a7570',
  repo: 'hahally.github.io',
  owner: 'hahally',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  // id: location.pathname.split('/').pop().substring(0, 49),
  id: md5(location.pathname),
  admin: ['hahally'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->




</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">请转给我女朋友</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/articles/Traffic-Network-Flow-Prediction/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/articles/Problem-Container/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>



  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    
  <section class="disqus-comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
    </div>
  </section>

  <script>
    var disqus_shortname = 'forsigner';
    
    var disqus_url = 'https://hahally.github.io/articles/GMAN/';
    
    (function(){
      var dsq = document.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>

  <script id="dsq-count-scr" src="//forsigner.disqus.com/count.js" async></script>



    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: 'ee979f0e6d62adec6722',
  clientSecret: 'af19f8bdcad202e18bd00c4b65105281776a7570',
  repo: 'hahally.github.io',
  owner: 'hahally',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  // id: location.pathname.split('/').pop().substring(0, 49),
  id: md5(location.pathname),
  admin: ['hahally'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>

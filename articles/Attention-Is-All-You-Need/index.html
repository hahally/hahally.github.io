<!DOCTYPE html>


  <html class="light page-post">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Attention-Is-All-You-Need | Hahally&#39;s BLOG</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="paper reading," />
  

  <meta name="description" content="Attention Is All You Need “懂的都懂”  Bib TeX  @inproceedings{NIPS2017_3f5ee243,author &#x3D; {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and K">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention-Is-All-You-Need">
<meta property="og:url" content="https://hahally.github.io/articles/Attention-Is-All-You-Need/index.html">
<meta property="og:site_name" content="Hahally&#39;s BLOG">
<meta property="og:description" content="Attention Is All You Need “懂的都懂”  Bib TeX  @inproceedings{NIPS2017_3f5ee243,author &#x3D; {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and K">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hahally.github.io/articles/Attention-Is-All-You-Need/image-20210715161152396.png">
<meta property="og:image" content="https://hahally.github.io/articles/Attention-Is-All-You-Need/image-20210715171719397.png">
<meta property="og:image" content="https://hahally.github.io/articles/Attention-Is-All-You-Need/image-20210715172114776.png">
<meta property="article:published_time" content="2021-07-15T10:30:21.000Z">
<meta property="article:modified_time" content="2021-07-15T10:37:34.721Z">
<meta property="article:author" content="hahally">
<meta property="article:tag" content="paper reading">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hahally.github.io/articles/Attention-Is-All-You-Need/image-20210715161152396.png">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    
<link rel="stylesheet" href="/css/personal-style.css">

  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Hahally's BLOG" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#前言"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型架构"><span class="toc-text">模型架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention"><span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Position-wise-Feed-Forward-Networks"><span class="toc-text">Position-wise Feed-Forward Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-text">总结</span></a></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-Attention-Is-All-You-Need" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">Attention-Is-All-You-Need</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2021.07.15</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>hahally</span>
        </span>
      

      


      
        <span>
          <i class="icon-comment"></i>
          <a href="https://hahally.github.io/articles/Attention-Is-All-You-Need/#disqus_thread"></a>
        </span>
      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <blockquote>
<p><a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">Attention Is All You Need</a></p>
<p>“懂的都懂”</p>
</blockquote>
<p><strong>Bib TeX</strong></p>
<blockquote>
<p>@inproceedings{NIPS2017_3f5ee243,<br>author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},<br>booktitle = {Advances in Neural Information Processing Systems},<br>editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},<br>pages = {},<br>publisher = {Curran Associates, Inc.},<br>title = {Attention is All you Need},<br>url = {<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}" target="_blank" rel="noopener">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}</a>,<br>volume = {30},<br>year = {2017}<br>}</p>
</blockquote>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><em>Attention is All you Need</em> 这篇论文是Google的又一神作。传统的 encoder-decoder序列转化模型架构主要以卷积和循环神经网络为基础。在引入 attention 后，其模型效果达到最佳。除了 <strong>cnn</strong> 或 <strong>rnn</strong> 进行编码解码，Google团队提出了一个更加简单有效的模型，即大名鼎鼎的  <strong>transformer</strong> 。正如论文标题所说，该模型的encoder和decoder都将基于attention机制实现。</p>
<blockquote>
<p> We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</p>
</blockquote>
<p>【PS：想来<em>simple</em> 一词 似乎有些凡尔赛的意味，正所谓大道至简，用在这篇论文似乎也毫不违和。相比于堆叠卷积层的 <em>cnn</em> 亦或是递归的 <em>rnn</em> 来说，确实 simple。不过标题着实有些“狂”了，可能也只有Google的名气才配的上吧，换做别人估计被喷惨了/滑稽】</p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p><img src="/articles/Attention-Is-All-You-Need/image-20210715161152396.png" alt="image-20210715161152396"></p>
<p><strong>Encoder</strong></p>
<p>这一部分由多个(N=6)相同的层堆叠而成。每一层又包括两个子层，对应图中部分。第一个是一个多头自注意力机制[<em>a multi-head self-attention</em>]，第二个是一个全连接的前馈神经网络[a position-wise feed-forward networks]。这两个子层通过残差连接（residual connection ）。所有子层和embedding 层的输出维度都统一为512（便于进行残差连接）。</p>
<p><strong>Decoder</strong></p>
<p>同样地，解码器也由多个(N=6)相同的层堆叠而成。每层包括三个子层。在编码器的子层基础上增加了一个<em>masked multi-head self-attention</em> 。三个子层通过残差连接起来。</p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>论文中对 attention定义如下：<br>$$<br>Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>Q, k, V即quary、key、value对应的向量。$Q\in R^{n\times d_k},K\in R^{m\times d_k},V\in R^{d_v}$。</p>
<p>两种常见的注意力机制是：additive attention 和 dot-product attention ，显然从公式中可见使用的是后者。其中$\sqrt{d_k}$ 起到了缩放的作用，使得softmax能更好的work。并且命名为 Scaled Dot-Product Attention：</p>
<p><img src="/articles/Attention-Is-All-You-Need/image-20210715171719397.png" alt="image-20210715171719397"></p>
<p><strong>Multi-Head Attention</strong></p>
<p><img src="/articles/Attention-Is-All-You-Need/image-20210715172114776.png" alt="image-20210715172114776"></p>
<p>Multi-Head Attention其实就是多个Scaled Dot-Product Attention并行，然后将结果 concat 拼接。从公式来看：<br>$$<br>MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^O\<br>Head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)<br>$$<br>其中，$W_i^Q\in R^{d_{model}\times d_k},W_i^K\in R^{d_{model}\times d_k},W_i^V\in R^{d_{model}\times d_v},W^O\in R^{hd_v\times d_{model}}$ 。本文采用的self-attention，即取Q，K，V相同。</p>
<h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>进行非线性变换：也就是 linear 后面加个 ReLU而已。<br>$$<br>FFN(X)=max(0,xW_1+b_1)W_2+b_2<br>$$</p>
<blockquote>
<p>While the linear transformations are the same across different positions, they use different parameters from layer to layer.   Another way of describing this is as two convolutions with kernel size 1.</p>
</blockquote>
<p>【PS：Another way of describing this is as <em>two convolutions with kernel size 1</em> /惊！看来还是有借鉴cnn的卷积思想嘛。/滑稽】</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>这一部分的作用就是为了引入序列中的顺序信息，或者说位置信息。positional encodings（位置编码） 与input embeddings（如：词向量） 具有相同的维度$d_{model}$ 。通过将两向量相加，从而将对应的位置信息嵌入进去。位置编码的方式如下：<br>$$<br>PE(pos,2i)=sin(pos/10000^{2i/d_{model}})\<br>PE(pos,2i+1)=cos(pos/10000^{2i/d_{model}})<br>$$<br>其中，$pos$ 表示位置，$i$ 是维度，这样，每个位置编码都对应一个正弦信号。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>抛弃cnn和rnn后，仅仅使用attention显然是不够的，为了捕获输入序列中词向量之间的前后位置信息，不得不引入Positional Encoding。不过很好解决了传统encoder-decoder无法并行训练的问题。因此，transformer 训练速度要快得多。transformer模型在nlp任务上的非凡表现，也引来了其他人将其应用在其他任务上的思考，目前在其他领域上，也出现了各种花里胡哨的transformer改版。</p>
<p><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">官方开源代码</a></p>
<p>这里给上苏神大佬对《Attention is All You Need》的解读：<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码） - 科学空间|Scientific Spaces (kexue.fm)</a></p>

    
  </div>

     <div id="gitalk-container"></div>
     

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: 'ee979f0e6d62adec6722',
  clientSecret: 'af19f8bdcad202e18bd00c4b65105281776a7570',
  repo: 'hahally.github.io',
  owner: 'hahally',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  // id: location.pathname.split('/').pop().substring(0, 49),
  id: md5(location.pathname),
  admin: ['hahally'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->




</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持hahally</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>



  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    
  <section class="disqus-comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
    </div>
  </section>

  <script>
    var disqus_shortname = 'forsigner';
    
    var disqus_url = 'https://hahally.github.io/articles/Attention-Is-All-You-Need/';
    
    (function(){
      var dsq = document.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>

  <script id="dsq-count-scr" src="//forsigner.disqus.com/count.js" async></script>



    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: 'ee979f0e6d62adec6722',
  clientSecret: 'af19f8bdcad202e18bd00c4b65105281776a7570',
  repo: 'hahally.github.io',
  owner: 'hahally',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  // id: location.pathname.split('/').pop().substring(0, 49),
  id: md5(location.pathname),
  admin: ['hahally'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>

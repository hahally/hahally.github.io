<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lora:300,300italic,400,400italic,700,700italic|Dancing Script:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hahally.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="I know nothing but my ignorance...">
<meta property="og:type" content="website">
<meta property="og:title" content="Hahally&#39;s BLOG">
<meta property="og:url" content="https://hahally.github.io/index.html">
<meta property="og:site_name" content="Hahally&#39;s BLOG">
<meta property="og:description" content="I know nothing but my ignorance...">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Hahally">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hahally.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Hahally's BLOG</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hahally's BLOG" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hahally's BLOG</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">只想做个无关紧要的副词。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/hahally" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/%E4%BC%91%E6%81%AF%E4%B8%80%E4%B8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/%E4%BC%91%E6%81%AF%E4%B8%80%E4%B8%8B/" class="post-title-link" itemprop="url">休息一下</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-04 00:09:25" itemprop="dateCreated datePublished" datetime="2024-01-04T00:09:25+08:00">2024-01-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-26 09:05:45" itemprop="dateModified" datetime="2025-04-26T09:05:45+08:00">2025-04-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">杂记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>2023年6月，印象中长沙的夏天总是闷热，让人透不过气来，今年也不例外。</p>
<p>现在回想起来，仿佛隔了很久很久，像翻开一本历史书中的某一页。</p>
</blockquote>
<p>一晃六月就到了，彼时研二即将结束。</p>
<p>那些天，我开始疯狂地投简历找暑假实习，大概中旬的时候进了一家小公司。</p>
<p>入职第一天中午吃饭的时候，大家在互相吐槽。K突然问我：会不会下午就跑了？我犹豫了一下，回答他：先干着吧。</p>
<p>是啊，先干着吧，投了很多份简历都石沉大海，目前这家公司做的事又刚好和我专业对口，再找下去夏天该结束了吧。最重要的原因可能是不想待在学校写论文，和导师之间有着说不清道不明的隔阂，也没有很想要去沟通，我选择逃避，所以跑出来实习了。</p>
<p>不久前，她才找我谈过话，十多分钟下来，我全程敷衍地点头简单回应。我记得窗外蝉鸣很吵很吵，可我并不知道它们在吵什么，否则我应该很乐意加入其中吧。</p>
<p>没过几天，我便发消息告诉她，我找了一个暑期实习。实习之后，我把论文的事暂且抛在了脑后。</p>
<p>在我上班第二天，来了一个女生L，做产品经理实习。至此，这个公司达到人数最多的时候。6个实习生加一个人事和一个会计。人事我们叫王哥，会计我们喊廖姐。两个人坐在最里面靠窗那一排，平常也不说话。</p>
<p>当时我们正以公司的名义参加一个比赛，而赛题与我前不久做过的比赛类似，于是我直接把原来的方案拿过来了，索性取得了不错的成绩，老板脸色也明显好了一点。</p>
<p>上班的时候，我们几个实习生还能时不时嘻嘻哈哈几下，小蜜蜂上也经常发些表情包。K和我一个学校大我一届，那时他经常在小蜜蜂上喊我们休息一下。我左边是后端大佬，不是在debug就是在与环境作斗争。L开始那几天经常被老板喊去打电话。我对面的留学生负责查找比赛相关的资料，我训练模型，K精致的后处理，我们调侃称这是公司的核心科技。</p>
<p>六月下旬，我们就这样还算愉快的过去了，我们也以第三名的成绩进入了复赛。</p>
<p>30号那天，刚好周五。后端大佬已经和老板谈离职了，中午我们一起吃了顿散伙饭，后端大佬说想去外面的地方看看。那天下午快下班的时候，我们被老板喊进办公室开会，拖延了十多分钟，出来时，后端大佬已经先走了。那天下班回学校的路上还挺感伤的，平常都是一起去地铁站，今天突然少了一个人，而且明天也是，心里莫名有些空落落的。</p>
<p>后来，看朋友圈得知，后端大佬去了一趟南宁旅游，再后来去深圳上班，那里应该就是他说的外面的地方。</p>
<p>周末结束，七月第一天上班，那天天气格外的燥热。K一大早就来了，在办公室和老板谈话。印象里谈了很久，我以为他也要走了，突然一下子觉得上班没有一点意思了。最后老板卖惨挽留他，磨了他很久，他才转正继续留下。后面又招了一个产品和一个后端接替之前的工作。新来的后端不怎么说话，平常一个人摸鱼就在那玩纸牌游戏。</p>
<p>某一天突然又觉得生活变得无趣起来。</p>
<p>每天七点多起床洗漱完出门。这个点学校里已经很热闹了，有人在打篮球，有人在打太极，有人在跳舞……在校门口买个早餐，边走边吃，到地铁口差不多八点左右。早高峰确实有点吓人，一号线经常是满的，偶尔有个空位，我站进去刚好塞满。六号线换乘，人来人往，匆匆忙忙的脚步声回荡在站内，都是打工人在赶地铁。</p>
<p>记得来面试那天第一次觉得这个换乘站好大，换线都要五到十分钟。上班后，突然感觉这个站还是小了。有天换乘的时候坐反了，那天迟到了半个小时。出站后，还要走个十分钟左右才到公司。路边很多早餐店，有一个路口围坐一群大爷在那打牌。有家药店门口经常放一把椅子，椅子上栓了一只小泰迪。再往前有所小学，六月份的时候经常有摆摊卖小吃，之后转弯，过个路口，就差不多到公司所在的那栋楼了。到公司坐下，缓一缓，然后开始干活。中午吃完午饭，就趴桌子上睡午觉。下午差不多快两点继续干活。六点下班，K住附近，在路口与我们分开。去地铁路上，只有我，留学生和产品L三个人。我们总结了规律，如果六点下班，正常的话会赶上六点二十二或者二十四的地铁，而且在车头位置大概率能够抢到位置坐。第一个下车的是L，然后是我，最后是留学生。我需要转一号线，人就比较多了，出站后大概七点多，然后在路口等红绿灯，我会找辆共享电动车坐一会，时不时看看天，看看来往的车，看看聚集的人，看看对面的倒计时。晚饭选择并不多，大概率杀猪粉，其次黄焖鸡，偶尔炒饭，记得有段时间吃了一周的杀猪粉。吃完回到自习室休息一下大概就八点了。这段时间还参加了高校大数据挑战赛，所以回学校后，基本在做这个比赛，竞争很激烈，拿奖也比去年难，但是还是想试一下。</p>
<p>日子不紧不慢的过着，直到复赛的日子到来。那天收拾东西准备去南京参赛，一大早老板告诉我们票没有抢到，让我们自己购票。拖着行李出门到高铁站那一段时间有点煎熬。甚至都不想去了，后面疯狂抢票，买到了九点之前的，不过要从武汉换乘。折腾一早上，也总算坐上了高铁。</p>
<p>一段神奇的旅途就这样开始了……</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/%E5%9F%BA%E4%BA%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%96%87%E6%9C%AC%E5%93%88%E5%B8%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/%E5%9F%BA%E4%BA%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%96%87%E6%9C%AC%E5%93%88%E5%B8%8C/" class="post-title-link" itemprop="url">基于弱监督的深度语义文本哈希</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-17 20:36:27" itemprop="dateCreated datePublished" datetime="2022-01-17T20:36:27+08:00">2022-01-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-26 22:51:09" itemprop="dateModified" datetime="2022-01-26T22:51:09+08:00">2022-01-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>论文：Deep Semantic Text Hashing with Weak Supervision，SIGIR，2018</p>
</blockquote>
<p>论文提出一种弱监督学习方法。采用bm25对相似文档进行排序，提取数据中的弱监督信号。先训练一个可以得到整个文档的语义向量表示的模型，然后根据语义向量，运用一些规则（设置阈值）将对应维度变成0或1。</p>
<ul>
<li>通过使用无监督排序来逼近真实的文档空间，从而弥补了标记数据的不足。</li>
<li>设计了两个深度生成模型来利用文档的内容和估计的邻域来学习语义哈希函数。（NbrReg和NbrReg+doc）</li>
</ul>
<p>两个语义向量表示模型（NbrReg和NbrReg+doc）区别在于是否利用了近邻文档信息。每个模型包含两个部分：encoder、decoder。</p>
<p>该方法步骤包括三个部分：Document Space Estimation —&gt; NbrReg（NbrReg+doc） —&gt; Binarization</p>
<ul>
<li>Document Space Estimation：得到整个文档数据的空间分布情况</li>
</ul>
<p>在有标签信息的情况下，可以得到真实文档空间分布。没有标签信息的时候，利用bm25为每个文档 d 检索出一组与之最相似的近邻文档NN(d)。论文假设：近邻文档中大多数与文档 d 具有相同标签，因此任何文档的二进制哈希值在相近的向量空间模型中应该更加近似。</p>
<ul>
<li>NbrReg：语义向量模型</li>
</ul>
<p>文档语义向量 s ，满足标准正态分布 N(0,1)</p>
<p> $w_i \in d$ ，概率 $P_A(w_i|s)$  ； $\hat w_j \in NN(d)$ ,概率$P_B(\hat w_j|s)$</p>
<p>定义联合概率： $P(d) = \prod_{i}P_A(w_i|s)$ ，$P(NN(d))=\prod_{j}P_B(\hat w_j|s)$</p>
<p>目标函数：最大化$P(d,NN(d)) = P(d)P(NN(d))$  </p>
<script type="math/tex; mode=display">
logP(d,NN(d)) = log\int_{s}P(d|s)P(NN(d)|s)P(s)ds\\\geq E_{Q(s|·)}[logP(dd|s)] + E_{Q(s|·)}[logP(NN(d)|s)]-D_{KL}(Q(s|·)||P(s))</script><p>其中 $Q(d|·)$ 表示从数据中学到的近似后验概率分布；<strong>·</strong> 符号表示输入随机变量的占位符；$D_{KL}$ 表示KL散度；</p>
<p><strong>Decoder Function</strong></p>
<script type="math/tex; mode=display">
P(d) = \prod_{i}P_A(w_i|s)=\prod_{i}\frac{exp(s^TAe_i)}{exp(\sum_{j}s^TAe_j)}</script><p>$e_j$ 表示一个词袋向量，矩阵A将语义向量s映射到词编码空间。$P(NN(d))$ 与上面类似，只是映射矩阵用B表示。</p>
<p><strong>Encoder Function</strong></p>
<p>定义 $Q(s|·)$ 为文档d参数化的正态分布：$Q(s|·) = N(s,f(d))$ 。<em>f(·)</em> 函数将d表示为均值为$\mu$ 标准差为$\sigma$ 正态分布的向量。 为了表征两个参数，定义$f = <f_{\mu},f_{\sigma}>$  ，相当于定义了两个前馈神经网络：</f_{\mu},f_{\sigma}></p>
<script type="math/tex; mode=display">
f_{\mu}(d) = W_{\mu}·h(d)+b_{\mu} \\f_{\sigma}(d) = W_{\sigma}·h(d)+b_{\sigma}\\h(d) = relu(W_2·relu(W_1·d+b_1)+b_2)</script><p>语义向量s从Q中采样：</p>
<script type="math/tex; mode=display">
s \sim Q(s|d)=N(s;\mu=f_{\mu}(d),\sigma = f_{\sigma}(d))</script><ul>
<li>Utilize Neighbor Documents：(NbrReg+Doc）</li>
</ul>
<p>论文中提到相邻文档使用的一组单词可以表示该区域所有文档的主题，但是来自相邻文档的额外的词可能会引入噪声，混淆模型。为了削减噪声带来的影响，引入了一层隐藏层，用该层向量来表示近邻文档，使用一个平均池化层得到 近邻文档的中心表示。只有编码器部分有所不同，其他与NbrReg一致。</p>
<script type="math/tex; mode=display">
Z^{NN} = relu(W_2^{NN}·relu(W_1^{NN}·NN(d)+b_1^{NN})+b_2)\\h_{NN}(NN(d)) = mean(Z^{NN})\\f_{\mu}(d,NN(d)) = W_{\mu}·(h(d)+h_{NN}(NN(d)))+b_{\mu}</script><ul>
<li>Binarization</li>
</ul>
<p>根据编码器 $Q(s|·)$ 为文档d生成一个连续的语义向量。论文中使用编码器输出的正态分布的均值来表示语义向量 $\overline s = E[Q(s|·)]$，然后使用中值法生成二进制编码。若大于该阈值就令该位为1，否者为0.</p>
<blockquote>
<p>思考</p>
</blockquote>
<p>论文并没有显示道德直接学习二进制表示，而是通过训练一个语义模型，假设语义相近文档对应二进制表示应该相近，然后通过语义向量进一步转化为二进制哈希值。值得一提的是语义向量是服从正太分布的，一方面便于训练，另一方面也可以给模型提供很好的可解释性，所有文档可以映射到正态分布的语义空间，语义相近的向量具有相近的分布值（论文假设语义向量服从正太分布，并用其均值表示），这也确保了二值化的时候语义相近的文档在映射为二进制哈希值后也保持距离相近。</p>
<blockquote>
<p>开源代码</p>
</blockquote>
<p>github上找到两处开源代码，一个是作者的低调开源，一个是路人甲的好心复现。</p>
<ul>
<li><p>作者开源：<a href="https://github.com/unsuthee/SemanticHashingWeakSupervision" target="_blank" rel="noopener">https://github.com/unsuthee/SemanticHashingWeakSupervision</a></p>
</li>
<li><p>复现代码：<a href="https://github.com/yfy-/nbrreg" target="_blank" rel="noopener">https://github.com/yfy-/nbrreg</a></p>
</li>
</ul>
<p>作者开源的代码，一言难尽，虽然很贴心的把对比模型也复现了出来，但是数据没给，如何用bm25算法处理的过程都给省去了。于是找到了一个好心人提供了nbrreg模型的复现，而且给了一份数据，以及对数据进行处理的代码。但是模型训练没有考虑到用gpu的情况。所以下面主要对复现代码进行分析。</p>
<p><strong>数据处理</strong></p>
<p>提供的数据是20newsgroups数据集，20ng-all-stemmed.txt：18820行，20个类别</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alt.atheism	alt atheism faq atheist resourc archiv <span class="built_in">name</span> atheism resourc alt atheism...</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<p>格式为：label    w1 w2 w3…，一行为一条数据，由标签和对应文档组成，文档由一个空格分开的词组成。</p>
<p>数据处理代码为：prepare_data.py</p>
<ul>
<li>输入：20ng-all-stemmed.txt中的文本</li>
<li>输出：train_docs、cv_docs、test_docs、train_cats、cv_cats、test_cats、train_knn<ul>
<li>train_docs、cv_docs、test_docs：分别为训练集、验证集、测试集，维度为vocab_size。</li>
<li>train_cats、cv_cats、test_cats：对应标签，one-hot向量，维度为20。</li>
<li>train_knn：train_docs中每条数据的近邻文档的索引。</li>
</ul>
</li>
</ul>
<p>这部分代码主要是得到用于模型输入的数据，即将文本数据用数值表示。这里将每个文档用bm25权重值表示。BM25是信息索引领域用来计算query与文档相似度得分的经典算法。论文中使用bm25检索近邻文档，作为训练的弱监督信号。</p>
<p>BM25的一般公式：</p>
<script type="math/tex; mode=display">
Score(Q,d) = \sum_{i=1}^{n}W_i*R(q_i,d)</script><p>$Q$表示一个query，$q_i$  表示$Q$中的单词，$d$表示某个搜索文档。$W_i$ 表示单词权重，用$idf$ 表示：</p>
<script type="math/tex; mode=display">
idf(q_i) = log\frac{N-df_i+0.5}{df_i+0.5}</script><p>$df_i$ 为包含了$q_i$ 的文档个数。依据IDF的作用，对于某个 $q_i$，包含 $q_i$的文档数越多，说明$q_i$重要性越小，或者区分度越低，IDF越小，因此IDF可以用来刻画$q_i$与文档的相似性。</p>
<p>$R(q_i,d)$ 表示为：</p>
<script type="math/tex; mode=display">
R(q_i,d) = \frac{(k_1+1)·f(q_i,d)}{f(q_i,d)+k_1·(1-b+b·\frac{|d|}{avgdL})}</script><p>$f(q_i,d)$ 表示$q_i$在文档 d 中的词频，$|d|$ 表示文档 d的长度，avgdL是语料库全部文档的平均长度。$k_1$ 和 $b$ 为经验参数，一般的$k_1\in [1.2,2.0],b=0.75$</p>
<p>假设一共有 n 个文档，按照该公式计算最终一个文档 d 会得到 n 个得分。但是代码中计算的是$Score(d,d)$ ，而且没有求和操作。所以一个文档 d 会由一个vocab_size维度大小的向量表示。按照论文要求，会根据 n 个得分进行降序排列，选 k 个作为文档 d 的近邻文档$NN(d)$ 。复现的代码中则是根据上述向量计算余弦相似度然后选取近邻文档的。</p>
<p><img src="/articles/%E5%9F%BA%E4%BA%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%96%87%E6%9C%AC%E5%93%88%E5%B8%8C/image-20220126205246354.png" alt="image-20220126205246354"></p>
<p>其中$term_freq$ 对应词频$f(q_i,d)$ 的$n\times vocab_size$ 大小的矩阵，$cosin_similarity(train_docs)$ 计算文档与文档之间的余弦相似度得分。<strong>代码中近邻文档选取了100个</strong> 。</p>
<p>计算idf值代码：</p>
<p><img src="/articles/%E5%9F%BA%E4%BA%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%96%87%E6%9C%AC%E5%93%88%E5%B8%8C/image-20220126210125077.png" alt="image-20220126210125077"></p>
<p>这一处分母应该是$(df+0.5)$ 。少了一个括号！！！</p>
<p>模型训练测试代码都在一个文件里：nbrreg.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NbrReg</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lex_size, bit_size=<span class="number">32</span>, h_size=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(NbrReg, self).__init__()</span><br><span class="line">        self.lnr_h1 = torch.nn.Linear(lex_size, h_size)</span><br><span class="line">        self.lnr_h2 = torch.nn.Linear(h_size, h_size)</span><br><span class="line">        self.lnr_mu = torch.nn.Linear(h_size, bit_size)</span><br><span class="line">        self.lnr_sigma = torch.nn.Linear(h_size, bit_size)</span><br><span class="line">        self.lnr_rec_doc = torch.nn.Linear(bit_size, lex_size)</span><br><span class="line">        self.lnr_nn_rec_doc = torch.nn.Linear(bit_size, lex_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, docs)</span>:</span></span><br><span class="line">        mu, sigma = self.encode(docs)</span><br><span class="line">        <span class="comment"># qdist表示语义向量s，服从正态分布 N~(mu,sigma^2)</span></span><br><span class="line">        qdist = tdist.Normal(mu, sigma)</span><br><span class="line">        log_prob_words, log_nn_prob_words = self.decode(qdist.rsample())</span><br><span class="line">        <span class="keyword">return</span> qdist, log_prob_words, log_nn_prob_words</span><br><span class="line">	</span><br><span class="line">    <span class="comment"># 对应论文中的编码函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, docs)</span>:</span></span><br><span class="line">        relu = torch.nn.ReLU()</span><br><span class="line">        sigmoid = torch.nn.Sigmoid()</span><br><span class="line">        hidden = relu(self.lnr_h2(relu(self.lnr_h1(docs))))</span><br><span class="line">        mu = self.lnr_mu(hidden)</span><br><span class="line">        <span class="comment"># Use sigmoid for positive standard deviation</span></span><br><span class="line">        sigma = sigmoid(self.lnr_sigma(hidden))</span><br><span class="line">        <span class="keyword">return</span> mu, sigma</span><br><span class="line">	<span class="comment"># 对应论文中解码函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, latent)</span>:</span></span><br><span class="line">        log_softmax = torch.nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">        log_prob_words = log_softmax(self.lnr_rec_doc(latent))</span><br><span class="line">        log_nn_prob_words = log_softmax(self.lnr_nn_rec_doc(latent))</span><br><span class="line">        <span class="keyword">return</span> log_prob_words, log_nn_prob_words</span><br></pre></td></tr></table></figure>
<p>模型部分按照论文中的描述，使前馈神经网络就可以实现。值得一提的是 $qdist$ 应该才是文中对应的服从正态分布的语义向量 s。但在生成二进制哈希值时，取的是编码器输出的均值。</p>
<p>训练代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_docs, train_cats, train_knn, cv_docs, cv_cats, bitsize=<span class="number">32</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          epoch=<span class="number">30</span>, bsize=<span class="number">100</span>, lr=<span class="number">1e-3</span>, latent_size=<span class="number">1000</span>, resume=None,</span></span></span><br><span class="line"><span class="function"><span class="params">          imp_trial=<span class="number">0</span>)</span>:</span></span><br><span class="line">    nsize, lexsize = train_docs.shape</span><br><span class="line">    num_iter = int(np.ceil(nsize / bsize))</span><br><span class="line">    model = resume <span class="keyword">if</span> resume <span class="keyword">else</span> NbrReg(lexsize, bitsize, h_size=latent_size)</span><br><span class="line">    model.double()</span><br><span class="line">    optim = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    norm = tdist.Normal(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    best_prec = <span class="number">0.0</span></span><br><span class="line">    trial = <span class="number">0</span></span><br><span class="line">    epoch_range = itertools.count() <span class="keyword">if</span> imp_trial <span class="keyword">else</span> epoch</span><br><span class="line">    epoch = <span class="string">"INF"</span> <span class="keyword">if</span> imp_trial <span class="keyword">else</span> epoch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> epoch_range:</span><br><span class="line">        model.train()</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter):</span><br><span class="line">            print(<span class="string">f"Epoch: <span class="subst">&#123;e + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epoch&#125;</span>, Iteration: <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_iter&#125;</span>"</span>,</span><br><span class="line">                  end=<span class="string">"\r"</span>)</span><br><span class="line">            batch_i = np.random.choice(nsize, bsize)</span><br><span class="line">            np_batch = train_docs[batch_i].todense()</span><br><span class="line">            doc_batch = torch.from_numpy(np_batch).double()</span><br><span class="line">            knn_batch = train_knn[batch_i]</span><br><span class="line">            optim.zero_grad()</span><br><span class="line">            qdist, log_prob_words, log_nn_prob_words = model(doc_batch)</span><br><span class="line">            doc_rl = doc_rec_loss(log_prob_words, doc_batch)</span><br><span class="line">            doc_nn_rl = doc_nn_rec_loss(log_nn_prob_words, knn_batch,train_docs)</span><br><span class="line">            kl_loss = tdist.kl_divergence(qdist, norm)</span><br><span class="line">            kl_loss = torch.mean(torch.sum(kl_loss, dim=<span class="number">1</span>))</span><br><span class="line">            loss = doc_rl + doc_nn_rl + kl_loss</span><br><span class="line">            losses.append(loss.item())</span><br><span class="line">            loss.backward()</span><br><span class="line">            optim.step()</span><br><span class="line">        avg_loss = np.mean(losses)</span><br><span class="line">        avg_prec = test(train_docs, train_cats, cv_docs, cv_cats, model)</span><br><span class="line">        best_prec = max(avg_prec, best_prec)</span><br><span class="line">        print(<span class="string">f"Epoch <span class="subst">&#123;e + <span class="number">1</span>&#125;</span>: Avg Loss: <span class="subst">&#123;avg_loss&#125;</span>, Avg Prec: <span class="subst">&#123;avg_prec&#125;</span>"</span>)</span><br><span class="line">        <span class="keyword">if</span> best_prec == avg_prec:</span><br><span class="line">            trial = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trial += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> trial == imp_trial:</span><br><span class="line">                print(<span class="string">f"Avg Prec could not be improved for <span class="subst">&#123;imp_trial&#125;</span> times, "</span></span><br><span class="line">                      <span class="string">"giving up training"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, best_prec</span><br></pre></td></tr></table></figure>
<p>没有使用GPU！！！<code>kl_loss = tdist.kl_divergence(qdist, norm)​</code> 计算KL散度。norm 为标准正态分布。</p>
<p>测试代码：</p>
<p><img src="/articles/%E5%9F%BA%E4%BA%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E6%96%87%E6%9C%AC%E5%93%88%E5%B8%8C/image-20220126214013570.png" alt="image-20220126214013570"></p>
<p>这里 k=100，表示近邻文档取100，这里为test进行二进制哈希映射后，根据汉明距离选取距离最近的k个，然后统计这k个中与test标签相同的数目，相同数目越大表示即准确率越大，模型效果越好。</p>
<blockquote>
<p>注意事项</p>
</blockquote>
<p>在使用该代码时，需要对数据处理成 20ng-all-stemmed.txt文件里的格式。然后用<code>prepare_data.py</code> 处理生成对应的<code>.mat</code> 文件。将源句子与其复述句标记为相同标签。</p>
<ul>
<li>固定种子，保证结果可复现。（基本操作）</li>
<li>计算 idf 时，把代码里的小错误纠正了。（分母加了括号）</li>
<li>去掉余弦相似度计算，在已知标签的情况下，近邻文档直接从标签相同的文档中取k个。（bm25已经名存实亡，文档向量用TF-IDF值效果差不多）</li>
<li>k值调整，代码中默认100，论文中说为50的时候准确率不在提升，真的是谜之操作。要根据实际情况而定，看每个源句子对应的复述句子的数量，如果k设置过大，则会引入大量噪声。<code>test</code> 函数中的k要与数据处理中的k保持一致，或者小于。（至关重要，不然准确率上不去，而且低到百分之零点几，k=2时，平均准确率有0.43+）</li>
<li>改成了可以使用gpu训练的代码。（至少可以快七倍）</li>
<li>解耦，把训练、测试、模型、数据处理分开。</li>
</ul>
<p>开始小数据训练，准确率很低。后面就增加数据，准确率依旧那样。开始以为bm25权重计算错误，然后发现代码中 idf 的计算与公式有出入。然后改正了，接着训练，效果还是不好。然后将两份代码对比，发现作者开源的代码里对KL散度值给了一个权重。然后又加权重值，效果还是那样。训练时开始调整knn-size的值，效果好了一点点，但还是很低很低。然后尝试解耦代码，把各个模块代码重新整理，然后发现<code>test</code> 函数里有个参数 k，默认值100，训练一轮后测试模型时，并没有设置该参数，还是默认100。<code>train_knn</code> 的 k 值过大，则会引入噪声，<code>test</code> 中 k 值过大，造成分母过大，准确率很难上去。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/knn-lm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/knn-lm/" class="post-title-link" itemprop="url">knn-lm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-09 19:26:46" itemprop="dateCreated datePublished" datetime="2021-11-09T19:26:46+08:00">2021-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-13 22:49:47" itemprop="dateModified" datetime="2021-11-13T22:49:47+08:00">2021-11-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>论文：<a href="https://arxiv.org/abs/1911.00172" target="_blank" rel="noopener">GENERALIZATION THROUGH MEMORIZATION: NEAREST NEIGHBOR LANGUAGE MODELS</a></p>
<p>code：<a href="https://github.com/urvashik/knnlm" target="_blank" rel="noopener">knn-lm</a></p>
<p>参考链接：<a href="https://zhuanlan.zhihu.com/p/90890672?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1046686491727724544&amp;utm_campaign=shareopn" target="_blank" rel="noopener">香侬读 | 用上文K最近邻特征表示增强语言模型 </a></p>
<p>论文的主要思想是使用传统 <em>knn</em> 算法对预训练神经语言模型进行线性插值扩展。</p>
<p>ps：传统算法在这个深度学习领域的一次融合……印象中，都是使用预训练模型在小数据集上进行微调，这篇论文似乎有点东西。</p>
<p>对于语言模型<strong>LM</strong>，给定一个上下文序列tokens：</p>
<script type="math/tex; mode=display">
c_t = (w_1,...,w_{t-1})</script><p>自回归语言模型通过建模$p(w_t|c_t)$ 来预测目标词 $w_t$  的概率分布。</p>
<p>kNN-LM可以在没有任何额外的训练情况下，用最邻近检索机制增强预训练语言模型。在模型预训练后，会对训练集的文本集合进行一次前向传播，任何得到 context-target pairs，并将其以键值对形式存储起来（a key-value datastore），以便在推理过程中查找。</p>
<p><img src="/articles/knn-lm/image-20211109212234007.png" alt="image-20211109212234007"></p>
<p>具体的：设语言模型为 f(·)，可以将一个上文 c 映射为固定长度的向量表示。对于给定的第 i 个训练样本$(c_i, w_i)\in D$ ，定义一个键值对$(k_i, v_i)$ ，$k_i$ 表示上文$c_i$ 的向量表示，$v_i$ 表示目标词$w_i$ ，<strong>datastore </strong>(K, V)表示这样一个集合：</p>
<script type="math/tex; mode=display">
(K, V) = \{(f(c_i), w_i)|(c_i,w_i)\in D\}</script><p>在推理阶段，对于给定上文信息 x ，预测 y 概率分布。使用knn算法进行插值，有：</p>
<script type="math/tex; mode=display">
p(y|x) = \lambda p_{knn}(y|x) + (1-\lambda)p_{LM}(y|x)\\
p_{knn}(y|x) \propto \sum_{(k_i,v_i)\in N} 1_{y=v_i}exp(-d(k_i,f(x)))\\</script><p>$\lambda$ 表示调谐参数，N表示更具距离得到的k邻近集合。距离计算公式采用欧氏距离（L2范数）。在这里knn只是为了得到集合N。</p>
<p>当然这种使用knn算法的方法不免存在一些算法本身的缺点。一是距离计算公式的选择，二是查询速度，三是k的选择。对于一个预训练语言模型，需要的语料是巨大的，该方法需要将训练集语料的所有键值对保存下来，便于查询。可想而知，从如此巨大的键值对中获取 k 近邻集合N，其查询代价是相当巨大的！！！</p>
<p>正因如此，为了knn-lm更好的work，在实现时，使用了FAISS库来加速查询过程。</p>
<p><strong>一点补充</strong></p>
<p>原本看完论文后，我就知道这个保存的datastore是很大的，但是我没想到这大的如此离谱！！！</p>
<p>readme中提到模型训练使用了8块GPU，而且基于是Fairseq的。脑阔疼，对Fairseq本来就没什么好印象。索性他提供了一个checkpoint，可以跳过模型训练部分了。但看到后面生成datastore时，我。。。</p>
<blockquote>
<p><strong>Caution</strong>: Running this step requires a large amount of disk space (400GB!). Please read the note about hardware above, before running this!</p>
</blockquote>
<p><strong>400GB</strong>的磁盘大小！！！！！真的是离了一个大谱！！！！！！</p>
<p>现在想想论文摘要里的那句：</p>
<blockquote>
<p>our kNN-LM achieves a new state-of-the-art perplexity of 15.79 – a 2.9 point improvement with no additional training.</p>
</blockquote>
<p>这让我不得不怀疑，这sota拼的是磁盘大小啊。真的是有点东西，我一个小作坊，GPU都就是白嫖的，现在整个400G磁盘，我也是活久见。</p>
<p><strong>一个小故事</strong></p>
<p>我本一介凡人，但是一心向往修仙炼丹之术。早闻各路大神每年都会在修仙圣地 <strong>ICLR</strong> 交流切磋修仙炼丹心得。 一次偶然机会，受高人指点，得到一本秘籍。看完秘籍，豁然开朗，炼丹之路，似乎有了些盼头。</p>
<p>欣喜之余，我也丝毫不敢懈怠。靠着几年的游历经验，白嫖到了一些炼丹器具，也习得一门奇门遁术python，更是窥得仙术tensorflow和pytorch几分奥秘，python大法从入门到入坑，深度学习从入门到放弃，从删库到跑路，我虽自认为资质平庸，在江湖掀不起大风大浪，却也勤勤恳恳苦心修炼，也是到了初识境界。</p>
<p>Github，无数修仙能人术士炫技圣地，在这里果然找到了秘籍之中提到的各种原料以及使用说明书（大神们愿称之为 ‘瑞德密’）。</p>
<p>于是开始每天起早贪黑，备药材，烧丹炉，研究秘籍。按照瑞德密一步一步修炼，但是依旧失败了一次又一次。深感才疏学浅带来的无力，莫不是修为尚浅，无法领略其中奥义。夜不能寐，辗转反侧，我仍百思不得其解。</p>
<p>偶然间，看到到瑞德密后面部分，再次豁然开朗：</p>
<blockquote>
<p>欲修此术修此丹药，需备八个丹炉，外部容器非四百G不可。</p>
</blockquote>
<p>感觉像是吃了闭门羹，无数人对修仙炼丹之术趋之若鹜，但真正修得正果的，基本是各大财大气粗的门派的人。而对于资质平凡，财力有限的小作坊而言，这条路似乎走的异常艰辛。曾无数次阅读各路大神秘籍，但因为各种苛刻的修炼条件望而却步。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" class="post-title-link" itemprop="url">一篇论文解读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-06 10:39:46" itemprop="dateCreated datePublished" datetime="2021-10-06T10:39:46+08:00">2021-10-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-26 10:09:52" itemprop="dateModified" datetime="2025-04-26T10:09:52+08:00">2025-04-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>那就告一段落吧。</p>
<blockquote>
<p> 在平衡内心与周遭的过程中，缝缝补补自己眼中千疮百孔的世界……</p>
</blockquote>
<p>很多事都不一样了，在表示同意赞赏nb还行的同时，其实内心也在保持着一些最后的倔强甚至不屑的态度。向往诗和远方的同时，也在吐槽当下糟糕的境况。这是暂时的妥协，而不是最后的结果。</p>
<p>可以预见的是，有一天，我也会被一块大饼圈住，为别人给的蛋糕沾沾自喜，因为天上掉的馅饼开始信奉神明，在推杯换盏中周旋，吃饱了面包然后驻足休息，养老等死，我的墓志铭大概就是我的第一个”hello world”代码。这是我最后的倔强，而不是暂时的妥协。</p>
<p>技术无罪，资本作祟的时代，人人都好像鬼怪，争夺面包，吸食人xie，手捧圣经，说着抱歉，最后还不忘总结，口感似乎差了点……</p>
<p>二十一岁我还在对自己说：<em>管他三七二十一，先做自己想做的事，说自己想说的话，走自己想走的路……</em></p>
<p>时过境迁，我才意识到，这是原来是叫愤青啊。在深感无力的同时，我也只能长叹一口气（很长很长，用英文就是long long long…）。我还以为我在做自己认为对的事情，我在做自己能做到的事情。</p>
<p>每一次成长，都是和自己谈判的过程，而每次妥协都是在塑造新的自己。over!!!</p>
<hr>
<p>真的不是在吐槽，有认认真真研读！！！</p>
<p>有关：<code>Integrating Linguistic Knowledge to Sentence Paraphrase Generation</code> 论文解读。</p>
<blockquote>
<p>模型框架</p>
</blockquote>
<p>典型<code>Transformer-based</code>  结构，编码器和解码器都是多个<code>Multi-head Attention</code> 组成。如图：</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006132242262.png" alt="image-20211006132242262"></p>
<p>大概分为三个部分：<code>Sentence Encoder</code>、<code>Paraphrase Decoder</code>、<code>Synonym Labeling</code></p>
<p>按照论文里的思路，模型训练包含了一个辅助任务即：<code>Synonym Labeling</code> 。先用encoder部分做辅助任务训练模型，然后整体训练做生成任务。但其实也是可以一起训练的。</p>
<p>下面结合作者开源的代码进行一些分析。</p>
<blockquote>
<p>数据处理</p>
</blockquote>
<ol>
<li>第一步</li>
</ol>
<p>执行 <code>data_processing.py</code>  脚本，生成一个字典文件 <code>vocab</code> 文件。</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;pad&gt;</span><br><span class="line">&lt;unk&gt;</span><br><span class="line">&lt;s&gt;</span><br><span class="line">&lt;/s&gt;</span><br><span class="line">的</span><br><span class="line">，</span><br><span class="line">。</span><br><span class="line">···</span><br><span class="line">&lt;eos&gt;</span><br><span class="line">&lt;sos&gt;</span><br></pre></td></tr></table></figure>
<p>神奇的是首行的<code>&lt;pad&gt;</code> 并不是脚本添加的，需要自己手动添加，这是运行后面程序发现的。而且多出的<code>&lt;eos&gt;,&lt;sos&gt;</code> 也并没有用到。</p>
<ol>
<li>第二步</li>
</ol>
<p>执行<code>prepro_dict.py</code> 脚本，生成数据集对应的同义词对文件：<code>train_paraphrased_pair.txt</code>、<code>dev_paraphrased_pair.txt</code> 、<code>test_paraphrased_pair.txt</code> 。</p>
<p><code>train_paraphrased_pair.txt</code> 为例：</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">年景-&gt;<span class="number">1</span> 或者-&gt;<span class="number">2</span> 年景-&gt;<span class="number">4</span> 或-&gt;<span class="number">2</span> 抑或-&gt;<span class="number">2</span> 要么-&gt;<span class="number">2</span> 要-&gt;<span class="number">2</span> 抑-&gt;<span class="number">2</span></span><br><span class="line">······</span><br></pre></td></tr></table></figure>
<p>对应<code>train</code>数据集中的第一行是：</p>
<figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">1929 </span>年 还是 <span class="number">1989</span> 年 ？</span><br></pre></td></tr></table></figure>
<p>具体对应论文中<code>Synonym Pairs Representation</code> 部分：同义词位置对<code>Synonym-position paris</code> 。</p>
<p><code>词-&gt;pos</code> ：其中<code>pos</code>表示<code>sentence</code> 中的位置，<code>词</code> 是指同义词，即句子中<code>pos</code>位置上词对应的同义词。<code>年景-&gt;1</code> 中位置<code>1</code>处的词为<code>年</code> ，<code>年景</code> 即是<code>年</code>的同义词。</p>
<ol>
<li>总结</li>
</ol>
<p>数据处理这一步两个脚本，生成需要的数据文件有：一个词表文件<code>.vocab</code> ，三个同义词位置对文件<code>_paraphrased_pair.txt</code> 。</p>
<p>整个实验还需要五个数据集文件：train{.src,.tgt}，dev{.src,.tgt}，test{.src}</p>
<p>最后还想提一下：</p>
<p>tcnp.train.src 第268178行竟然是空行！！！对应同义词对为：<code>&lt;unk&gt;-&gt;&lt;unk&gt;</code> ！！！<br><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcnp.train.src</span></span><br><span class="line"><span class="attr">268179</span> <span class="string">阿富汗 肯定 存在 错误 。</span></span><br><span class="line"><span class="attr">268180</span> <span class="string">在 阿富汗 肯定 有 错误 。</span></span><br><span class="line"><span class="attr">268181</span> <span class="string">事实上 ， 我们 确实 在 阿富汗 犯 了 许多 错误 。</span></span><br><span class="line"><span class="comment"># tcnp.train.tgt</span></span><br><span class="line"><span class="attr">268179</span> <span class="string">事实上 ， 我们 确实 在 阿富汗 犯 了 许多 错误 。</span></span><br><span class="line"><span class="attr">268180</span> <span class="string">阿富汗 肯定 存在 错误 。</span></span><br><span class="line"><span class="attr">268181</span> <span class="string">在 阿富汗 肯定 有 错误 。</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>train 训练</p>
</blockquote>
<p>训练部分代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, xs, ys, x_paraphrased_dict, synonym_label=None)</span>:</span></span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    memory, sents1 = self.encode(xs)</span><br><span class="line">    _, _, synonym_label_loss = self.labeling(synonym_label, memory)</span><br><span class="line">    logits, preds, y, sents2 = self.decode(ys, x_paraphrased_dict, memory)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train scheme</span></span><br><span class="line">    <span class="comment"># generation loss</span></span><br><span class="line">    y_ = label_smoothing(tf.one_hot(y, depth=self.hp.vocab_size))</span><br><span class="line">    ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)</span><br><span class="line">    nonpadding = tf.to_float(tf.not_equal(y, self.token2idx[<span class="string">"&lt;pad&gt;"</span>]))  <span class="comment"># 0: &lt;pad&gt;</span></span><br><span class="line">    loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + <span class="number">1e-7</span>)</span><br><span class="line">    <span class="comment"># multi task loss</span></span><br><span class="line">    tloss = self.hp.l_alpha * loss + (<span class="number">1.0</span>-self.hp.l_alpha) * synonym_label_loss</span><br><span class="line"></span><br><span class="line">    global_step = tf.train.get_or_create_global_step()</span><br><span class="line">    lr = noam_scheme(self.hp.lr, global_step, self.hp.warmup_steps)</span><br><span class="line">    optimizer = tf.train.AdamOptimizer(lr)</span><br><span class="line">    train_op = optimizer.minimize(tloss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">'lr'</span>, lr)</span><br><span class="line">    tf.summary.scalar(<span class="string">"loss"</span>, loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">"tloss"</span>, tloss)</span><br><span class="line">    tf.summary.scalar(<span class="string">"global_step"</span>, global_step)</span><br><span class="line"></span><br><span class="line">    summaries = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, train_op, global_step, summaries</span><br></pre></td></tr></table></figure>
<p>大概流程就是: <code>encoder -&gt; labeing and decoder</code> 。</p>
<blockquote>
<p>Sentence Encoder</p>
</blockquote>
<p>输入：</p>
<ul>
<li>sentence x token [x1,x2,x3,…,xn]</li>
</ul>
<p>输出：</p>
<ul>
<li>memory： 经过多个Multi-head Attention后的输出</li>
</ul>
<p>和常规的<code>transformer encoder</code>一样。先是对句子<code>token</code>向量进行<code>embedding</code>，然后添加位置编码<code>positional_encoding</code>。</p>
<p>对应代码在：<code>model.py</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, xs, training=True)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">        x, seqlens, sents1 = xs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding</span></span><br><span class="line">        enc = tf.nn.embedding_lookup(self.embeddings, x) <span class="comment"># (N, T1, d_model)</span></span><br><span class="line">        enc *= self.hp.d_model**<span class="number">0.5</span> <span class="comment"># scale</span></span><br><span class="line"></span><br><span class="line">        enc += positional_encoding(enc, self.hp.maxlen1)</span><br><span class="line">        enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Blocks</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.hp.num_blocks):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">"num_blocks_&#123;&#125;"</span>.format(i), reuse=tf.AUTO_REUSE):</span><br><span class="line">                <span class="comment"># self-attention</span></span><br><span class="line">                enc = multihead_attention(queries=enc,</span><br><span class="line">                                          keys=enc,</span><br><span class="line">                                          values=enc,</span><br><span class="line">                                          num_heads=self.hp.num_heads,</span><br><span class="line">                                          dropout_rate=self.hp.dropout_rate,</span><br><span class="line">                                          training=training,</span><br><span class="line">                                          causality=<span class="literal">False</span>)</span><br><span class="line">                <span class="comment"># feed forward</span></span><br><span class="line">                enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])</span><br><span class="line">    memory = enc</span><br><span class="line">    <span class="keyword">return</span> memory, sents1</span><br></pre></td></tr></table></figure>
<p>不过有趣的是论文中关于<code>Encoder</code>的部分貌似有些问题：</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006182655905.png" alt="image-20211006182655905"></p>
<p>这是论文中的式子。在transformer中是这样的：<code>Block(Q,K,V) = LNorm(FFN(m)+m)、m=LNorm(MultiAttn(Q,K,V)+Q)</code> 。先add再norm啊！！！</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006183417409.png" alt="image-20211006183417409"></p>
<p>不知道是不是排版错误的原因。主要作者开源的代码里<code>multihead_attention</code>部分还有<code>ffn</code> 部分是先add再norm的。</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006184128806.png" alt="image-20211006184128806"></p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006184216337.png" alt="image-20211006184216337"></p>
<p>属实给整蒙了。</p>
<blockquote>
<p>Paraphrase Decoder</p>
</blockquote>
<p>输入：</p>
<ul>
<li>sentence y token [y1,y2,y3,…,ym] 对应tgt中的句子的token</li>
<li>x_paraphrased_dict 引入的外部知识，也就是同义词位置对：<code>synonyms-
position pairs</code></li>
<li>memory: 编码器的输出</li>
</ul>
<p>输出：</p>
<ul>
<li>logits, y_hat： <code>logits</code>是最后一层的输出，<code>y_hat</code>是预测值 </li>
</ul>
<p>这部分有self-attention 、vanilla attention、paraphrased dictionary attention。</p>
<p>self-attention部分和原本的transformer中一样，key=value=query。vanilla attention其实就是key与value相同，query不一样。paraphrased dictionary attention就是引入外部同义词字典知识的部分。这部分主要是计算论文中<code>ct</code> ，按论文中公式来即可。</p>
<p>对应代码在：<code>model.py</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, ys, x_paraphrased_dict, memory, training=True)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"decoder"</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">            decoder_inputs, y, seqlens, sents2 = ys</span><br><span class="line">            x_paraphrased_dict, paraphrased_lens, paraphrased_sents = x_paraphrased_dict</span><br><span class="line">            <span class="comment"># embedding</span></span><br><span class="line">            dec = tf.nn.embedding_lookup(self.embeddings, decoder_inputs)  <span class="comment"># (N, T2, d_model)</span></span><br><span class="line">            dec *= self.hp.d_model ** <span class="number">0.5</span>  <span class="comment"># scale</span></span><br><span class="line"></span><br><span class="line">            dec += positional_encoding(dec, self.hp.maxlen2)</span><br><span class="line">            dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)</span><br><span class="line"></span><br><span class="line">            batch_size = tf.shape(decoder_inputs)[<span class="number">0</span>] <span class="comment"># (N, T2, 2)</span></span><br><span class="line">            seqlens = tf.shape(decoder_inputs)[<span class="number">1</span>]  <span class="comment"># (N, T2, 2)</span></span><br><span class="line">            paraphrased_lens = tf.shape(x_paraphrased_dict)[<span class="number">1</span>]  <span class="comment"># (N, T2, 2)</span></span><br><span class="line"></span><br><span class="line">            x_paraphrased_o, x_paraphrased_p = x_paraphrased_dict[:,:,<span class="number">0</span>], x_paraphrased_dict[:,:,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            x_paraphrased_o_embedding = tf.nn.embedding_lookup(self.embeddings, x_paraphrased_o)  <span class="comment"># N, W2, d_model</span></span><br><span class="line">            <span class="keyword">if</span> self.hp.paraphrase_type == <span class="number">0</span>:</span><br><span class="line">                x_paraphrased_p_embedding = tf.nn.embedding_lookup(self.embeddings, x_paraphrased_p)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x_paraphrased_p_embedding = paraphrased_positional_encoding(x_paraphrased_p, self.hp.maxlen2, self.hp.d_model)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Blocks</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.hp.num_blocks):</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">"num_blocks_&#123;&#125;"</span>.format(i), reuse=tf.AUTO_REUSE):</span><br><span class="line">                    <span class="comment"># Masked self-attention (Note that causality is True at this time)</span></span><br><span class="line">                    dec = multihead_attention(queries=dec,</span><br><span class="line">                                              keys=dec,</span><br><span class="line">                                              values=dec,</span><br><span class="line">                                              num_heads=self.hp.num_heads,</span><br><span class="line">                                              dropout_rate=self.hp.dropout_rate,</span><br><span class="line">                                              training=training,</span><br><span class="line">                                              causality=<span class="literal">True</span>,</span><br><span class="line">                                              scope=<span class="string">"self_attention"</span>)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Vanilla attention</span></span><br><span class="line">                    dec = multihead_attention(queries=dec,</span><br><span class="line">                                              keys=memory,</span><br><span class="line">                                              values=memory,</span><br><span class="line">                                              num_heads=self.hp.num_heads,</span><br><span class="line">                                              dropout_rate=self.hp.dropout_rate,</span><br><span class="line">                                              training=training,</span><br><span class="line">                                              causality=<span class="literal">False</span>,</span><br><span class="line">                                              scope=<span class="string">"vanilla_attention"</span>)</span><br><span class="line">                    <span class="comment">### Feed Forward</span></span><br><span class="line">                    dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># add paraphrased dictionary attention</span></span><br><span class="line">            h = tf.fill([batch_size, seqlens, paraphrased_lens, self.hp.d_model], <span class="number">1.0</span>) * tf.expand_dims(dec, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            o_embeding = tf.fill([batch_size, seqlens, paraphrased_lens, self.hp.d_model], <span class="number">1.0</span>) * tf.expand_dims(x_paraphrased_o_embedding, axis=<span class="number">1</span>)</span><br><span class="line">            W_a_o = tf.get_variable(<span class="string">"original_word_parameter_w"</span>, [<span class="number">2</span>*self.hp.d_model], initializer=tf.initializers.random_normal(</span><br><span class="line">          stddev=<span class="number">0.01</span>, seed=<span class="literal">None</span>))</span><br><span class="line">            V_a_o = tf.get_variable(<span class="string">"original_word_parameter_v"</span>, [<span class="number">2</span>*self.hp.d_model], initializer=tf.initializers.random_normal(</span><br><span class="line">          stddev=<span class="number">0.01</span>, seed=<span class="literal">None</span>))</span><br><span class="line">            h_o_concat = tf.concat([h, o_embeding], <span class="number">-1</span>) <span class="comment"># N, T2, W2, 2*d_model</span></span><br><span class="line">            score_tem_o = tf.tanh(W_a_o * h_o_concat) <span class="comment"># N, T2, W2, 2*d_model</span></span><br><span class="line">            score_o = tf.reduce_sum(V_a_o * score_tem_o, axis=<span class="number">-1</span>) <span class="comment"># N, T2, W2</span></span><br><span class="line">            a = tf.nn.softmax(score_o) <span class="comment"># N, T2, W2</span></span><br><span class="line">            c_o = tf.matmul(a, x_paraphrased_o_embedding) <span class="comment"># (N, T2, W2) * (N, W2, d_model) --&gt; N, T2, d_model</span></span><br><span class="line"></span><br><span class="line">            p_embeding = tf.fill([batch_size, seqlens, paraphrased_lens, self.hp.d_model], <span class="number">1.0</span>) * tf.expand_dims(x_paraphrased_p_embedding, axis=<span class="number">1</span>)</span><br><span class="line">            W_a_p = tf.get_variable(<span class="string">"paraphrased_word_parameter_w"</span>, [<span class="number">2</span>*self.hp.d_model], initializer=tf.initializers.random_normal(</span><br><span class="line">          stddev=<span class="number">0.01</span>, seed=<span class="literal">None</span>))</span><br><span class="line">            V_a_p = tf.get_variable(<span class="string">"paraphrased_word_parameter_v"</span>, [<span class="number">2</span>*self.hp.d_model], initializer=tf.initializers.random_normal(</span><br><span class="line">          stddev=<span class="number">0.01</span>, seed=<span class="literal">None</span>))</span><br><span class="line">            h_p_concat = tf.concat([h, p_embeding], <span class="number">-1</span>) <span class="comment"># N, T2, W2, 2*d_model</span></span><br><span class="line">            score_tem_p = tf.tanh(W_a_p * h_p_concat) <span class="comment"># N, T2, W2, 2*d_model</span></span><br><span class="line">            score_p = tf.reduce_sum(V_a_p * score_tem_p, axis=<span class="number">-1</span>) <span class="comment"># N, T2, W2</span></span><br><span class="line">            a = tf.nn.softmax(score_p) <span class="comment"># N, T2, W2</span></span><br><span class="line">            c_p = tf.matmul(a, x_paraphrased_p_embedding) <span class="comment"># (N, T2, W2) * (N, W2, d_model) --&gt; N, T2, d_model</span></span><br><span class="line"></span><br><span class="line">            c_t = tf.concat([c_o, c_p], axis=<span class="number">-1</span>) <span class="comment"># N, T2, d_model --&gt; N, T2, 2*d_model</span></span><br><span class="line">            out_dec = tf.layers.dense(tf.concat([dec, c_t], axis=<span class="number">-1</span>), self.hp.d_model, activation=tf.tanh, use_bias=<span class="literal">False</span>, kernel_initializer=tf.initializers.random_normal(</span><br><span class="line">          stddev=<span class="number">0.01</span>, seed=<span class="literal">None</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Final linear projection (embedding weights are shared)</span></span><br><span class="line">        weights = tf.transpose(self.embeddings) <span class="comment"># (d_model, vocab_size)</span></span><br><span class="line">        logits = tf.einsum(<span class="string">'ntd,dk-&gt;ntk'</span>, out_dec, weights) <span class="comment"># (N, T2, vocab_size)</span></span><br><span class="line">        y_hat = tf.to_int32(tf.argmax(logits, axis=<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits, y_hat, y, sents2</span><br></pre></td></tr></table></figure>
<p>在最后的输出层部分:论文中的 <code>softmax layer</code>:</p>
<script type="math/tex; mode=display">
y_t = softmax(W_yConcat[y_t^*,c_t]))</script><p>代码中很巧妙的使用 <code>tf.transpose(self.embeddings)</code> 来表示<code>Wy</code> 从而将输出映射到vocab输出。 </p>
<p>这里需要注意的是<code>x_paraphrased_dict</code>的表示。论文中叫做<code>synonyms-
position pairs</code>,使用<code>P</code> 表示。</p>
<script type="math/tex; mode=display">
P = {(si,pi)}_{i=1}^M</script><p><code>si</code> 表示同义词，<code>pi</code> 表示同义词对应sentence中的位置。训练时，会将<code>si</code> 进行<code>embedding</code> ，<code>pi</code> 进行位置编码<code>positional_encoding</code> 。这里的<code>embedding</code> 与<code>positional_encoding</code> 和<code>encoder</code>部分共享。</p>
<blockquote>
<p><code>Synonym Labeling</code> </p>
</blockquote>
<p>输入：</p>
<ul>
<li>synonym_label：同义词标签</li>
<li>memory： encoder的输出</li>
</ul>
<p>输出：</p>
<ul>
<li>logits, y_hat, loss: 一个全连接层的输出、一个预测值、一个损失</li>
</ul>
<p>synonym_label：[True,False,…]，对于给定的一个句子，如果句中词对应位置有同义词这对应label为True,否者为False。这一部分的loss对应论文中的loss2。</p>
<p>这是一个辅助任务，目的是确定给定句子中每个词是否有对应的同义词。有助于更好地定位同义词的位置，结合短语和同义词在原句中的语言关系。可以肯定的是，这是一个二分类任务，并且两个任务共用一个encoder。描述如下：</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006192852063.png" alt="image-20211006192852063"></p>
<p>对应代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">labeling</span><span class="params">(self, x, menmory)</span>:</span></span><br><span class="line">    synonym_label, seqlens, sents1 = x</span><br><span class="line">    logits = tf.layers.dense(menmory, <span class="number">2</span>, activation=tf.tanh, use_bias=<span class="literal">False</span>,</span><br><span class="line">                              kernel_initializer=tf.initializers.random_normal(stddev=<span class="number">0.01</span>, seed=<span class="literal">None</span>))</span><br><span class="line">    y_hat = tf.to_int32(tf.argmax(logits, axis=<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Synonym Labeling loss</span></span><br><span class="line">    y = tf.one_hot(synonym_label, depth=<span class="number">2</span>)</span><br><span class="line">    ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y)</span><br><span class="line">    nonpadding = tf.to_float(tf.not_equal(sents1, self.token2idx[<span class="string">"&lt;pad&gt;"</span>]))  <span class="comment"># 0: &lt;pad&gt;</span></span><br><span class="line">    loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + <span class="number">1e-7</span>)</span><br><span class="line">    <span class="keyword">return</span> logits, y_hat, loss</span><br></pre></td></tr></table></figure>
<p>需要注意的是代码中还有<code>def train_labeling(self, xs, synonym_label=None):</code> 的地方。按照论文中的描述：</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006194032511.png" alt="image-20211006194032511"></p>
<p>这个函数是用来单独做<code>Synonym Labeling</code> 任务的。按论文中的原意，应该是先<code>model.train_labeling(xs, synonym_label)</code> 然后 在进行<code>model.train(xs, ys, x_paraphrased_dict, synonym_label)</code>。</p>
<p>但是<code>train.py</code> 代码中并没有这样做。而是直接进行<code>train</code> 。</p>
<blockquote>
<p>有关细节问题</p>
</blockquote>
<ol>
<li>vocab</li>
</ol>
<p>前面提到过，生成的词表中多出<code>&lt;sos&gt;,&lt;eos&gt;</code> 两个没有用到的词，少了用于填充的<code>&lt;pad&gt;</code>，并且需要自己在首行手动插入<code>&lt;pad&gt;</code> 。猜测可能是课题组的祖传代码。</p>
<p>而且在生成同义词位置对的文件的代码那里，将不在vocab中的同义词统统过滤掉了。</p>
<ol>
<li>Synonym Pairs Representation</li>
</ol>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006200116402.png" alt="image-20211006200116402"></p>
<p>论文里提到如果对应同义词是一个短语，那么就将短语中词嵌入向量求和来表示该短语的向量表示。但是！！！有意思的是，代码中并未体现。而细挖他的数据会发现，给定的数据已是分好词的按空格分隔的。而且是中文数据。中文分好的词，如果是短语，分词后，还是表示一个词。而英文如：abandon同义词give up。give up分词后就是两个单词。也就出现上述情况。</p>
<p>因此，得出结论，作者开源的代码是不完整的。如果换成英文的数据，那么需要考虑的复杂一些了。当然也可以选择把短语同义词过滤掉，那么和中文上处理就是一样的了。</p>
<ol>
<li>paraphrase_type</li>
</ol>
<p>paraphrase_type这个是代码中的一个配置参数，默认为1。</p>
<p><code>parser.add_argument(&#39;--paraphrase_type&#39;, default=1, type=int)</code></p>
<p>这是所有参数中为数不多没有help提示信息的参数。并且我相信这也是唯一一个没有help整不明白的参数。释义类型？</p>
<p>在<code>data_load.py</code> 中找到了蛛丝马迹：</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006202356732.png" alt="image-20211006202356732"></p>
<p>而这一段代码，也属实有些魔幻。</p>
<p>首先<code>parser.add_argument(&#39;--paraphrase_type&#39;, default=1, type=int)</code> 这里使用的是1。而取值为1时，对应代码中esle：部分。不用很仔细就可以看出：0时，使用word_set，1时，使用pos_set。我们在观察后面的<code>synonym_label</code> 那一句：</p>
<p><code>synonym_label = [i in word_set if paraphrase_type else w in word_set for i, w in enumerate(in_words)]</code></p>
<p>码字到这里，我掐了以下人中，方才缓过神来。接着分析，为0的情况下，那么考虑<code>w in word_set</code> 的真值情况。而<code>for i, w in enumerate(in_words)</code> ，w属于in_words。word_set是什么呢？<code>word_set.add(tem1)</code> 哇哦，是同义词集合诶，in_words是什么？<code>in_words=sent1.split()+[&#39;&lt;/s&gt;&#39;]</code> 欸，那w难道不是don’t exit in word_set forever？</p>
<p>离谱的是，这里讨论的是paraphrase_type = 1的情况，也就是关word_set屁事的情况。word_set这时都是空的。所以synonym_label 难道不是<em>always be False</em> 。</p>
<p><code>Are you kidding me? %$*#@&gt;?*&amp;。。。。</code> </p>
<p>而且<code>x_paraphrase_dic</code> 那里也是有问题的。</p>
<p><code>x_paraphrase_dict.append([token2idx.get(tem1, token2idx[&quot;&lt;unk&gt;&quot;]), token2idx.get(tem2, token2idx[&quot;&lt;unk&gt;&quot;])])</code></p>
<p>这个<code>token2idx.get(tem2, token2idx[&quot;&lt;unk&gt;&quot;])</code> 就很有问题，tem2表示的是pos啊，句子中词的位置，直接给我<code>token2idx</code> 我是无法理解的。</p>
<p><code>x_paraphrase_dict.append([token2idx.get(tem1, token2idx[&quot;&lt;unk&gt;&quot;]), int(tem2 if tem2!=&quot;&lt;unk&gt;&quot; else 0)])</code></p>
<p><code>int(tem2 if tem2!=&quot;&lt;unk&gt;&quot; else 0)</code> 也就是说tem2为<code>&lt;unk&gt;</code> 时，大概就是tem2取0的意思。而tem2出现<code>&lt;unk&gt;</code> 的情况时，tem1也是<code>&lt;unk&gt;</code> 。此时x_paraphrase_dict添加的就是<code>[1, 0]</code> （token2idx[“<unk>“] = 1）。举个例子：</unk></p>
<figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">句子<span class="keyword">x</span>: <span class="keyword">x</span><span class="number">1</span> <span class="keyword">x</span><span class="number">2</span> <span class="keyword">x</span><span class="number">3</span> <span class="keyword">x</span><span class="number">4</span> 。</span><br><span class="line">对应同义词位置对： &lt;unk&gt;-&gt;&lt;unk&gt;</span><br></pre></td></tr></table></figure>
<p>这表示这个句子中没有词含有同义词。这个时候x_paraphrase_dict添加<code>[1, 0]</code>，就相当于<code>&lt;unk&gt;</code> 为 x1的同义词。这怎么可能？<em>It’s impossible！！！</em>  而且就很不<em>reasonable</em> 。简直离谱！！！离了个大谱。</p>
<p>甚至这段代码的第二个for循环后面的那部分代码逻辑都是有问题的。synonym labeling 任务我认为是有问题的。而将<code>&lt;unk&gt;</code> 与位置0处单词绑定，本身就引入了一些噪声，甚至可能增加<code>&lt;unk&gt;</code> 释义的潜在可能性。至于模型能work，我想，synonym labeling本身作为辅助任务，其loss权值占比为0.1，影响应该是很小的。</p>
<p>这里大胆揣测以下paraphrase_type 意图：</p>
<ul>
<li>paraphrase_type  = 0时：x_paraphrase_dict包含句子中的词以及对应同义词，不含位置信息。</li>
<li>paraphrase_type = 1 时： x_paraphrase_dict其实包含同义词以及对应位置pos。</li>
</ul>
<p>无论哪一种情况，其实都是在做一件事：就是将词与其对应的同义词之间进行绑定。第一种更像是比较直接的方式，第二种则略显委婉点。殊途同归！！！</p>
<p>不管是不是这两种情况，其实那块代码都是有问题的。</p>
<ol>
<li>有关 paddings</li>
</ol>
<p><code>data_load.py</code> 中有段这样的代码，用来获取一个batch size 数据的dataset函数：</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211007104255666.png" alt="image-20211007104255666"></p>
<p>其中关于<code>paddings</code> 处的地方自认为还是有些不妥的，对于src、tgt句子进行0填充是正常操作，但是对于<code>x_paraphrased_dict</code>也进行0填充是欠考虑的。</p>
<p>对于<code>x_paraphrased_dict</code> ，0填充，就会出现一部分<code>[[0,0],[0,0],…]</code>的情况 默认将[pad]字符与位置0的词对应了。</p>
<blockquote>
<p>总结</p>
</blockquote>
<p>这篇论文做的是: <strong>Sentence Paraphrase Generation</strong> 。</p>
<p>其中真正核心地方在于 <strong>Knowledge-Enhanced</strong> ,知识增强。主要就是通过引入外部信息（这里是同义词字典信息）来指导模型生成更加多样性的句子。关于知识增强的方式还是有很多的，这篇论文采用的应该是词表注意力机制，得到外部信息特征表示<strong>ct</strong> 。 </p>
<p>代码开源了，貌似有没有完全开源！！！</p>
<p>开源的代码基于python3.6 + tensorflow-gpu==1.12。调试起来真的好麻烦。看不惯tf1.x的代码风格，然后用tf2.x复现了下。</p>
<ul>
<li>对于 paraphrase_type 的两种情况按上述理解做了调整。</li>
<li>对<code>&lt;unk&gt;</code> 匹配第一个单词的情况进行纠正，将<code>tme2 = &lt;unk&gt;</code> 时(句子中没有词存在同义词的情况)，用第一个词与第一个词匹配。即将x1的同义词匹配为x1，这样还是比较妥当的。</li>
<li>过滤了空行，减少不必要的噪声。（空行对应的同义词对为 \<unk\>-&gt;\<unk\>）</unk\></unk\></li>
<li>synonym_label中使用2进行padding。训练时，是需要对其进行padding的保证输入的数据工整的。比如句子idx会使用0进行填充直到maxlen，而idx=0对应词为 <code>&lt;pad&gt;</code> 。显然，<code>&lt;pad&gt;</code> 和其他词一样，是一个单独的类别了。所以，为了区分，synonym_label的padding_value设置为2。最后做成一个三分类任务。无伤大雅，主要是为了适配句子的填充。</li>
<li>为了适应<code>x_paraphrased_dict</code> 的0填充，对输入句子src的首位置引入一个填充符<code>&lt;pad&gt;</code> 。这一点与第二点先呼应。</li>
</ul>
<p>不知道效果如何，小作坊，资源有限，训练完要很久很久。敬佩所有敢于开源代码的科研人员，也希望所有开源代码可读性越来越好吧。也希望所有开源代码都能复现结果。至少把种子固定了吧！！！</p>
<p>————————————–——–———-———10月16日更——————-–—-———————————————</p>
<blockquote>
<p>一点思考</p>
</blockquote>
<p>兜兜转转，模型训练了好几遍，从训练指标来看，loss有下降，acc有升高，但是推理的时候，预测的起始符号\<s>后一个词总是结束符\</s> 。debug无数遍，优化了一些细节上的小问题，还是出现那样的情况。最后将问题锁定在了<strong>padding_mask</strong> 和<strong>look_ahead_mask</strong> 上，其实最可能猜到就是<strong>look_ahead_mask</strong> 有问题。此处的mask都是0和1填充的，代码中使用了<code>tf.keras.layers.MultiHeadAttention</code> 接口，对于0、1矩阵的mask，在里面并没有进行<code>mask*-1e9</code> 的掩码操作，这也导致了训练时出现了数据穿越/泄露问题。所以在推理时，输入的起始字符进行预测时不能得到正确结果，至于为什么是结束符，可能是起止符在词表中相邻的缘故。</p>
<p>今天改好后，重新训练，十多个小时过去了，还没训练完（3090，24G显存）。</p>
<p>如今的顶会基本被财大气粗的大公司大实验室的团队承包，小作坊式实验室夹缝求生。顶会期刊也不乏滥竽充数者，实验结果复现难，开源名存实亡……等等一系列骚操作。现有大环境下，一言难尽。</p>
<p>————————————–——–———-———10月17日更——————-–—-———————————————</p>
<p>开完组会，被老师叫停，没有硬件资源，也只好先放弃，把其他事情提上日程。</p>
<p><img src="/articles/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/image-20211006221538257.png" alt="image-20211006221538257"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/TS-Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/TS-Transformer/" class="post-title-link" itemprop="url">TS-Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-17 21:58:41" itemprop="dateCreated datePublished" datetime="2021-09-17T21:58:41+08:00">2021-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-19 14:35:57" itemprop="dateModified" datetime="2021-09-19T14:35:57+08:00">2021-09-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>前言</p>
</blockquote>
<p>最近，老师让复现几篇论文中的方法。打开一篇有关<code>cnn</code> 的论文，初略一看，这个模型结构不就是<code>textcnn</code> 吗？！论文中改头换面变成了<code>LS-CNN</code>，着实有些摸不着头脑。那就仔细看看模型说明吧，看看到底有什么神奇之处。</p>
<p>十多分钟后······，大概懂了，<code>LS-CNN = TextCNN(w*stack(A,B))</code>  。A、B分别表示layer embedding特征、Google word2vec 词向量特征，*表示卷积，stack表示堆叠（两个大小维度相同的矩阵，堆叠后，通道变成2），通过一维卷积操作进行降维（融合两个嵌入特征）。</p>
<blockquote>
<p>I know nothing but my ignorance……</p>
</blockquote>
<p>2017年谷歌一篇<code>Attention is all you need</code> 在自然语言处理领域炸开了锅。此后<code>transformer</code> 成为了许多人发paper密码 。之后的<code>bert</code> 更是在各大nlp任务上霸榜。各种魔改层出不求。至此，如果不了解<code>transfomer</code> ，不会微调<code>bert</code> 都不好意思说自己是一个 <code>nlper</code> 。不仅如此，隔壁的<code>cv</code>圈都要沾一下光（<code>VIT</code>）。要我说以后投稿就喊一句：<em>哦斯，喊出我的名字吧！transformer.</em> 或者 <em>构筑未来，希望之光，特利迦，transformer type/bert type</em> 。颇有一股新生代奥特曼借力量的趣味（滑稽）。</p>
<p>距离<code>transformer</code>发布已经过去4年，这一波热潮何时褪去，或者下一次革命性的模型什么时候出现，这似乎很难预测。<code>self-attention</code> 的尽头是什么？在这急功近利的时代，各大<code>AI Lab</code> 又有几个愿意沉下心来思考研究呢？毕竟资本家只在乎短期能不能变现。</p>
<p>有意思的是，<code>transformer</code> 又名变形金刚，这也预示这它花里胡哨的各式变形成为可能。</p>
<blockquote>
<p>方兴未艾</p>
</blockquote>
<p>基于自己有限的认知，随便瞎扯了一下。</p>
<p>回归正题，自然语言处理技术在其他领域的应用正在悄悄进行中，就像开头提到的那个团队所做的工作一样。仔细一想，他们似乎也是在填充这一块空白，为后继者提供一个新的基线，这是有利于领域发展的。这是一个十分优秀的团队，有责任有担当。</p>
<p>而作为新入行者的我或者其他人，应该也是倍感压力的。眼下借助自然语言处理技术发光发热的路子似乎并没有那么简单了。</p>
<blockquote>
<p>班门弄斧</p>
</blockquote>
<p>所以，在此，不妨大胆预测一下，他们接下来会不会对<code>transformer</code> 那一大家子动手呢，又或者另辟蹊径采用<code>GNN(GCN)</code> 来建模呢？这两种可能性还是很大的。</p>
<p>哈哈哈哈哈哈。在这里挖个坑，献丑提名个 <code>TS-Transformer</code> 来做隐写分析。</p>
<p>采用<code>Transformer</code> 的<code>encoder</code> 部分提取句子中词与词之间的关系特征和甚至句子的语义特征，然后进行<code>max-pool</code>及<code>avg-pool</code>，然后<code>concat</code> 两个pool特征进行融合，在通过最后全连接进行分类。当然对于词嵌入向量也使用两种embedding，即<code>word2vec</code>和<code>layer embedding</code> 。基于此实现的<code>TS-Transformer</code> 已经在训练了。事实证明这是可以work的。至于效果，留个悬念，暂不公布，代码暂不开源（就图一乐，/滑稽.jpg）。</p>
<p>【后续补个模型图】</p>
<p>【后续补个实验结果】</p>
<p>似乎使用大规模预训练<code>bert</code>模型来代替<code>word2vec</code> 效果应该更好吧。毕竟<code>word2vec</code> 还是属于浅层特征表示吧。【又挖个坑】</p>
<p>按照这个路子，<code>TS-bert、TS-GNN、TS-GCN......</code> 都是可能work的。</p>
<blockquote>
<p>有空在更</p>
</blockquote>
<p>然后……中秋放假了。</p>
<p>哦斯，喊出我的名字吧！<code>TS-Transformer</code> 。构筑未来，希望之光，<code>transformer</code>，<code>TS type</code> 。</p>
<p>【高开低走的特特利迦竟然试图让泽塔串场来拯救低迷的收视率以及低到可怜的评分，笑死】</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/%E5%BD%93%E6%88%91%E9%81%87%E5%88%B0tensorflow2-x%E6%97%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/%E5%BD%93%E6%88%91%E9%81%87%E5%88%B0tensorflow2-x%E6%97%B6/" class="post-title-link" itemprop="url">当我遇到tensorflow2.x时</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-15 16:12:03" itemprop="dateCreated datePublished" datetime="2021-09-15T16:12:03+08:00">2021-09-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-16 18:14:35" itemprop="dateModified" datetime="2021-10-16T18:14:35+08:00">2021-10-16</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>前言</p>
</blockquote>
<p>近日，使用tensorflow的频率比较高，使用过程中也是遇到了一些大大小小的问题。有些着实让人脑瓜子疼。此刻借着模型训练的时间，开始码码字。本来标题想取：</p>
<ul>
<li>什么？2021了，还有人用Tensorflow?</li>
<li>震惊！代码练习生竟然在用······Tensorflow?</li>
<li>我和tensorflow不共戴天</li>
<li>我想给tensorflow来一大嘴巴子</li>
<li>······</li>
</ul>
<p>最后，用了这个<code>当我遇到tensorflow2.x时</code> 。无论学习还是生活中，我们都会遇到各种各样的人或事或物。当我们遇到时，会发生什么？我们是会充满期待的。<code>当我遇到···时，我会···</code> 。这个句式是我喜欢的，大部分人习惯在前半部分大胆设想，后半句夸下豪言壮语。这里取前半句，是因为已经发生了，而省去后半句，恰恰是因为豪言壮语很容易翻车。</p>
<p>这是一篇记录使用tensorflow过程中遇到的一些小而折磨人的问题的博文。但我预言这也将是一篇持久的对tensorflow的血泪吐槽文。</p>
<blockquote>
<p>如何看待Keras正式从TensorFlow中分离？</p>
</blockquote>
<p>不知道为什么想到了这个知乎话题。六月份的某天，Keras 之父 Francois Chollet宣布将 Keras 的代码从 TensorFlow 代码库中分离出来，移回到了自己的 repo。乍一看，还以为以后tensorflow的keras接口用不了了。但人家只是把keras代码搬回了属于自己的repo。原本的<code>tf.keras</code> 还是能用的。</p>
<blockquote>
<p>For you as a user, absolutely nothing changes, now or in the future.</p>
</blockquote>
<p>底下全是一片叫好，天下苦tensorflow久已。而我也并不看好这对情侣或者说组合。各自单飞，独自美丽不好吗？keras何必委曲求全做别人的嫁衣。</p>
<blockquote>
<p>抛开keras，tensorflow还剩什么？</p>
</blockquote>
<p>我想这应该是吐槽后，该冷静思考的问题。而回答这个问题，是需要去阅读官方文档以及实践的。所以，那个句式的后半句也可以是下面的记录。才疏学浅，当厚积薄发。</p>
<p>言归正传，之后遇到的bug都记录在下面部分。</p>
<p>—————————————–———-—-————分割线——————-—————————————————–—</p>
<blockquote>
<p><code>tf.config.run_functions_eagerly(True)</code></p>
</blockquote>
<p>有关<code>Eager Execution</code> <a href="https://www.tensorflow.org/guide/eager" target="_blank" rel="noopener">戳这里</a> </p>
<p>然后以下是我粗俗的理解：</p>
<p>这是即时运行和计算图运行相关的概念。即时运行可以让你的程序立马返回结果，计算图运行会先构建计算图（记录你的程序执行行为及顺序），在最后按照构建的图进行计算。</p>
<p>有些晦涩难理解。</p>
<p>模型训练时一般有：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        ···train model code···</span><br></pre></td></tr></table></figure>
<p>这在模型训练过程中是会构建计算图的（具体参考<a href="https://www.tensorflow.org/guide/intro_to_graphs" target="_blank" rel="noopener">戳这里</a>），构建计算图可以，这时如果在代码中<code>print(x)</code> 一下，就会发现这是没有具体值的，而且没有<code>.numpy()</code> 属性。返回的即<code>计算图中节点的符号句柄</code> 。所以我为什么要在这里<code>print</code>呢？当然是为了调试代码(/滑稽.jpg)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"x:0"</span>, shape=(<span class="number">32</span>, <span class="number">32</span>), dtype=int32)</span><br></pre></td></tr></table></figure>
<p>官网提到tensorflow2.x是默认开启<code>Eager Execution</code> 的，然而代码中(如上)使用了<code>@tf.function</code> 装饰器，默认以图的方式执行。</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">The</span> <span class="meta">code</span> in a <span class="meta">Function</span> can <span class="keyword">be </span>executed <span class="keyword">both </span>eagerly <span class="keyword">and </span>as a graph. <span class="keyword">By </span>default, <span class="meta">Function</span> executes <span class="keyword">its </span><span class="meta">code</span> as a graph.</span><br></pre></td></tr></table></figure>
<p>要关闭默认方式，可以通过设置：<code>tf.config.run_functions_eagerly(True)</code> 来实现。或者干脆不要加这个装饰器。</p>
<p>最后，<code>Eager Execution</code> 增强了开发和调试的交互性，而<code>@tf.function</code> 计算图执行在分布式训练、性能优化和生产部署方面具有优势。简而言之，<code>Eager Execution</code>适合开发过程中调试，<code>@tf.function</code>适合线上部署。</p>
<p>——————-2021.9.17更新———————</p>
<blockquote>
<p>自定义</p>
</blockquote>
<p>参考-&gt;<a href="https://www.tensorflow.org/guide/basic_training_loops" target="_blank" rel="noopener">这里</a></p>
<p><strong>定义模型</strong></p>
<p>抛开keras的<code>sequential</code>, 使用 tensorflow定义模型时，可以有两种继承选择：<code>tf.keras.Model</code> 和 <code>tf.Module</code></p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TFModel</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">    super().__init__(**kwargs)</span><br><span class="line">    self.w = tf.Variable(<span class="number">5.0</span>)</span><br><span class="line">    self.b = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.w * x + self.b</span><br><span class="line">tf_model = TFModel()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KerasModel</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">    super().__init__(**kwargs)</span><br><span class="line">    self.w = tf.Variable(<span class="number">5.0</span>)</span><br><span class="line">    self.b = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.w * x + self.b</span><br><span class="line">keras_model = KerasModel()</span><br></pre></td></tr></table></figure>
<p>定义训练循环：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(x_train,y_train,x_valid,y_valid,model,epochs = <span class="number">5</span>,batch_size = <span class="number">64</span>, lr =<span class="number">0.001</span>, print_freq = <span class="number">10</span>)</span>:</span></span><br><span class="line">    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)</span><br><span class="line"></span><br><span class="line">    train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">    test_loss = tf.keras.metrics.Mean(name=<span class="string">'test_loss'</span>)</span><br><span class="line">    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'test_accuracy'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="comment"># 在下一个epoch开始时，重置评估指标</span></span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        train_accuracy.reset_states()</span><br><span class="line">        test_loss.reset_states()</span><br><span class="line">        test_accuracy.reset_states()</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(int(len(x_train)/batch_size)):</span><br><span class="line">            rand_id = np.asarray(random.sample(range(len(x_train)), batch_size))</span><br><span class="line">            bs_x_train = x_train[rand_id]</span><br><span class="line">            bs_y_train = y_train[rand_id]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># train step</span></span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                predictions = model(bs_x_train)</span><br><span class="line">                loss = loss_object(bs_y_train, predictions)</span><br><span class="line">            gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">            optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line">            train_loss(loss)</span><br><span class="line">            train_accuracy(bs_y_train, predictions)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># test step</span></span><br><span class="line">            predictions = model(x_valid)</span><br><span class="line">            t_loss = loss_object(y_valid, predictions)</span><br><span class="line">            test_loss(t_loss)</span><br><span class="line">            test_accuracy(y_valid, predictions)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># print info</span></span><br><span class="line">            <span class="keyword">if</span> step%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">                template = <span class="string">'Epoch &#123;&#125;,step &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;'</span></span><br><span class="line">                print(template.format(epoch+<span class="number">1</span>,</span><br><span class="line">                            step,</span><br><span class="line">                            train_loss.result(),</span><br><span class="line">                            train_accuracy.result()*<span class="number">100</span>,</span><br><span class="line">                            test_loss.result(),</span><br><span class="line">                            test_accuracy.result()*<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">        template = <span class="string">'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;'</span></span><br><span class="line">        print(template.format(epoch+<span class="number">1</span>,</span><br><span class="line">                    train_loss.result(),</span><br><span class="line">                    train_accuracy.result()*<span class="number">100</span>,</span><br><span class="line">                    test_loss.result(),</span><br><span class="line">                    test_accuracy.result()*<span class="number">100</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>若是继承自<code>tf.keras.Model</code> 则可以使用 <code>model.compile()</code> 去设置参数, 使用<code>model.fit()</code> 进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">keras_model = KerasModel()</span><br><span class="line">keras_model.compile(</span><br><span class="line">    <span class="comment"># 默认情况下，fit()调用tf.function()。</span></span><br><span class="line">    <span class="comment"># Debug时你可以关闭这一功能，但是现在是打开的。</span></span><br><span class="line">    run_eagerly=<span class="literal">False</span>,</span><br><span class="line">    optimizer=tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>),</span><br><span class="line">    loss=tf.keras.losses.mean_squared_error,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>——————-2021.9.18更———————</p>
<blockquote>
<p>种子</p>
</blockquote>
<p>为了确保每次运行结果的稳定，设置固定种子是有必要的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">random.seed(<span class="number">2021</span>)</span><br><span class="line">np.random.seed(<span class="number">2021</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">2021</span>)</span><br></pre></td></tr></table></figure>
<p>——————-2021.9.19更———————</p>
<p>当自定义模型时，继承<code>tf.keras.Model</code> 则需要实现<code>call</code> 方法而不是<code>__call__</code> 。如果是<code>tf.Module</code> 就实现<code>__call__</code> 。尽量使用<code>tf.keras.Model</code> ，因为真的很方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KerasModel</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">    super().__init__(**kwargs)</span><br><span class="line">    self.w = tf.Variable(<span class="number">5.0</span>)</span><br><span class="line">    self.b = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.w * x + self.b</span><br><span class="line">model = KerasModel()</span><br><span class="line">bst_model_path = <span class="string">"./best.model.h5"</span></span><br><span class="line"></span><br><span class="line">early_stopping = tf.keras.callbacks.EarlyStopping(monitor=<span class="string">'val_accuracy'</span>, patience=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">model_checkpoint = tf.keras.callbacks.ModelCheckpoint(bst_model_path, save_best_only=<span class="literal">True</span>, save_weights_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">False</span>),</span><br><span class="line">              optimizer=tf.keras.optimizers.Adam(<span class="number">1e-3</span>),</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train, train_y, </span><br><span class="line">          batch_size=<span class="number">16</span>, </span><br><span class="line">          validation_data=(x_valid,valid_y),</span><br><span class="line">          epochs=<span class="number">200</span>,</span><br><span class="line">          callbacks=[early_stopping,model_checkpoint]</span><br><span class="line">         )</span><br></pre></td></tr></table></figure>
<p>——————-2021.10.4更———————</p>
<blockquote>
<p>从python生成器中加载数据</p>
</blockquote>
<p>官方文档：<a href="https://www.tensorflow.org/guide/data?hl=zh_cn#consuming_python_generators" target="_blank" rel="noopener">consuming_python_generators</a></p>
<p>code template:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(args, batch_size,shuffle=False)</span>:</span></span><br><span class="line">    output_types = (tf.int32, tf.int32, tf.int32, tf.int32)</span><br><span class="line">    output_shapes = ((<span class="literal">None</span>,),</span><br><span class="line">                     (<span class="literal">None</span>,),</span><br><span class="line">                     (<span class="literal">None</span>,<span class="number">2</span>),</span><br><span class="line">                     (<span class="literal">None</span>,)</span><br><span class="line">                     )</span><br><span class="line">    dataset = tf.data.Dataset.from_generator(generator_fn, </span><br><span class="line">                                              args=args, </span><br><span class="line">                                              output_types= output_types, </span><br><span class="line">                                              output_shapes = output_shapes</span><br><span class="line">                                              )</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        dataset = dataset.shuffle(buffer_size = <span class="number">1000</span>*batch_size)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># dataset = dataset.repeat() 这行代码有毒</span></span><br><span class="line">    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=output_shapes, padding_values=(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    dataset = dataset.prefetch(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure>
<p><code>output_types</code> 是必须的，<code>buffer_size</code> 一般取大于等于数据集大小。<code>generator_fn</code> 为生成器函数。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(stop)</span>:</span></span><br><span class="line">  i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> i&lt;stop:</span><br><span class="line">    <span class="keyword">yield</span> i</span><br><span class="line">    i += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>——————-2021.10.5更———————</p>
<blockquote>
<p>有关<code>call()</code></p>
</blockquote>
<p>子类化<code>tf.keras.Model</code>时，在实现<code>call()</code> 函数需要注意的是接受的参数一般只能是两个：<code>inputs</code> ，<code>training</code></p>
<p><code>training</code> 一般给用户自定义训练模式提供一定的自由度，<code>training</code> 为布尔类型。当然，<code>training</code> 是非必须的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KerasModel</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.w = tf.Variable(<span class="number">5.0</span>)</span><br><span class="line">        self.b = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.w * x + self.b</span><br></pre></td></tr></table></figure>
<p>如果模型需要多个输入时：可以通过 <code>inputs = (x1,x2,x3,...)</code> 将<code>inputs</code> 传入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">··省略··</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x1,x2,x3,... = inputs</span><br></pre></td></tr></table></figure>
<p>当然也可以通过<code>**kwargs</code> 将其他数据传入。而不是像这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x1,x2,x3,...)</span>:</span></span><br><span class="line">    ···</span><br></pre></td></tr></table></figure>
<blockquote>
<p>类型转换</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.cast(x, dtype=tf.int64)</span><br></pre></td></tr></table></figure>
<p>——————-2021.10.16更———————</p>
<blockquote>
<p><code>tf.keras.layers.MultiHeadAttention</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">multi = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)</span><br><span class="line"></span><br><span class="line">out = multi(v,k,q,mask*<span class="number">-1e9</span>)</span><br></pre></td></tr></table></figure>
<p>如果mask是0、1矩阵，记得乘以<strong>-1e9</strong> ，否者掩码无效。</p>
<blockquote>
<p>TODO</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B/" class="post-title-link" itemprop="url">蛋白质结构预测</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-07-30 13:47:35 / 修改时间：14:20:51" itemprop="dateCreated datePublished" datetime="2021-07-30T13:47:35+08:00">2021-07-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>赛题：<a href="https://challenge.xfyun.cn/topic/info?type=protein" target="_blank" rel="noopener">蛋白质结构预测挑战赛</a></p>
<p>数据集一共包含245种折叠类型，11843条蛋白质序列样本，其中训练集中有9472个样本，测试集中有2371个样本。</p>
<p>继上次<a href="https://hahally.github.io/articles/蛋白质结构预测之lgb的baseline/">lgb的base模型</a> 后，尝试过word2vec + 神经网络的方法，最后效果甚微。今天尝试了一下双向GRU模型，相比之前，有几个百分点的提高。</p>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_fa</span><span class="params">(file, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> mode <span class="keyword">in</span> &#123;<span class="string">'train'</span>,<span class="string">'test'</span>&#125;</span><br><span class="line">    labels = []</span><br><span class="line">    seqs_info = []</span><br><span class="line">    cates_id = []</span><br><span class="line">    seq = <span class="string">''</span></span><br><span class="line">    <span class="keyword">with</span> open(file,mode=<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        line = f.readline().strip()</span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            <span class="keyword">if</span> line[<span class="number">0</span>]==<span class="string">'&gt;'</span>:</span><br><span class="line">                info = line[<span class="number">1</span>:].split(<span class="string">' '</span>)</span><br><span class="line">                cates_id.append(info[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">                    label = <span class="string">''</span>.join(info[<span class="number">1</span>].split(<span class="string">'.'</span>)[:<span class="number">2</span>]</span><br><span class="line">                    label = label[<span class="number">0</span>]+<span class="string">'.'</span>+label[<span class="number">1</span>:]</span><br><span class="line">                    labels.append(label)</span><br><span class="line">                <span class="keyword">if</span> seq:</span><br><span class="line">                    seqs_info.append(seq)</span><br><span class="line">                    seq = <span class="string">''</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                seq += line</span><br><span class="line">            line = f.readline().strip()</span><br><span class="line">        seqs_info.append(seq)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cates_id,seqs_info,labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    train_file = <span class="string">'/kaggle/input/textfiles/astral_train.fa'</span></span><br><span class="line">    test_file = <span class="string">'/kaggle/input/textfiles/astral_test.fa'</span></span><br><span class="line"></span><br><span class="line">    train_sample_id, train_seqs_info, train_labels = read_fa(train_file, mode=<span class="string">'train'</span>)</span><br><span class="line">    test_sample_id, test_seqs_info, _ = read_fa(test_file, mode=<span class="string">'test'</span>)</span><br><span class="line">    </span><br><span class="line">    train_data = &#123;</span><br><span class="line">    <span class="string">'sample_id'</span>: train_sample_id,</span><br><span class="line">    <span class="string">'seq_info'</span>: train_seqs_info,</span><br><span class="line">    <span class="string">'label'</span>: train_labels</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    test_data = &#123;</span><br><span class="line">        <span class="string">'sample_id'</span>: test_sample_id,</span><br><span class="line">        <span class="string">'seq_info'</span>: test_seqs_info,</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    train = pd.DataFrame(data=train_data)</span><br><span class="line">    train = shuffle(train,random_state=<span class="number">2021</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    test = pd.DataFrame(data=test_data)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train,test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 滑窗分词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_windows</span><span class="params">(sentence,w = <span class="number">3</span>)</span>:</span></span><br><span class="line">    new_sentence = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sentence)-w+<span class="number">1</span>):</span><br><span class="line">        new_sentence.append(sentence[i:i+w])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> new_sentence</span><br><span class="line">     </span><br><span class="line">data = pd.concat(load_data(),ignore_index=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># label to idx</span></span><br><span class="line">label2idx = &#123; l:idx <span class="keyword">for</span> idx, l <span class="keyword">in</span> enumerate(data[~data[<span class="string">'label'</span>].isna()][<span class="string">'label'</span>].unique().tolist())&#125;</span><br><span class="line">idx2label = &#123; idx:l <span class="keyword">for</span> l,idx <span class="keyword">in</span> label2idx.items()&#125;</span><br><span class="line"></span><br><span class="line">data[<span class="string">'label'</span>] = data[<span class="string">'label'</span>].map(label2idx)</span><br><span class="line">data[<span class="string">'new_seq_info'</span>] = data[<span class="string">'seq_info'</span>].apply(<span class="keyword">lambda</span> x:split_windows(x,w = <span class="number">1</span>))</span><br><span class="line">train,test = data[~data[<span class="string">'label'</span>].isna()].reset_index(drop=<span class="literal">True</span>),data[data[<span class="string">'label'</span>].isna()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">max_features= <span class="number">1000</span></span><br><span class="line">max_len= <span class="number">256</span></span><br><span class="line">embed_size=<span class="number">128</span></span><br><span class="line">batch_size = <span class="number">24</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"></span><br><span class="line">tokens = Tokenizer(num_words = max_features)</span><br><span class="line">tokens.fit_on_texts(list(data[<span class="string">'new_seq_info'</span>]))</span><br><span class="line"></span><br><span class="line">x_data = tokens.texts_to_sequences(data[<span class="string">'new_seq_info'</span>])</span><br><span class="line">x_data = sequence.pad_sequences(x_data, maxlen=max_len)</span><br><span class="line">x_train = x_data[:<span class="number">9472</span>]</span><br><span class="line">y_train = data[<span class="string">'label'</span>][:<span class="number">9472</span>]</span><br><span class="line">x_test = x_data[<span class="number">9472</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D<span class="comment"># Keras Callback Functions:</span></span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Callback</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping,ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> initializers, regularizers, constraints, optimizers, layers, callbacks</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line">sequence_input = Input(shape=(max_len, ))</span><br><span class="line">x = Embedding(max_features, embed_size, trainable=<span class="literal">True</span>)(sequence_input)</span><br><span class="line">x = SpatialDropout1D(<span class="number">0.2</span>)(x)</span><br><span class="line">x = Bidirectional(GRU(<span class="number">128</span>, return_sequences=<span class="literal">True</span>,dropout=<span class="number">0.1</span>,recurrent_dropout=<span class="number">0.1</span>))(x)</span><br><span class="line">x = Conv1D(<span class="number">64</span>, kernel_size = <span class="number">3</span>, padding = <span class="string">"valid"</span>, kernel_initializer = <span class="string">"glorot_uniform"</span>)(x)</span><br><span class="line">avg_pool = GlobalAveragePooling1D()(x)</span><br><span class="line">max_pool = GlobalMaxPooling1D()(x)</span><br><span class="line">x = concatenate([avg_pool, max_pool]) </span><br><span class="line">preds = Dense(<span class="number">245</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(sequence_input, preds)</span><br><span class="line">model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              optimizer=keras.optimizers.Adam(<span class="number">1e-3</span>),</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train, y_train, </span><br><span class="line">          batch_size=batch_size, </span><br><span class="line">          validation_split=<span class="number">0.2</span>,</span><br><span class="line">          epochs=epochs)</span><br></pre></td></tr></table></figure>
<p>提交结果：目前【39/130(提交团队数)】</p>
<p><img src="/articles/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B/image-20210730140527003.png" alt="image-20210730140527003"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">基于注意力机制的神经机器翻译的有效方法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-07-12 20:52:30 / 修改时间：20:59:57" itemprop="dateCreated datePublished" datetime="2021-07-12T20:52:30+08:00">2021-07-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p><a href="https://aclanthology.org/D15-1166/" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></p>
<p>基于注意力机制的神经机器翻译的有效方法</p>
</blockquote>
<p><strong>Bib TeX</strong></p>
<blockquote>
<p>@inproceedings{luong-etal-2015-effective,<br> title = “Effective Approaches to Attention-based Neural Machine Translation”,<br> author = “Luong, Thang  and<br>   Pham, Hieu  and<br>   Manning, Christopher D.”,<br> booktitle = “Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing”,<br> month = sep,<br> year = “2015”,<br> address = “Lisbon, Portugal”,<br> publisher = “Association for Computational Linguistics”,<br> url = “<a href="https://aclanthology.org/D15-1166" target="_blank" rel="noopener">https://aclanthology.org/D15-1166</a>“,<br> doi = “10.18653/v1/D15-1166”,<br> pages = “1412—1421”,<br>}</p>
</blockquote>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>在神经机器翻译中引入注意力机制(Attention)，使模型在翻译过程中选择性的关注句子中的某一部分。本文研究了两种简单有效的注意力机制。</p>
<ul>
<li>a global approach which always attends to all source words【全局方法，每次关注所有源词】</li>
<li>a local one that only looks at a subset of source words at a time【局部方法，每次关注原词的一个子集】</li>
</ul>
<p><em>global attention</em> 类似方法<strong>[1]</strong>，但架构上更加简单。<em>local attention</em> 更像是 <em>hard and soft attention</em> <strong>[2]</strong>的结合。两种方法在英德语双向翻译任务中取得了不错的成绩。与已经结合了已知技术（例如 dropout）的非注意力系统相比，高了5.0个BLEU点。在WMT’15英语到德语的翻译任务中表现 SOTA（state-of-the-art）。</p>
<blockquote>
<p>With local attention, we achieve a significant gain of　5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.</p>
</blockquote>
<h3 id="Neural-Machine-Translation"><a href="#Neural-Machine-Translation" class="headerlink" title="Neural Machine Translation"></a>Neural Machine Translation</h3><p>模型结构：</p>
<p><img src="/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712173309790.png" alt="image-20210712173309790"></p>
<p>采用堆叠的 <em>LSTM</em>结构<strong>[3]</strong>。其目标函数为：</p>
<script type="math/tex; mode=display">
J_t = \sum_{(x,y)\in D}-logP(y|x)</script><p>D为训练的语料。x 表示源句子，y表示翻译后的目标句子。</p>
<h3 id="Attention-based-Models"><a href="#Attention-based-Models" class="headerlink" title="Attention-based Models"></a>Attention-based Models</h3><p>这部分包括两种注意力机制：global 和 local。两种方式在解码阶段，将使用堆叠LSTM顶层的隐藏状态 $h_t$ 作为输入。区别在于获取上下文向量表示$c_t$方法不同。然后通过一个 简单的 <em>concatenate layer</em> 获得一个注意力隐藏状态$\hat h_t$:</p>
<script type="math/tex; mode=display">
\hat h_t = tanh(W_c[c_t;h_t])</script><p>最后通过 <em>softmax layer</em> 得出预测概率分布:</p>
<script type="math/tex; mode=display">
p(y_t|y<t,x)=softmax(W_s\hat h_t)</script><p><strong>Global Attention</strong></p>
<p><img src="/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712174342526.png" alt="image-20210712174342526"></p>
<p>主要思想是通过编码器的所有隐藏状态(hidden state)来获取上下文向量(context vector)表示 $c_t$。可变长度对齐向量$a_t$通过比较当前目标隐藏状态$h_t$和每个源隐藏状态$\overline h_s$得到：</p>
<script type="math/tex; mode=display">
a_t(s) = align(h_t,\overline h_s)=\frac{exp(score(h_t,\overline h_s))}{\sum_{s'}exp(h_t,\overline h_{s^{'}})}</script><p>score被称为 <em>content-based</em> 函数：</p>
<script type="math/tex; mode=display">
score(h_t,\overline h_s)=\begin{cases}
h_t^{T}\overline h_s, dot\\
h_t^{T}W_a\overline h_s, general\\
W_a[h_t;\overline h_s], concat
\end{cases}</script><p>与<strong>[1]</strong>的区别在于：</p>
<ul>
<li>只在编码器和解码器的顶部使用隐藏状态</li>
<li>计算路径更加简单：$h_t-&gt;a_t-&gt;c_t-&gt;\hat h_t$</li>
</ul>
<p><strong>Local Attention</strong></p>
<p>global 模式下，模型需要关注全局信息，其代价是非常大的。因此也就出现了 local attention。让注意力机制只去关注其中的一个子集部分。<em>其灵感来自于</em> <strong>[2]</strong>。</p>
<p><img src="/articles/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95/image-20210712185709989.png" alt="image-20210712185709989"></p>
<p>对比两张模型图来看，其中的局部对齐权重$a_t$由一部分局部隐藏状态计算得到，其长度变成了固定的，并且还多了一个Aligned position $p_t$ ，然后上下文向量(context vector) $c_t$ 由窗口$[p_t-D,p_t+D]$内的隐藏状态集合的加权平均得到。<em>其中D根据经验所得</em>。</p>
<p>考虑两种变体：</p>
<ul>
<li><p>Monotonic alignment (local-m)</p>
<p>即简单设置 $p_t = t$ ，认为源序列于目标序列是单调对齐的，那么$a_t$ 其实就和公式（4）计算方法一样了。</p>
</li>
<li><p>Predictive alignment (local-p)</p>
<p>$p_t=S·sigmoid(v_p^{T}tanh(W_ph_t))$ ，$v_p$和$W_p$是预测$p_t$ 的模型参数。S为源句子长度。最后$p_t\in [0,S]$ 。同时为了使对齐点更靠近$p_t$，设置一个以$p_t$为中心 的高斯分布，即$a_t$ 为：$a_t(s)=align(h_t,\overline h_s)exp(-\frac{(s-p_t)^2}{2\sigma^2}),\sigma=\frac{D}{2}$，s为高斯分布区间内的一个整数。</p>
</li>
</ul>
<h3 id="Input-feeding-Approach"><a href="#Input-feeding-Approach" class="headerlink" title="Input-feeding Approach"></a>Input-feeding Approach</h3><p>这一部分，主要是为了捕获在翻译过程中哪些源单词已经被翻译过了。对齐决策应当综合考虑过去对齐的信息。该方法将注意力向量$\hat h_t$ 作为下一个时间步的输入。主要有两个作用：</p>
<ul>
<li>希望模型充分关注到先前的对齐信息</li>
<li>创建一个在水平和垂直方向上都很深的网络</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>整篇论文看下来，大概就是在别人的baseline中引入注意力机制（global and local），然后使用<em>Input-feeding</em> 方法将过去的对齐信息考虑进来（大概就是加入了一个先验知识吧）。【PS：震惊！这些创新的点的灵感都来自其让人的论文中的方法。】</p>
<p>最后手动滑稽：</p>
<blockquote>
<p>Attention is all you need!</p>
</blockquote>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.  2015. Neural machine translation by jointly learning to align and translate. InICLR.</p>
<p>[2] Kelvin Xu,  Jimmy Ba,  Ryan Kiros,  Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show,attend and tell: Neural image caption generation with visual attention. InICML.</p>
<p>[3] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2015. Recurrent neural network regularization. InICLR.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/sequence-to-sequence-learning-with-neural-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/sequence-to-sequence-learning-with-neural-networks/" class="post-title-link" itemprop="url">sequence-to-sequence-learning-with-neural-networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-07-10 17:30:14 / 修改时间：17:55:31" itemprop="dateCreated datePublished" datetime="2021-07-10T17:30:14+08:00">2021-07-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p><a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a></p>
<p>基于神经网络的seq2seq</p>
</blockquote>
<p>很好的解决了序列到序列之间的映射问题，在语音识别和机器翻译这种长度未知且具有顺序的问题能够得到很好的解决。该模型应用到英语到法语的翻译任务，数据集来自WMT’14，在整个测试集上的BLEU得到达到34.8。</p>
<p>模型结构：</p>
<p><img src="/articles/sequence-to-sequence-learning-with-neural-networks/image-20210710154526137.png" alt="image-20210710154526137"></p>
<p>这里使用了两个串联的 lstm 网络，前一个用于读取输入序列，产生一个大的固定维度的向量表示，然后再用一个lstm 网络从向量表示中提取输出序列。一个编码器(Encoder)，一个解码器(Decoder)。</p>
<p>值得注意的是，论文提到在实现时，有三点与 lstm 不一样。【Our actual models differ from the above description in three important ways.】</p>
<ul>
<li>使用两个不同的lstm</li>
<li>4层 lstm</li>
<li>将输入序列进行反转【a,b,c —&gt; c,b,a】【PS： 这就是传说中的反向操作吗？！莫名其妙的trick，滑稽.jpg】</li>
</ul>
<p>目标函数：</p>
<script type="math/tex; mode=display">
\frac{1}{|S|}\sum_{(T,S)\in S}logP(T|S)</script><p>S为训练集，T表示正确的翻译。训练结束后，进行翻译时，在模型产生的多个翻译结果中找到最可能正确的翻译：</p>
<script type="math/tex; mode=display">
\hat T = \mathop{argmax}_{T}P(T|S)</script><p>通过一个简单的 <em>left-to-right beam search</em> 解码器(decoder)搜索最可能的翻译。</p>
<blockquote>
<p>We search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some translation. At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the model’s log probability. As soon as the “<EOS>“ symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam search(Table 1).</EOS></p>
</blockquote>
<p><strong>一个小的总结</strong></p>
<p>总的来说， <em>Reversing the Source Sentences</em> 这个操作给模型带来了很大的提升。</p>
<blockquote>
<p>the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU scores of its decoded translations increased from 25.9 to 30.6.</p>
</blockquote>
<p>至于为什么，论文中也没有给出很好的解释（大概是说，反转后，前面源句子的前几个词与目标句子的前几个词距离更近，模型能更好收敛。【a,b,c |w,x,y—&gt; c,b,a|w,x,y】w,x,y为a,b,c对应的翻译，反转后，a,b,c与w,x,y的平均距离不变，a离w更近了。但是c离y更远却没有影响模型精度。可能这就是玄学吧？！）。【PS：难道是因为误打误撞的尝试然后发现效果惊人，然后就发论文了？不过这篇论文确实奠定了之后的seq2seq模型的基础。】</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hahally.github.io/articles/Lost-in-just-the-translation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Hahally">
      <meta itemprop="description" content="I know nothing but my ignorance...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hahally's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/articles/Lost-in-just-the-translation/" class="post-title-link" itemprop="url">Lost-in-just-the-translation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-07-09 22:09:24 / 修改时间：22:10:35" itemprop="dateCreated datePublished" datetime="2021-07-09T22:09:24+08:00">2021-07-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Lost in just the translation</p>
</blockquote>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>本文介绍了自然语言翻译文本信息隐藏系统的设计与实现，并给出了实验结果。与前人的工作不同，本文提出的协议<strong>只需要翻译文本就可以恢复隐藏信息</strong>。这是一个重大的改进，因为传输源文本既浪费资源又不安全。现在，系统的安全性得到了改善，这不仅是因为源文本不再对敌方有用，还因为现在可以使用更广泛的防御系统(如混合人机翻译)。</p>
<h3 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h3><p><img src="/articles/Lost-in-just-the-translation/image-20210709214343204.png" alt="image-20210709214343204"></p>
<ul>
<li><p>Producing translations</p>
<p>方法大致与论文【Translation-Based Steganography】中提到的一样</p>
</li>
<li><p>Tokenization</p>
<p>双方使用相同的 Tokenization 算法，以获得相同的句子序列</p>
</li>
<li><p>Choosing h</p>
<p>选择合适的 h ($h \ge 0$)，h表示将信息隐藏在每个句子中的长度的位数。</p>
</li>
<li><p>Selecting translations</p>
<p>对于所有翻译，编码器首先使用与接收方共享的密钥计算每个翻译的加密键值散列。其基本思想是在给定句子的所有译文中选择一个句子，然后对其进行适当的长度编码，并在隐藏的信息中选择合适的位置。然而，由于给定句子中的位编码数量是可变的，因此该算法在这方面有很大的自由度。</p>
</li>
<li><p>Optimized Handling of Hash Collisions</p>
<p>哈希冲突的处理优化</p>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>为了不传输源文本，从而，引入了哈希映射和 Tokenization以及参数 h。产生翻译文本过程中混合人机翻译结果，使得隐藏的信息更加难以检测。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hahally"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Hahally</p>
  <div class="site-description" itemprop="description">I know nothing but my ignorance...</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hahally</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
